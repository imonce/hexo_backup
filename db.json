{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/assets/files/data.py","path":"assets/files/data.py","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_1.jpg","path":"assets/images/zabbix_1.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_10.jpg","path":"assets/images/zabbix_10.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_11.jpg","path":"assets/images/zabbix_11.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_12.jpg","path":"assets/images/zabbix_12.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_13.jpg","path":"assets/images/zabbix_13.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_4.jpg","path":"assets/images/zabbix_4.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_2.jpg","path":"assets/images/zabbix_2.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_5.jpg","path":"assets/images/zabbix_5.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_6.jpg","path":"assets/images/zabbix_6.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_8.jpg","path":"assets/images/zabbix_8.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_7.jpg","path":"assets/images/zabbix_7.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_9.jpg","path":"assets/images/zabbix_9.jpg","modified":1,"renderable":0},{"_id":"themes/hexo-theme-archer/source/assets/algolia_logo.svg","path":"assets/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/avatar/Misaka.jpg","path":"avatar/Misaka.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/avatar/C_Meng.png","path":"avatar/C_Meng.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/assets/example_qr.png","path":"assets/example_qr.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/assets/favicon.ico","path":"assets/favicon.ico","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/assets/loading.svg","path":"assets/loading.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/css/mobile.css","path":"css/mobile.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/lib/webfontloader.min.js","path":"lib/webfontloader.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/font/Source Sans Pro.woff","path":"font/Source Sans Pro.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/font/Source Sans Pro.woff2","path":"font/Source Sans Pro.woff2","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/scripts/search.js","path":"scripts/search.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/scripts/main.js","path":"scripts/main.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/scripts/share.js","path":"scripts/share.js","modified":1,"renderable":1},{"_id":"source/assets/images/zabbix_14.jpg","path":"assets/images/zabbix_14.jpg","modified":1,"renderable":0},{"_id":"source/assets/images/zabbix_3.jpg","path":"assets/images/zabbix_3.jpg","modified":1,"renderable":0},{"_id":"themes/hexo-theme-archer/source/intro/404-bg.jpg","path":"intro/404-bg.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/font/Oswald-Regular.ttf","path":"font/Oswald-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/font/SourceCodePro-Regular.ttf.woff2","path":"font/SourceCodePro-Regular.ttf.woff2","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/font/SourceCodePro-Regular.ttf.woff","path":"font/SourceCodePro-Regular.ttf.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/intro/about-bg.jpg","path":"intro/about-bg.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/intro/post-bg.jpg","path":"intro/post-bg.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-archer/source/intro/index-bg.jpg","path":"intro/index-bg.jpg","modified":1,"renderable":1},{"_id":"source/assets/font/normalblack.ttf","path":"assets/font/normalblack.ttf","modified":1,"renderable":0},{"_id":"source/assets/font/thinround.ttf","path":"assets/font/thinround.ttf","modified":1,"renderable":0},{"_id":"source/assets/font/thinblack.ttf","path":"assets/font/thinblack.ttf","modified":1,"renderable":0},{"_id":"source/assets/font/whiteboat.ttf","path":"assets/font/whiteboat.ttf","modified":1,"renderable":0}],"Cache":[{"_id":"themes/hexo-theme-archer/.gitignore","hash":"e2d7fa953bade3b94ed1cab25ffcf442c7540433","modified":1532125204000},{"_id":"themes/hexo-theme-archer/.eslintrc.json","hash":"e33afb2192ffc60103391860a973722169cfad96","modified":1532125204000},{"_id":"themes/hexo-theme-archer/LICENSE","hash":"0da0c361bf299375739c6b668a44af0f5faf37bb","modified":1532125204000},{"_id":"source/.DS_Store","hash":"27d3faa4bceb830d45bb821ce9f0f17da07897a1","modified":1537329534645},{"_id":"themes/hexo-theme-archer/README.md","hash":"18bca8cf11cd1cd223fa05e2aa8b0f39a0cb7140","modified":1532125204000},{"_id":"themes/hexo-theme-archer/_config.yml","hash":"725647f10c5188b7eda7d19ef407770648bd53b6","modified":1533109329155},{"_id":"themes/hexo-theme-archer/gulpfile.js","hash":"88e2615c21ca95ae7bcdba499e50a54aebcd9f56","modified":1532125204000},{"_id":"themes/hexo-theme-archer/package.json","hash":"80a1d936e347e61d9a41a4303ee32dc236017242","modified":1532125204000},{"_id":"themes/hexo-theme-archer/webpack.config.js","hash":"d20b6350c50e2981484cb9356a3e21a2124bcd72","modified":1532125204000},{"_id":"themes/hexo-theme-archer/webpack.prod.js","hash":"48de76cfa6274895801d3afc89ecbf04ee182a1c","modified":1532125204000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1526287204144},{"_id":"source/_posts/Chainer入门教程(上)：在Chainer中做线性回归.md","hash":"ec438e7ba152c952dde954ef1bc29ae21263ef48","modified":1533828726769},{"_id":"source/_posts/Mac如何卸载pandoc.md","hash":"1d075d96c0a6f76515ea2407c6a95e848bcbeb4e","modified":1538099455195},{"_id":"source/_posts/Python-对dict字典进行排序.md","hash":"28e84519573518c89561c8c944fc4980acb42538","modified":1533694370893},{"_id":"source/_posts/Chainer入门教程(下)-MNIST手写体识别.md","hash":"041ddc1fc12fef9ed1db3c0ece7de90ee6d0b75b","modified":1533828728977},{"_id":"source/_posts/Python-python通过threading开启多线程.md","hash":"5e3b5810cd043dc9fc6df902c090acc86db378b3","modified":1533827085384},{"_id":"source/_posts/Trick-bash-tensorboard-未找到命令.md","hash":"94c3d656ca0ce2d9fe36bfb630cb973f04a5f125","modified":1536202799912},{"_id":"source/_posts/Python入门：pickle模块简介.md","hash":"9d63beec8c61f8e520d73becf7642a678d439104","modified":1493011202000},{"_id":"source/_posts/Trick-git-pull-强制覆盖本地文件.md","hash":"5e659c04055f0c8d23f50256b1f91221474c324b","modified":1533016549383},{"_id":"source/_posts/[Reading Notes] Privacy-CNH- A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features.md","hash":"04c9d090e227e2eb445127780505879ea8a84d5a","modified":1481386474000},{"_id":"source/_posts/[Reading Notes] UniCrawl- A Practical Geographically Distributed Web Crawler.md","hash":"272adff7a5e7b69249de03d3d1068a97fc47da33","modified":1481386490000},{"_id":"source/_posts/hello-world.md","hash":"a9a0f86effcf3d23832bdc0459ff9410926fd4a1","modified":1533012789314},{"_id":"source/_posts/[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model.md","hash":"3126aa4970c9fce1d7c7d99aec5b0514fae15e97","modified":1481859010000},{"_id":"source/_posts/jupyter配置远程访问.md","hash":"358f8085051f9d4168c62fb9e16ef5b5e7f9e8fe","modified":1537327703056},{"_id":"source/_posts/linux如何查看磁盘可用空间.md","hash":"1318776900253bb76ec221d6944fcb9dbc9d0363","modified":1537326304344},{"_id":"source/_posts/mysql-python安装错误：EnvironmentError- mysql_config not found.md","hash":"6454605256336a0258b7cde178b03a99fd9c9d49","modified":1533016566519},{"_id":"source/_posts/从0到100：zabbix及其支持环境的完整安装教程.md","hash":"878e672897bd9eba1690fc052fba5de292dccc24","modified":1481385520000},{"_id":"source/_posts/python-获取某文件夹下所有文件名.md","hash":"17d8d67249308b2817e57dbaa75903f3d7e29597","modified":1533691930968},{"_id":"source/_posts/在linux上创建root权限用户（不修改系统文件）.md","hash":"0d682f059122bdc36b85659602d765d9fcc71335","modified":1533106779882},{"_id":"source/_posts/远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server.md","hash":"98ba6e973a022b1cd64c8d25cdc9a7fb42cce9f3","modified":1533016599974},{"_id":"source/assets/.DS_Store","hash":"d7ebab54a4004eae6ca8268e0823429496d83874","modified":1533019496106},{"_id":"source/_posts/远程连接mysql报错：error 2003 （hy000）-can't connect to mysql server on 'localhost' (10061).md","hash":"272962507403c9f5a60090080ad67cb2c80abf69","modified":1533016596646},{"_id":"themes/hexo-theme-archer/docs/develop-guide-zh.md","hash":"effc2bc9e0cecfd228b19283337ff29649ea5985","modified":1532125204000},{"_id":"themes/hexo-theme-archer/docs/develop-guide-en.md","hash":"0bb09c3c9d5f56820cb84e3316f60352b731d70c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/docs/README-en.md","hash":"8ccbc48f3fe4c3458567a918019d48692ba34b91","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/404.ejs","hash":"879641b1b5e49c43f2e096cad281f7d74df05127","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/index.ejs","hash":"716ba4a30860e36077dfdfffa02c1cd60301d8a3","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/about.ejs","hash":"06019d835c0a51fed8aa086d1dfcf368f9921b42","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/post.ejs","hash":"f8ae54b82a472319b2673c8312b5e779395f2f07","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/layout.ejs","hash":"6573c53b013e8ea12607fad7d3d8bd0236664176","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/site-meta.ejs","hash":"d866aa1a892d4bfe3368f6dbd0f1baa8c5831947","modified":1532125204000},{"_id":"source/assets/files/data.py","hash":"9b786598a7f4f755d08f2d46ce8f44e218869b51","modified":1519628309784},{"_id":"source/assets/images/zabbix_1.jpg","hash":"422f10b96383741cf0fe0c3e137e9516c9671027","modified":1470618138000},{"_id":"source/assets/images/zabbix_10.jpg","hash":"eccaf58d03d929085d5c9a111eb151b7c67204e8","modified":1470618138000},{"_id":"source/assets/images/zabbix_11.jpg","hash":"673c4eab7bc61ecaed0d0db3fd8d3dc63ed7759b","modified":1470618138000},{"_id":"source/assets/images/zabbix_12.jpg","hash":"2299054bb9cfc5d1692d39ae68a67912f1f1b698","modified":1470618138000},{"_id":"source/assets/images/zabbix_13.jpg","hash":"24f23aad3ef6b70e19764750fda68be072ac51a6","modified":1470618138000},{"_id":"source/assets/images/zabbix_4.jpg","hash":"181f9050699f9f8fa43e8ecdd97d9e1cc87a8c53","modified":1470618138000},{"_id":"source/assets/images/zabbix_2.jpg","hash":"4d152c9991dc2883805d15c9ef7e963920ca6a84","modified":1470618138000},{"_id":"source/assets/images/zabbix_5.jpg","hash":"83c828870ed494697980a11ad1037e5859e5a3fd","modified":1470618138000},{"_id":"source/assets/images/zabbix_6.jpg","hash":"6d4c31a6b425ea80d165dc788d8ca989e3f58346","modified":1470618138000},{"_id":"source/assets/images/zabbix_8.jpg","hash":"e0219e62e2da70c2658127600f81e8dec066de9d","modified":1470618138000},{"_id":"source/assets/images/zabbix_7.jpg","hash":"7967d247d2050b8b4bc9215f261dfdb3d6e41b84","modified":1470618138000},{"_id":"themes/hexo-theme-archer/.github/ISSUE_TEMPLATE/-----------bug--help-wanted-or-bug-report-.md","hash":"012bd3afea5565204f75d8c89048ad33b9c2f948","modified":1532125204000},{"_id":"source/assets/images/zabbix_9.jpg","hash":"06044eb60fc7ec1dde5fbd2266061f436296dd0d","modified":1470618138000},{"_id":"themes/hexo-theme-archer/.github/ISSUE_TEMPLATE/-----feature-request-.md","hash":"886fb1252702d46c89536b579d35ce49e53ec54d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/.github/ISSUE_TEMPLATE/-----other-issue-.md","hash":"4bf3c8ebef14a81bb55e8e0468d816e83c4133ac","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/algolia.ejs","hash":"2f5f9073378f628a9124792b5301c1540bc5a264","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-background-image.ejs","hash":"20044d7e2740a690181149d137b9aef8b9860b4d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-footer.ejs","hash":"7bfb8af21f7d30780d1a3af5484760b07b4fbb1d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-preload-polyfill.ejs","hash":"2aea84cb43f4479131620b3c3dfd7cebe9be36ba","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-head.ejs","hash":"36b0a328beffca85fa564b9b3a49af8136e7042c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-profile.ejs","hash":"d2a8ae42792ffd391f591dbcda3bdbc6fddebe02","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-header.ejs","hash":"12e28693dc53cab5f6fd389f15ea0f811c9c37e9","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-title-tags.ejs","hash":"418559ab11726f69621c3a58cd21903adff1e48e","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/base-social.ejs","hash":"2d068432031b3bebf9438c775370a2d1b4492d6b","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/intro-height.ejs","hash":"cd0d34a811dc4666980d57c00d70dd82bd5450eb","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/assets/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/avatar/C_Meng.png","hash":"31fff0df4a27d5cdc204456c2065dc5911eb7cf3","modified":1432522870000},{"_id":"themes/hexo-theme-archer/source/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/assets/favicon.ico","hash":"8b200c575d273d41a179c102442e191414e74eae","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/assets/loading.svg","hash":"45be17d07697d604d8981890eb21e308530c7a38","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/css/mobile.css","hash":"a52f4801fa1134caf783cd3a79a130b4cf8aa5d3","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/css/style.css","hash":"e523b75de9bb75e8a106c5b0fa6992826bb28a3c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/lib/webfontloader.min.js","hash":"bc6ffe9c0d8b3285564619a445c6ca575eb9d0f5","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/scripts/search.js","hash":"d5f739e261e8ce74f993c6157b248663bda122bf","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/scripts/main.js","hash":"cde31e8850e6c2e854f1eef2bc49769a9c5f6290","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/init.js","hash":"989dea03ce93962b6a52818ee770ca3891679322","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/scripts/share.js","hash":"bb5bb37ce7f47f8c084b232df3e5fe2378d7ca01","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/browser.js","hash":"03017b1e89b59346e681464c7609593c9aafa54c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/fancybox.js","hash":"0ce36efa325767c0ee7d5e5dfa174c68dd606e72","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/mobile.js","hash":"3c826a4385dc58ee878e5ea9bf3a25fac5d2b307","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/initSidebar.js","hash":"522aba19524b49efd323a2199f6eaa7396f1bd48","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/main.js","hash":"5fb052e7a92c3fae0ecc86b66a032a98d7b9423f","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/scroll.js","hash":"11ae5ca33f6f84897ef4a01697a624770bea2025","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/share.js","hash":"dff48b27ac212c4ea8e0c4c5ee323862c06c2882","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/search.js","hash":"af9bdbff06987fdca8340ea49ccd91e993b9be53","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/sidebar.js","hash":"8707392554fe813e33be84e9c64d30cb733dd0b4","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/tag.js","hash":"156547cad5230c899cac12d3d29e60f5a103b7c4","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/toc.js","hash":"2a01f07f302ccea3d36787571e34fd070ac42797","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_mixin.scss","hash":"daf254814076d79ae943e7c866d9307125c16328","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/js/util.js","hash":"1c244b8def678df797ab3c049a03998db4f9dabb","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_common.scss","hash":"71aa8bf9d5c66a432fd32b96a0953d53fcb533d2","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_normalize.scss","hash":"b33d0d7e2e2807f50735f43e742f3c33471d38f7","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/mobile.scss","hash":"76a65f93c8bbcd5bc5fb7fca55919a12e7543a2b","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_variables.scss","hash":"a9026af1f66bf4a0b1a7b36420b1c7b5ac4c2c12","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/style.scss","hash":"2d959b9c6a3a5b70df1bfa4ba0a6b8b758454e74","modified":1532125204000},{"_id":"source/assets/images/zabbix_14.jpg","hash":"35d5f48bb8b4cdfa6909861b3ffebb3c76f437de","modified":1470618138000},{"_id":"source/assets/images/zabbix_3.jpg","hash":"f2527292c91f487b0d2c0590b3a6fa952a05a9a4","modified":1470618138000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/custom.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/lib/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1532125204000},{"_id":"themes/hexo-theme-archer/package-lock.json","hash":"229960bd92beff7974f422f7cb2025ae610a2191","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/gitment.ejs","hash":"794647a8a4ac9e1d01d74c07717175a141b1e01c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/disqus.ejs","hash":"cadd97820a23d01d8b7e2f19a86e5290cef5a8a6","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/livere.ejs","hash":"b27469f19f35ec2037c7bce736a12e9dfb5e360b","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/changyan.ejs","hash":"7691642fb9375607e0ac988a7420999e242cb2c9","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/valine.ejs","hash":"2154360e330dab94540e168e9d301c7b7091968e","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/critical-css/critical-style.ejs","hash":"731d3980ac665d96dc6a40d31c2f8b86a221a8bb","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/script/font-loader.ejs","hash":"291cc0a4a444a93b93c32e2c39383c70e36e46de","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/comment/youyan.ejs","hash":"790d76f2667377db78a7d04bad8d629e47e7aab0","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/sidebar/base-sidebar.ejs","hash":"2d38af7cb5aabc0af99de239bd180c516ec4c56c","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/sidebar/sidebar-archives.ejs","hash":"daa78b17a773bca569f1c8ca2c916495ca138da6","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/sidebar/sidebar-categories.ejs","hash":"147214469c4909eb94943599883f9fa0f3cc090d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/layout/_partial/sidebar/sidebar-tags.ejs","hash":"d6e1a192cac5dcf365ce1a52df4f23feafeeb849","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_algolia.scss","hash":"75776d8b85c8d2edc27eb7ed60d7c371f5109da0","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_index-page.scss","hash":"8a74f79686e327b0566ea606fd7625eb57daf898","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_404.scss","hash":"64a721c19c8c4d5b2b07d6aedcde1565be734e6d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_post-page.scss","hash":"0880d1be411e0a574e422234f6b9fe429c12ed10","modified":1532125204000},{"_id":"themes/hexo-theme-archer/docs/snap.png","hash":"0b2a8bf016f6eed576abfdcdb7dcf8de51c12562","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_footer.scss","hash":"73caccbe2634ff84e386d58e7f6ecd52d5ca2151","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_paginator.scss","hash":"dbd7f802f3812f7e15c12885e495296f3697c580","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_intro.scss","hash":"65efbfbaec7d09120ba62fee6e4643e6108098f6","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_header.scss","hash":"eaff5b3e942e461109cdb6fd52f1d124b5a11951","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_post/_code.scss","hash":"1229364f7c3484cc2ada6f118c859e3fd1dd9129","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_profile.scss","hash":"e69b62df7c36d938cc4cf8e1fdcbc7109be41119","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_partial/_scrollbar.scss","hash":"9f7a3877adccbfaf66ad574dbe2c8dc85fbf2f5a","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_sidebar/_sidebar-archive.scss","hash":"a25960d1d9f501e50cb1fe86328dbc65faa29d22","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_sidebar/_sidebar.scss","hash":"9226a04dd9ea8389f88ced34962f1f55192d7a1a","modified":1532125204000},{"_id":"themes/hexo-theme-archer/src/scss/_partial/_sidebar/_sidebar-tags.scss","hash":"fe42459805e4caa303df08964184548c33507d23","modified":1532125204000},{"_id":"themes/hexo-theme-archer/source/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1532125204000},{"_id":"source/assets/font/normalblack.ttf","hash":"272ada9f20f08a245b8f8e3f9277e50662207931","modified":1470618138000},{"_id":"source/assets/font/thinround.ttf","hash":"6e582119d485025a0fc1d329c13ec5c47830b039","modified":1470618138000},{"_id":"source/assets/font/thinblack.ttf","hash":"0f6d5e73135a1a0eab1b6dbf5aca557eeed5721c","modified":1470618138000},{"_id":"source/assets/font/whiteboat.ttf","hash":"fc619e7d12aaa5ddeb797c01bbf1023aee3a9718","modified":1470618138000},{"_id":"public/2018/09/19/jupyter配置远程访问/index.html","hash":"3159d07f485ab35f5317fa6bee09f3fd34d54ef5","modified":1538099465807},{"_id":"public/2018/09/19/linux如何查看磁盘可用空间/index.html","hash":"89bda5192a45aa72a811635ffc235fd97cfb22aa","modified":1538099465824},{"_id":"public/2018/09/06/Trick-bash-tensorboard-未找到命令/index.html","hash":"6eef93ad8d84a4c7687279bc1de7bf7c43e65374","modified":1538099465824},{"_id":"public/2018/08/09/Python-python通过threading开启多线程/index.html","hash":"e34adc1d51e79a4bfaa84f43af898b0f331eb7cc","modified":1538099465825},{"_id":"public/2018/08/06/python-获取某文件夹下所有文件名/index.html","hash":"ea34532cf95fa0ab6f5d4ae50eddb0b50856e19e","modified":1538099465825},{"_id":"public/2018/08/08/Python-对dict字典进行排序/index.html","hash":"abc47a26277472236d79e1fb001094b97ba2a455","modified":1538099465825},{"_id":"public/2018/07/31/Trick-git-pull-强制覆盖本地文件/index.html","hash":"a3cea32c753164ac6d9b3367686da99529b798b9","modified":1538099465825},{"_id":"public/2018/07/31/在linux上创建root权限用户（不修改系统文件）/index.html","hash":"23ab84e5c883f1a218460adbcd6bbdb617da4074","modified":1538099465825},{"_id":"public/2017/04/24/远程连接mysql报错：error 2003 （hy000）-can't connect to mysql server on 'localhost' (10061)/index.html","hash":"87450f60a5012f1329befe191278c1c3708b418f","modified":1538099465825},{"_id":"public/2017/04/24/远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server/index.html","hash":"038f22b1a342492df2aea6b7e37365473174da8a","modified":1538099465825},{"_id":"public/2017/04/24/Python入门：pickle模块简介/index.html","hash":"1d2da96ed050e03ef856e5ffc12d0e95e9925bf6","modified":1538099465825},{"_id":"public/2017/04/24/mysql-python安装错误：EnvironmentError- mysql_config not found/index.html","hash":"172b4ecaabbe15765212c0b81216dc19e2d1560e","modified":1538099465826},{"_id":"public/2016/12/15/Chainer入门教程(下)-MNIST手写体识别/index.html","hash":"bc60e0a830efb76f6438052a5ed114e447c63456","modified":1538099465826},{"_id":"public/2016/12/14/Chainer入门教程(上)：在Chainer中做线性回归/index.html","hash":"1d7030a676dfd062c48d0c6dd7c038e8220683c8","modified":1538099465826},{"_id":"public/2016/12/11/[Reading Notes] UniCrawl- A Practical Geographically Distributed Web Crawler/index.html","hash":"6f467f7fd2895805fce02e849cd31135bf46701e","modified":1538099465826},{"_id":"public/2016/12/13/[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model/index.html","hash":"12664d46fcf69e732157cb111386298dc6aa0bc7","modified":1538099465826},{"_id":"public/2016/12/10/hello-world/index.html","hash":"ddc28b768e3e319de5d4159be3857ed5f58d6e98","modified":1538099465826},{"_id":"public/2016/12/10/[Reading Notes] Privacy-CNH- A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features/index.html","hash":"6c1817d7730a768e16448c5113ae6b78c9901bb4","modified":1538099465826},{"_id":"public/2016/08/08/从0到100：zabbix及其支持环境的完整安装教程/index.html","hash":"557a36cfdd3e17c22a98e668cd260cbb59a7cdf4","modified":1538099465826},{"_id":"public/archives/index.html","hash":"8d2732392186195d062cd533bed0a84d2bcb202a","modified":1538099465826},{"_id":"public/archives/page/2/index.html","hash":"93a5a0cd6d4171b63b3e7cd45ede7c497c2742e3","modified":1538099465826},{"_id":"public/archives/2016/index.html","hash":"cfe056f55d251870e5d9385af0da0affefdf5ed4","modified":1538099465826},{"_id":"public/archives/2016/08/index.html","hash":"38bec70adba2ab3baee5dff2272eb975055f82b3","modified":1538099465826},{"_id":"public/archives/2016/12/index.html","hash":"a603daaec60d936b3c1a0094dde73b8f6280ce09","modified":1538099465826},{"_id":"public/archives/2017/index.html","hash":"a8d397fb8bb28c7a6e8ab8bd83c2de2a7027456a","modified":1538099465826},{"_id":"public/archives/2017/04/index.html","hash":"a8d397fb8bb28c7a6e8ab8bd83c2de2a7027456a","modified":1538099465826},{"_id":"public/archives/2018/index.html","hash":"9cd00c4ed63cf9ca74401000f13519b063c4cb28","modified":1538099465826},{"_id":"public/archives/2018/07/index.html","hash":"d75833d75492f7f34bc2011cc9ca6e314ef79c37","modified":1538099465827},{"_id":"public/archives/2018/08/index.html","hash":"2c07ea440dba7110d0f15d2ac5357935005c44e2","modified":1538099465827},{"_id":"public/archives/2018/09/index.html","hash":"75e4a6ed3cb8c8aa274eb8a69f0635b6661339db","modified":1538099465827},{"_id":"public/index.html","hash":"fdecb5afe61a2c0191828900fc583b073b855fc3","modified":1538099465827},{"_id":"public/page/2/index.html","hash":"4e9440e96136d41dbb74294f1be7933219b4a7aa","modified":1538099465827},{"_id":"public/tags/chainer/index.html","hash":"83a554671b6b8a97f12b908db05d05166ebf2f10","modified":1538099465827},{"_id":"public/tags/入门教程/index.html","hash":"83a554671b6b8a97f12b908db05d05166ebf2f10","modified":1538099465827},{"_id":"public/tags/机器学习/index.html","hash":"83a554671b6b8a97f12b908db05d05166ebf2f10","modified":1538099465827},{"_id":"public/tags/神经网络/index.html","hash":"83a554671b6b8a97f12b908db05d05166ebf2f10","modified":1538099465827},{"_id":"public/tags/python/index.html","hash":"a1cae209cb5f36b12438094f9c26fc854f36c686","modified":1538099465827},{"_id":"public/tags/dict/index.html","hash":"420c68e99dc992d66c9a022cef93ec73e2520a29","modified":1538099465827},{"_id":"public/tags/字典/index.html","hash":"420c68e99dc992d66c9a022cef93ec73e2520a29","modified":1538099465827},{"_id":"public/tags/python入门/index.html","hash":"c87fd9ffa76a049734f7bb1f4d9b6a4529b36bdc","modified":1538099465827},{"_id":"public/tags/threading/index.html","hash":"26b109d8f3420851dad8ecec27d0662cb7ed83cd","modified":1538099465827},{"_id":"public/tags/多线程/index.html","hash":"26b109d8f3420851dad8ecec27d0662cb7ed83cd","modified":1538099465827},{"_id":"public/tags/linux/index.html","hash":"32109926529098f2c5a93047291f719fd90df904","modified":1538099465828},{"_id":"public/tags/trick/index.html","hash":"c79a953e1b8a0421df03ed6a200e5f085702d619","modified":1538099465828},{"_id":"public/tags/tensorboard/index.html","hash":"6475ef6a9cdf78be4bd0b9b42228a196bee848d5","modified":1538099465828},{"_id":"public/tags/git/index.html","hash":"7eaa35166da0cdbec3116df12de168c4bc503be9","modified":1538099465828},{"_id":"public/tags/pickle/index.html","hash":"3661aae842a2e413e1bfca760a99030263bd764f","modified":1538099465828},{"_id":"public/tags/PCNH/index.html","hash":"392ad45d486ac2a36bec38cc1d5ec4c99d228a8d","modified":1538099465828},{"_id":"public/tags/CNN/index.html","hash":"2194f93c1400897c29decf52cf177d51044e0203","modified":1538099465828},{"_id":"public/tags/Privacy-Detect/index.html","hash":"392ad45d486ac2a36bec38cc1d5ec4c99d228a8d","modified":1538099465828},{"_id":"public/tags/Reading-Notes/index.html","hash":"1b4dc8021d7d8de223d6759f991aefa2377d616c","modified":1538099465828},{"_id":"public/tags/web/index.html","hash":"a7f10e38f1293a613fd5e76b408d518a3d8078b2","modified":1538099465828},{"_id":"public/tags/crawler/index.html","hash":"a7f10e38f1293a613fd5e76b408d518a3d8078b2","modified":1538099465828},{"_id":"public/tags/unicrawl/index.html","hash":"a7f10e38f1293a613fd5e76b408d518a3d8078b2","modified":1538099465828},{"_id":"public/tags/object-detection/index.html","hash":"34b79e481127d1a2019a77c882f396f98a4f16cc","modified":1538099465828},{"_id":"public/tags/multi-region/index.html","hash":"34b79e481127d1a2019a77c882f396f98a4f16cc","modified":1538099465828},{"_id":"public/tags/sematic-segmentation-aware/index.html","hash":"34b79e481127d1a2019a77c882f396f98a4f16cc","modified":1538099465828},{"_id":"public/tags/jupyter/index.html","hash":"ca6b8f4f1596ae8d5b6ea686b3cf6d1abc34e08f","modified":1538099465828},{"_id":"public/tags/远程访问/index.html","hash":"ca6b8f4f1596ae8d5b6ea686b3cf6d1abc34e08f","modified":1538099465828},{"_id":"public/tags/ipython/index.html","hash":"ca6b8f4f1596ae8d5b6ea686b3cf6d1abc34e08f","modified":1538099465828},{"_id":"public/tags/notebook/index.html","hash":"ca6b8f4f1596ae8d5b6ea686b3cf6d1abc34e08f","modified":1538099465828},{"_id":"public/tags/磁盘/index.html","hash":"ef212d63674f2a9205164608b008baafd807906b","modified":1538099465829},{"_id":"public/tags/空间/index.html","hash":"ef212d63674f2a9205164608b008baafd807906b","modified":1538099465829},{"_id":"public/tags/mysql/index.html","hash":"9f19aba38a819590539f2241df8f4f9631b35fe7","modified":1538099465829},{"_id":"public/tags/mysql-python安装错误/index.html","hash":"f00b0849aa8261a9d5d7322e50fc2dca0dcb5564","modified":1538099465829},{"_id":"public/tags/zabbix安装/index.html","hash":"38bec70adba2ab3baee5dff2272eb975055f82b3","modified":1538099465829},{"_id":"public/tags/lamp配置/index.html","hash":"38bec70adba2ab3baee5dff2272eb975055f82b3","modified":1538099465829},{"_id":"public/tags/os/index.html","hash":"8a5e021f22caa08fb0d831a23eaa189ee9e9e644","modified":1538099465829},{"_id":"public/tags/远程连接报错/index.html","hash":"a400fc0c2a8b829512ca0debcdc8758f2d1352da","modified":1538099465829},{"_id":"public/2018/09/28/Mac如何卸载pandoc/index.html","hash":"95a0d660f6da69afb8e0f372a72cfc8a8e80d070","modified":1538099465834},{"_id":"public/tags/mac/index.html","hash":"6e6c36b4f8e377da1519248dd948eefac6dff406","modified":1538099465834},{"_id":"public/tags/卸载/index.html","hash":"6e6c36b4f8e377da1519248dd948eefac6dff406","modified":1538099465834},{"_id":"public/tags/pandoc/index.html","hash":"6e6c36b4f8e377da1519248dd948eefac6dff406","modified":1538099465834},{"_id":"public/assets/files/data.py","hash":"9b786598a7f4f755d08f2d46ce8f44e218869b51","modified":1538099465837},{"_id":"public/assets/images/zabbix_1.jpg","hash":"422f10b96383741cf0fe0c3e137e9516c9671027","modified":1538099465837},{"_id":"public/assets/images/zabbix_10.jpg","hash":"eccaf58d03d929085d5c9a111eb151b7c67204e8","modified":1538099465837},{"_id":"public/assets/images/zabbix_11.jpg","hash":"673c4eab7bc61ecaed0d0db3fd8d3dc63ed7759b","modified":1538099465837},{"_id":"public/assets/images/zabbix_12.jpg","hash":"2299054bb9cfc5d1692d39ae68a67912f1f1b698","modified":1538099465837},{"_id":"public/assets/images/zabbix_13.jpg","hash":"24f23aad3ef6b70e19764750fda68be072ac51a6","modified":1538099465837},{"_id":"public/assets/images/zabbix_4.jpg","hash":"181f9050699f9f8fa43e8ecdd97d9e1cc87a8c53","modified":1538099465837},{"_id":"public/assets/images/zabbix_2.jpg","hash":"4d152c9991dc2883805d15c9ef7e963920ca6a84","modified":1538099465837},{"_id":"public/assets/images/zabbix_5.jpg","hash":"83c828870ed494697980a11ad1037e5859e5a3fd","modified":1538099465837},{"_id":"public/assets/images/zabbix_8.jpg","hash":"e0219e62e2da70c2658127600f81e8dec066de9d","modified":1538099465837},{"_id":"public/assets/images/zabbix_6.jpg","hash":"6d4c31a6b425ea80d165dc788d8ca989e3f58346","modified":1538099465837},{"_id":"public/assets/images/zabbix_7.jpg","hash":"7967d247d2050b8b4bc9215f261dfdb3d6e41b84","modified":1538099465838},{"_id":"public/assets/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1538099465838},{"_id":"public/assets/images/zabbix_9.jpg","hash":"06044eb60fc7ec1dde5fbd2266061f436296dd0d","modified":1538099465838},{"_id":"public/avatar/Misaka.jpg","hash":"74a0372523f98dfbba992bf80642e160d04dc9b1","modified":1538099465838},{"_id":"public/assets/example_qr.png","hash":"cce20432c34875f4d9c6df927ede0fc0f00bb194","modified":1538099465838},{"_id":"public/avatar/C_Meng.png","hash":"31fff0df4a27d5cdc204456c2065dc5911eb7cf3","modified":1538099465839},{"_id":"public/assets/favicon.ico","hash":"8b200c575d273d41a179c102442e191414e74eae","modified":1538099465839},{"_id":"public/assets/loading.svg","hash":"45be17d07697d604d8981890eb21e308530c7a38","modified":1538099465839},{"_id":"public/font/Source Sans Pro.woff","hash":"a6722c9b6439b7a020a9be3d3178970757a9265c","modified":1538099465839},{"_id":"public/font/Source Sans Pro.woff2","hash":"da65f527a8da65d5eb6721626d28cfdb46ab104a","modified":1538099465839},{"_id":"public/assets/images/zabbix_14.jpg","hash":"35d5f48bb8b4cdfa6909861b3ffebb3c76f437de","modified":1538099465847},{"_id":"public/assets/images/zabbix_3.jpg","hash":"f2527292c91f487b0d2c0590b3a6fa952a05a9a4","modified":1538099465847},{"_id":"public/intro/404-bg.jpg","hash":"3afb5bb26f4ff0bd0e0a28df955c8aa7d746d3c5","modified":1538099465847},{"_id":"public/font/Oswald-Regular.ttf","hash":"965d729546a43a8490ad4cf33c25ac475682100c","modified":1538099465847},{"_id":"public/font/SourceCodePro-Regular.ttf.woff2","hash":"f5991289ec17884cb641da0646d278d36702a190","modified":1538099465847},{"_id":"public/font/SourceCodePro-Regular.ttf.woff","hash":"12eef75e1ad3eca9dae42b65505010ce4464a315","modified":1538099465848},{"_id":"public/css/mobile.css","hash":"a52f4801fa1134caf783cd3a79a130b4cf8aa5d3","modified":1538099465852},{"_id":"public/scripts/search.js","hash":"d5f739e261e8ce74f993c6157b248663bda122bf","modified":1538099465852},{"_id":"public/lib/webfontloader.min.js","hash":"bc6ffe9c0d8b3285564619a445c6ca575eb9d0f5","modified":1538099465859},{"_id":"public/intro/about-bg.jpg","hash":"ab388276822417cc4e703312c14e20280ec783b3","modified":1538099465859},{"_id":"public/intro/post-bg.jpg","hash":"525fafb2238c27754d8fa751f143ff1de9b8482d","modified":1538099465860},{"_id":"public/scripts/share.js","hash":"bb5bb37ce7f47f8c084b232df3e5fe2378d7ca01","modified":1538099465863},{"_id":"public/intro/index-bg.jpg","hash":"96b52e177b8bc53e64ec6ee1e10b2b6a4e13083b","modified":1538099465864},{"_id":"public/css/style.css","hash":"e523b75de9bb75e8a106c5b0fa6992826bb28a3c","modified":1538099465868},{"_id":"public/scripts/main.js","hash":"cde31e8850e6c2e854f1eef2bc49769a9c5f6290","modified":1538099465868},{"_id":"public/lib/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1538099465872},{"_id":"public/assets/font/normalblack.ttf","hash":"272ada9f20f08a245b8f8e3f9277e50662207931","modified":1538099465883},{"_id":"public/assets/font/thinround.ttf","hash":"6e582119d485025a0fc1d329c13ec5c47830b039","modified":1538099465891},{"_id":"public/assets/font/thinblack.ttf","hash":"0f6d5e73135a1a0eab1b6dbf5aca557eeed5721c","modified":1538099465893},{"_id":"public/assets/font/whiteboat.ttf","hash":"fc619e7d12aaa5ddeb797c01bbf1023aee3a9718","modified":1538099465905}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Chainer入门教程(上)：在Chainer中做线性回归","date":"2016-12-14T06:14:00.000Z","comments":1,"_content":"\n## 简介\n\n神经网络技术在统计建模、数据的转换分类回归等各大领域都有很大的应用空间。但是由于计算本身的复杂性以及早期的计算能力不足，神经网络一直没有得到很大的发展。然而近几年，随着GPU计算的进步，涌现出一大批非常强大而实用的神经网络的训练框架如Caffe、Keras、CUDA convnet、Torch7等。在这片教程中，我们着重介绍另一款灵活又好用的框架：[Chainer](http://chainer.org)的基础使用方法。你可以通过Jupyter Notebook或是其他的python终端来跟进这篇教程。\n\n在这篇教程中，我们将先通过编写一个简单的线性回归器来帮助你入门，然后我们再编写一个用于识别[MNIST](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)手写数字的标准的深度学习模型来让你熟悉编程逻辑。\n\n<!-- more-->\n\n首先我们需要安装几个python包：\n\n```\n#首先更新一下cython，旧的版本可能会导致chainer安装出错\npip install --upgrade cython\n#再把numpy、matplotlib、chainer都安装一下\npip install numpy\npip install matplotlib\npip install chainer\n```\n\n## I. Chainer基础\n\n首先我们要导入这篇教程中要用到的包，关于每个包的作用，之后会有简单的介绍：\n\n```python\n#Matplotlib and Numpy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Chainer Specific\nfrom chainer import FunctionSet, Variable, optimizers, serializers\nimport chainer.functions as F\nimport chainer.links as L\n```\n\n### 了解Chainer中Variables和Functions的特点和作用\n\n首先，我们通过包裹numpy数组定义两个简单的Chainer Variables变量。数组中只有一个值，这样可以方便我们后续做一些标量运算。\n\n```python\n# Create 2 chainer variables then sum their squares\n# and assign it to a third variable.\na = Variable(np.array([3], dtype=np.float32))\nb = Variable(np.array([4], dtype=np.float32))\nc = a**2 + b**2\n```\n\n在Chainer中，Variables对象既是象征的又是数字的。它们在data属性中包含数据的值，但也包含已在它们上执行的操作链的信息。当你需要训练神经网络时，这段操作历史是非常有用的。我们通过调用backward()方法对变量进行BP或（反向模式）自动分化，这给我们提供了所选择的优化与所有更新我们的神经网络所需要的权重信息。\n\n这个过程之所以可以发生，是因为Chainer的Variables对象把所有对其进行操作的函数都进行了存储，分析了其表达式及导数。你将会使用到的一些函数会是带参的，包含在chainer.links中(这里我们作为L导入)。这些函数的参数将在我们的网络的每个训练迭代中更新。其他包含在chainer.functions(这里我们作为F导入)中的函数将会是无参的，只是对变量执行预定义的数学操作。连加减运算都需要调用Chainer Functions，各变量的操作历史都将保存为变量本身的一部分。这使我们能够计算任何变量的相对于任何其他变量的导数。\n\n下面我们来看一个例子，过程如下：\n\n1. 通过调用data属性检查之前定义的变量\n2. 使用backward()方法，对变量c进行反向传播\n3. 通过在变量中存储的grad属性，检查其导数\n\n```python\n#Inspect the value of your variables.\nprint(\"a.data: {0}, b.data: {1}, c.data: {2}\".format(a.data, b.data, c.data))\n```\n\noutput: `a.data: [ 3.], b.data: [ 4.], c.data: [ 25.]`\n\n```python\n#Now call backward() on the sum of squares.\nc.backward()\n#And inspect the gradients.\nprint(\"dc/da = {0}, dc/db = {1}, dc/dc = {2}\".format(a.grad, b.grad, c.grad))\n```\n\noutput: `dc/da = [ 6.], dc/db = [ 8.], dc/dc = [ 1.]`\n\n## II. 在Chainer中做线性回归\n\n现在我们知道了一点关于基础的Chainer在做什么，让我们用它来训练最基本的神经网络、线性回归网络。当然，这里所涉及的最小二乘优化的解决方案，通过正常的等式计算分析可能更有效，但这个过程将展示每个网络的基本组成部分，你可以直接进行训练。\n\n这个网络没有隐藏的节点，只涉及一个输入节点，一个输出节点，和一个连接他们两个的线性函数。\n\n我们将要进行下列步骤：\n\n1. 生成随机的线性数据集\n2. 通过Chainer Link构造一个前向的网络\n3. 构造一个函数来进行网络训练\n\n```python\n# Generate linearly related datasets x and y.\n\nx = 30*np.random.rand(1000).astype(np.float32)\ny = 7*x+10\ny += 10*np.random.randn(1000).astype(np.float32)\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809233029.png)\n\n一般来说，在Chainer中想要让你的结构保持共同的神经网络，你需要构造一个forward函数，这个函数会带入你的不同的带参的link函数并在序列中把所有数据运行一遍。\n\n然后，我们需要写一个train函数，它将在你的所有的数据上把forward函数运行epochs次。并且在每次forward之后，都调用loss/objective函数，然后通过optimizer和通过backward方法算出的梯度来更新权重。\n\nChainer使用者通常会在一开始的时候就定义好Link的层（这里我们只需要一层）。然后他们会通过实例化一个optimizer类来指定要用的优化器。最后，他们会通过调用优化实例的设置方法，告诉optimizer来跟踪和更新指定的模型层的参数，该层将作为一个参数被跟踪。\n\n```python\n# Setup linear link from one variable to another.\n\nlinear_function = L.Linear(1,1)\n\n# Set x and y as chainer variables, make sure to reshape\n# them to give one value at a time.\nx_var = Variable(x.reshape(1000,-1))\ny_var = Variable(y.reshape(1000,-1))\n\n# Setup the optimizer.\noptimizer = optimizers.MomentumSGD(lr=0.001)\noptimizer.setup(linear_function)\n\n# Define a forward pass function taking the data as input.\n# and the linear function as output.\ndef linear_forward(data):\n    return linear_function(data)\n\n\n# Define a training function given the input data, target data,\n# and number of epochs to train over.\ndef linear_train(train_data, train_target,n_epochs=200):\n\n    for _ in range(n_epochs):\n        # Get the result of the forward pass.    \n        output = linear_forward(train_data)\n\n        # Calculate the loss between the training data and target data.\n        loss = F.mean_squared_error(train_target,output)\n\n        # Zero all gradients before updating them.\n        linear_function.zerograds()\n\n        # Calculate and update all gradients.\n        loss.backward()\n\n        # Use the optmizer to move all parameters of the network\n        # to values which will reduce the loss.\n        optimizer.update()\n```\n\n### 绘制训练结果\n\n下面的代码将会把此模型每次训练5遍，并绘制线性链接中当前的参数下的线。你将会看到模型是如何从蓝色的线收敛到红色的线（最终状态）。\n\n```python\n# This code is supplied to visualize your results.\n\nplt.scatter(x,y, alpha =0.5)\n\nfor i in range(150):    \n    linear_train(x_var, y_var, n_epochs=5)\n    y_pred = linear_forward(x_var).data\n    plt.plot(x, y_pred, color=plt.cm.cool(i/150.), alpha = 0.4, lw =3)\n\n\nslope = linear_function.W.data[0,0]\nintercept = linear_function.b.data[0]\nplt.title(\"Final Line: {0:.3}x + {1:.3}\".format(slope, intercept))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809233133.png)\n\n### Note:\n本文译自：[Introduction to Chainer: Neural Networks in Python](http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/)\n\n### 每日一句\nOh là là ! C'est incroyable !（艾玛，真是令人难以置信！）","source":"_posts/Chainer入门教程(上)：在Chainer中做线性回归.md","raw":"---\ntitle: \"Chainer入门教程(上)：在Chainer中做线性回归\"\ndate: 2016-12-14 14:14:00\ntags: [chainer, 入门教程, 机器学习, 神经网络]\ncomments: true\n---\n\n## 简介\n\n神经网络技术在统计建模、数据的转换分类回归等各大领域都有很大的应用空间。但是由于计算本身的复杂性以及早期的计算能力不足，神经网络一直没有得到很大的发展。然而近几年，随着GPU计算的进步，涌现出一大批非常强大而实用的神经网络的训练框架如Caffe、Keras、CUDA convnet、Torch7等。在这片教程中，我们着重介绍另一款灵活又好用的框架：[Chainer](http://chainer.org)的基础使用方法。你可以通过Jupyter Notebook或是其他的python终端来跟进这篇教程。\n\n在这篇教程中，我们将先通过编写一个简单的线性回归器来帮助你入门，然后我们再编写一个用于识别[MNIST](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)手写数字的标准的深度学习模型来让你熟悉编程逻辑。\n\n<!-- more-->\n\n首先我们需要安装几个python包：\n\n```\n#首先更新一下cython，旧的版本可能会导致chainer安装出错\npip install --upgrade cython\n#再把numpy、matplotlib、chainer都安装一下\npip install numpy\npip install matplotlib\npip install chainer\n```\n\n## I. Chainer基础\n\n首先我们要导入这篇教程中要用到的包，关于每个包的作用，之后会有简单的介绍：\n\n```python\n#Matplotlib and Numpy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Chainer Specific\nfrom chainer import FunctionSet, Variable, optimizers, serializers\nimport chainer.functions as F\nimport chainer.links as L\n```\n\n### 了解Chainer中Variables和Functions的特点和作用\n\n首先，我们通过包裹numpy数组定义两个简单的Chainer Variables变量。数组中只有一个值，这样可以方便我们后续做一些标量运算。\n\n```python\n# Create 2 chainer variables then sum their squares\n# and assign it to a third variable.\na = Variable(np.array([3], dtype=np.float32))\nb = Variable(np.array([4], dtype=np.float32))\nc = a**2 + b**2\n```\n\n在Chainer中，Variables对象既是象征的又是数字的。它们在data属性中包含数据的值，但也包含已在它们上执行的操作链的信息。当你需要训练神经网络时，这段操作历史是非常有用的。我们通过调用backward()方法对变量进行BP或（反向模式）自动分化，这给我们提供了所选择的优化与所有更新我们的神经网络所需要的权重信息。\n\n这个过程之所以可以发生，是因为Chainer的Variables对象把所有对其进行操作的函数都进行了存储，分析了其表达式及导数。你将会使用到的一些函数会是带参的，包含在chainer.links中(这里我们作为L导入)。这些函数的参数将在我们的网络的每个训练迭代中更新。其他包含在chainer.functions(这里我们作为F导入)中的函数将会是无参的，只是对变量执行预定义的数学操作。连加减运算都需要调用Chainer Functions，各变量的操作历史都将保存为变量本身的一部分。这使我们能够计算任何变量的相对于任何其他变量的导数。\n\n下面我们来看一个例子，过程如下：\n\n1. 通过调用data属性检查之前定义的变量\n2. 使用backward()方法，对变量c进行反向传播\n3. 通过在变量中存储的grad属性，检查其导数\n\n```python\n#Inspect the value of your variables.\nprint(\"a.data: {0}, b.data: {1}, c.data: {2}\".format(a.data, b.data, c.data))\n```\n\noutput: `a.data: [ 3.], b.data: [ 4.], c.data: [ 25.]`\n\n```python\n#Now call backward() on the sum of squares.\nc.backward()\n#And inspect the gradients.\nprint(\"dc/da = {0}, dc/db = {1}, dc/dc = {2}\".format(a.grad, b.grad, c.grad))\n```\n\noutput: `dc/da = [ 6.], dc/db = [ 8.], dc/dc = [ 1.]`\n\n## II. 在Chainer中做线性回归\n\n现在我们知道了一点关于基础的Chainer在做什么，让我们用它来训练最基本的神经网络、线性回归网络。当然，这里所涉及的最小二乘优化的解决方案，通过正常的等式计算分析可能更有效，但这个过程将展示每个网络的基本组成部分，你可以直接进行训练。\n\n这个网络没有隐藏的节点，只涉及一个输入节点，一个输出节点，和一个连接他们两个的线性函数。\n\n我们将要进行下列步骤：\n\n1. 生成随机的线性数据集\n2. 通过Chainer Link构造一个前向的网络\n3. 构造一个函数来进行网络训练\n\n```python\n# Generate linearly related datasets x and y.\n\nx = 30*np.random.rand(1000).astype(np.float32)\ny = 7*x+10\ny += 10*np.random.randn(1000).astype(np.float32)\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809233029.png)\n\n一般来说，在Chainer中想要让你的结构保持共同的神经网络，你需要构造一个forward函数，这个函数会带入你的不同的带参的link函数并在序列中把所有数据运行一遍。\n\n然后，我们需要写一个train函数，它将在你的所有的数据上把forward函数运行epochs次。并且在每次forward之后，都调用loss/objective函数，然后通过optimizer和通过backward方法算出的梯度来更新权重。\n\nChainer使用者通常会在一开始的时候就定义好Link的层（这里我们只需要一层）。然后他们会通过实例化一个optimizer类来指定要用的优化器。最后，他们会通过调用优化实例的设置方法，告诉optimizer来跟踪和更新指定的模型层的参数，该层将作为一个参数被跟踪。\n\n```python\n# Setup linear link from one variable to another.\n\nlinear_function = L.Linear(1,1)\n\n# Set x and y as chainer variables, make sure to reshape\n# them to give one value at a time.\nx_var = Variable(x.reshape(1000,-1))\ny_var = Variable(y.reshape(1000,-1))\n\n# Setup the optimizer.\noptimizer = optimizers.MomentumSGD(lr=0.001)\noptimizer.setup(linear_function)\n\n# Define a forward pass function taking the data as input.\n# and the linear function as output.\ndef linear_forward(data):\n    return linear_function(data)\n\n\n# Define a training function given the input data, target data,\n# and number of epochs to train over.\ndef linear_train(train_data, train_target,n_epochs=200):\n\n    for _ in range(n_epochs):\n        # Get the result of the forward pass.    \n        output = linear_forward(train_data)\n\n        # Calculate the loss between the training data and target data.\n        loss = F.mean_squared_error(train_target,output)\n\n        # Zero all gradients before updating them.\n        linear_function.zerograds()\n\n        # Calculate and update all gradients.\n        loss.backward()\n\n        # Use the optmizer to move all parameters of the network\n        # to values which will reduce the loss.\n        optimizer.update()\n```\n\n### 绘制训练结果\n\n下面的代码将会把此模型每次训练5遍，并绘制线性链接中当前的参数下的线。你将会看到模型是如何从蓝色的线收敛到红色的线（最终状态）。\n\n```python\n# This code is supplied to visualize your results.\n\nplt.scatter(x,y, alpha =0.5)\n\nfor i in range(150):    \n    linear_train(x_var, y_var, n_epochs=5)\n    y_pred = linear_forward(x_var).data\n    plt.plot(x, y_pred, color=plt.cm.cool(i/150.), alpha = 0.4, lw =3)\n\n\nslope = linear_function.W.data[0,0]\nintercept = linear_function.b.data[0]\nplt.title(\"Final Line: {0:.3}x + {1:.3}\".format(slope, intercept))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809233133.png)\n\n### Note:\n本文译自：[Introduction to Chainer: Neural Networks in Python](http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/)\n\n### 每日一句\nOh là là ! C'est incroyable !（艾玛，真是令人难以置信！）","slug":"Chainer入门教程(上)：在Chainer中做线性回归","published":1,"updated":"2018-08-09T15:32:06.769Z","layout":"post","photos":[],"link":"","_id":"cjmlcpipv0000t7x0a36oz7ae","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>神经网络技术在统计建模、数据的转换分类回归等各大领域都有很大的应用空间。但是由于计算本身的复杂性以及早期的计算能力不足，神经网络一直没有得到很大的发展。然而近几年，随着GPU计算的进步，涌现出一大批非常强大而实用的神经网络的训练框架如Caffe、Keras、CUDA convnet、Torch7等。在这片教程中，我们着重介绍另一款灵活又好用的框架：<a href=\"http://chainer.org\" target=\"_blank\" rel=\"noopener\">Chainer</a>的基础使用方法。你可以通过Jupyter Notebook或是其他的python终端来跟进这篇教程。</p>\n<p>在这篇教程中，我们将先通过编写一个简单的线性回归器来帮助你入门，然后我们再编写一个用于识别<a href=\"http://colah.github.io/posts/2014-10-Visualizing-MNIST/\" target=\"_blank\" rel=\"noopener\">MNIST</a>手写数字的标准的深度学习模型来让你熟悉编程逻辑。</p>\n<a id=\"more\"></a>\n<p>首先我们需要安装几个python包：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#首先更新一下cython，旧的版本可能会导致chainer安装出错</span><br><span class=\"line\">pip install --upgrade cython</span><br><span class=\"line\">#再把numpy、matplotlib、chainer都安装一下</span><br><span class=\"line\">pip install numpy</span><br><span class=\"line\">pip install matplotlib</span><br><span class=\"line\">pip install chainer</span><br></pre></td></tr></table></figure>\n<h2 id=\"I-Chainer基础\"><a href=\"#I-Chainer基础\" class=\"headerlink\" title=\"I. Chainer基础\"></a>I. Chainer基础</h2><p>首先我们要导入这篇教程中要用到的包，关于每个包的作用，之后会有简单的介绍：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Matplotlib and Numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Chainer Specific</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> chainer <span class=\"keyword\">import</span> FunctionSet, Variable, optimizers, serializers</span><br><span class=\"line\"><span class=\"keyword\">import</span> chainer.functions <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> chainer.links <span class=\"keyword\">as</span> L</span><br></pre></td></tr></table></figure>\n<h3 id=\"了解Chainer中Variables和Functions的特点和作用\"><a href=\"#了解Chainer中Variables和Functions的特点和作用\" class=\"headerlink\" title=\"了解Chainer中Variables和Functions的特点和作用\"></a>了解Chainer中Variables和Functions的特点和作用</h3><p>首先，我们通过包裹numpy数组定义两个简单的Chainer Variables变量。数组中只有一个值，这样可以方便我们后续做一些标量运算。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Create 2 chainer variables then sum their squares</span></span><br><span class=\"line\"><span class=\"comment\"># and assign it to a third variable.</span></span><br><span class=\"line\">a = Variable(np.array([<span class=\"number\">3</span>], dtype=np.float32))</span><br><span class=\"line\">b = Variable(np.array([<span class=\"number\">4</span>], dtype=np.float32))</span><br><span class=\"line\">c = a**<span class=\"number\">2</span> + b**<span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n<p>在Chainer中，Variables对象既是象征的又是数字的。它们在data属性中包含数据的值，但也包含已在它们上执行的操作链的信息。当你需要训练神经网络时，这段操作历史是非常有用的。我们通过调用backward()方法对变量进行BP或（反向模式）自动分化，这给我们提供了所选择的优化与所有更新我们的神经网络所需要的权重信息。</p>\n<p>这个过程之所以可以发生，是因为Chainer的Variables对象把所有对其进行操作的函数都进行了存储，分析了其表达式及导数。你将会使用到的一些函数会是带参的，包含在chainer.links中(这里我们作为L导入)。这些函数的参数将在我们的网络的每个训练迭代中更新。其他包含在chainer.functions(这里我们作为F导入)中的函数将会是无参的，只是对变量执行预定义的数学操作。连加减运算都需要调用Chainer Functions，各变量的操作历史都将保存为变量本身的一部分。这使我们能够计算任何变量的相对于任何其他变量的导数。</p>\n<p>下面我们来看一个例子，过程如下：</p>\n<ol>\n<li>通过调用data属性检查之前定义的变量</li>\n<li>使用backward()方法，对变量c进行反向传播</li>\n<li>通过在变量中存储的grad属性，检查其导数</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Inspect the value of your variables.</span></span><br><span class=\"line\">print(<span class=\"string\">\"a.data: &#123;0&#125;, b.data: &#123;1&#125;, c.data: &#123;2&#125;\"</span>.format(a.data, b.data, c.data))</span><br></pre></td></tr></table></figure>\n<p>output: <code>a.data: [ 3.], b.data: [ 4.], c.data: [ 25.]</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Now call backward() on the sum of squares.</span></span><br><span class=\"line\">c.backward()</span><br><span class=\"line\"><span class=\"comment\">#And inspect the gradients.</span></span><br><span class=\"line\">print(<span class=\"string\">\"dc/da = &#123;0&#125;, dc/db = &#123;1&#125;, dc/dc = &#123;2&#125;\"</span>.format(a.grad, b.grad, c.grad))</span><br></pre></td></tr></table></figure>\n<p>output: <code>dc/da = [ 6.], dc/db = [ 8.], dc/dc = [ 1.]</code></p>\n<h2 id=\"II-在Chainer中做线性回归\"><a href=\"#II-在Chainer中做线性回归\" class=\"headerlink\" title=\"II. 在Chainer中做线性回归\"></a>II. 在Chainer中做线性回归</h2><p>现在我们知道了一点关于基础的Chainer在做什么，让我们用它来训练最基本的神经网络、线性回归网络。当然，这里所涉及的最小二乘优化的解决方案，通过正常的等式计算分析可能更有效，但这个过程将展示每个网络的基本组成部分，你可以直接进行训练。</p>\n<p>这个网络没有隐藏的节点，只涉及一个输入节点，一个输出节点，和一个连接他们两个的线性函数。</p>\n<p>我们将要进行下列步骤：</p>\n<ol>\n<li>生成随机的线性数据集</li>\n<li>通过Chainer Link构造一个前向的网络</li>\n<li>构造一个函数来进行网络训练</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Generate linearly related datasets x and y.</span></span><br><span class=\"line\"></span><br><span class=\"line\">x = <span class=\"number\">30</span>*np.random.rand(<span class=\"number\">1000</span>).astype(np.float32)</span><br><span class=\"line\">y = <span class=\"number\">7</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">y += <span class=\"number\">10</span>*np.random.randn(<span class=\"number\">1000</span>).astype(np.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(x,y)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809233029.png\" alt=\"\"></p>\n<p>一般来说，在Chainer中想要让你的结构保持共同的神经网络，你需要构造一个forward函数，这个函数会带入你的不同的带参的link函数并在序列中把所有数据运行一遍。</p>\n<p>然后，我们需要写一个train函数，它将在你的所有的数据上把forward函数运行epochs次。并且在每次forward之后，都调用loss/objective函数，然后通过optimizer和通过backward方法算出的梯度来更新权重。</p>\n<p>Chainer使用者通常会在一开始的时候就定义好Link的层（这里我们只需要一层）。然后他们会通过实例化一个optimizer类来指定要用的优化器。最后，他们会通过调用优化实例的设置方法，告诉optimizer来跟踪和更新指定的模型层的参数，该层将作为一个参数被跟踪。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Setup linear link from one variable to another.</span></span><br><span class=\"line\"></span><br><span class=\"line\">linear_function = L.Linear(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Set x and y as chainer variables, make sure to reshape</span></span><br><span class=\"line\"><span class=\"comment\"># them to give one value at a time.</span></span><br><span class=\"line\">x_var = Variable(x.reshape(<span class=\"number\">1000</span>,<span class=\"number\">-1</span>))</span><br><span class=\"line\">y_var = Variable(y.reshape(<span class=\"number\">1000</span>,<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Setup the optimizer.</span></span><br><span class=\"line\">optimizer = optimizers.MomentumSGD(lr=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">optimizer.setup(linear_function)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Define a forward pass function taking the data as input.</span></span><br><span class=\"line\"><span class=\"comment\"># and the linear function as output.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(data)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> linear_function(data)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Define a training function given the input data, target data,</span></span><br><span class=\"line\"><span class=\"comment\"># and number of epochs to train over.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_train</span><span class=\"params\">(train_data, train_target,n_epochs=<span class=\"number\">200</span>)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(n_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># Get the result of the forward pass.    </span></span><br><span class=\"line\">        output = linear_forward(train_data)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate the loss between the training data and target data.</span></span><br><span class=\"line\">        loss = F.mean_squared_error(train_target,output)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Zero all gradients before updating them.</span></span><br><span class=\"line\">        linear_function.zerograds()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate and update all gradients.</span></span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Use the optmizer to move all parameters of the network</span></span><br><span class=\"line\">        <span class=\"comment\"># to values which will reduce the loss.</span></span><br><span class=\"line\">        optimizer.update()</span><br></pre></td></tr></table></figure>\n<h3 id=\"绘制训练结果\"><a href=\"#绘制训练结果\" class=\"headerlink\" title=\"绘制训练结果\"></a>绘制训练结果</h3><p>下面的代码将会把此模型每次训练5遍，并绘制线性链接中当前的参数下的线。你将会看到模型是如何从蓝色的线收敛到红色的线（最终状态）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This code is supplied to visualize your results.</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(x,y, alpha =<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">150</span>):    </span><br><span class=\"line\">    linear_train(x_var, y_var, n_epochs=<span class=\"number\">5</span>)</span><br><span class=\"line\">    y_pred = linear_forward(x_var).data</span><br><span class=\"line\">    plt.plot(x, y_pred, color=plt.cm.cool(i/<span class=\"number\">150.</span>), alpha = <span class=\"number\">0.4</span>, lw =<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">slope = linear_function.W.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">intercept = linear_function.b.data[<span class=\"number\">0</span>]</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Final Line: &#123;0:.3&#125;x + &#123;1:.3&#125;\"</span>.format(slope, intercept))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809233133.png\" alt=\"\"></p>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note:\"></a>Note:</h3><p>本文译自：<a href=\"http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/\" target=\"_blank\" rel=\"noopener\">Introduction to Chainer: Neural Networks in Python</a></p>\n<h3 id=\"每日一句\"><a href=\"#每日一句\" class=\"headerlink\" title=\"每日一句\"></a>每日一句</h3><p>Oh là là ! C’est incroyable !（艾玛，真是令人难以置信！）</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>神经网络技术在统计建模、数据的转换分类回归等各大领域都有很大的应用空间。但是由于计算本身的复杂性以及早期的计算能力不足，神经网络一直没有得到很大的发展。然而近几年，随着GPU计算的进步，涌现出一大批非常强大而实用的神经网络的训练框架如Caffe、Keras、CUDA convnet、Torch7等。在这片教程中，我们着重介绍另一款灵活又好用的框架：<a href=\"http://chainer.org\" target=\"_blank\" rel=\"noopener\">Chainer</a>的基础使用方法。你可以通过Jupyter Notebook或是其他的python终端来跟进这篇教程。</p>\n<p>在这篇教程中，我们将先通过编写一个简单的线性回归器来帮助你入门，然后我们再编写一个用于识别<a href=\"http://colah.github.io/posts/2014-10-Visualizing-MNIST/\" target=\"_blank\" rel=\"noopener\">MNIST</a>手写数字的标准的深度学习模型来让你熟悉编程逻辑。</p>","more":"<p>首先我们需要安装几个python包：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#首先更新一下cython，旧的版本可能会导致chainer安装出错</span><br><span class=\"line\">pip install --upgrade cython</span><br><span class=\"line\">#再把numpy、matplotlib、chainer都安装一下</span><br><span class=\"line\">pip install numpy</span><br><span class=\"line\">pip install matplotlib</span><br><span class=\"line\">pip install chainer</span><br></pre></td></tr></table></figure>\n<h2 id=\"I-Chainer基础\"><a href=\"#I-Chainer基础\" class=\"headerlink\" title=\"I. Chainer基础\"></a>I. Chainer基础</h2><p>首先我们要导入这篇教程中要用到的包，关于每个包的作用，之后会有简单的介绍：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Matplotlib and Numpy</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Chainer Specific</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> chainer <span class=\"keyword\">import</span> FunctionSet, Variable, optimizers, serializers</span><br><span class=\"line\"><span class=\"keyword\">import</span> chainer.functions <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> chainer.links <span class=\"keyword\">as</span> L</span><br></pre></td></tr></table></figure>\n<h3 id=\"了解Chainer中Variables和Functions的特点和作用\"><a href=\"#了解Chainer中Variables和Functions的特点和作用\" class=\"headerlink\" title=\"了解Chainer中Variables和Functions的特点和作用\"></a>了解Chainer中Variables和Functions的特点和作用</h3><p>首先，我们通过包裹numpy数组定义两个简单的Chainer Variables变量。数组中只有一个值，这样可以方便我们后续做一些标量运算。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Create 2 chainer variables then sum their squares</span></span><br><span class=\"line\"><span class=\"comment\"># and assign it to a third variable.</span></span><br><span class=\"line\">a = Variable(np.array([<span class=\"number\">3</span>], dtype=np.float32))</span><br><span class=\"line\">b = Variable(np.array([<span class=\"number\">4</span>], dtype=np.float32))</span><br><span class=\"line\">c = a**<span class=\"number\">2</span> + b**<span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n<p>在Chainer中，Variables对象既是象征的又是数字的。它们在data属性中包含数据的值，但也包含已在它们上执行的操作链的信息。当你需要训练神经网络时，这段操作历史是非常有用的。我们通过调用backward()方法对变量进行BP或（反向模式）自动分化，这给我们提供了所选择的优化与所有更新我们的神经网络所需要的权重信息。</p>\n<p>这个过程之所以可以发生，是因为Chainer的Variables对象把所有对其进行操作的函数都进行了存储，分析了其表达式及导数。你将会使用到的一些函数会是带参的，包含在chainer.links中(这里我们作为L导入)。这些函数的参数将在我们的网络的每个训练迭代中更新。其他包含在chainer.functions(这里我们作为F导入)中的函数将会是无参的，只是对变量执行预定义的数学操作。连加减运算都需要调用Chainer Functions，各变量的操作历史都将保存为变量本身的一部分。这使我们能够计算任何变量的相对于任何其他变量的导数。</p>\n<p>下面我们来看一个例子，过程如下：</p>\n<ol>\n<li>通过调用data属性检查之前定义的变量</li>\n<li>使用backward()方法，对变量c进行反向传播</li>\n<li>通过在变量中存储的grad属性，检查其导数</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Inspect the value of your variables.</span></span><br><span class=\"line\">print(<span class=\"string\">\"a.data: &#123;0&#125;, b.data: &#123;1&#125;, c.data: &#123;2&#125;\"</span>.format(a.data, b.data, c.data))</span><br></pre></td></tr></table></figure>\n<p>output: <code>a.data: [ 3.], b.data: [ 4.], c.data: [ 25.]</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Now call backward() on the sum of squares.</span></span><br><span class=\"line\">c.backward()</span><br><span class=\"line\"><span class=\"comment\">#And inspect the gradients.</span></span><br><span class=\"line\">print(<span class=\"string\">\"dc/da = &#123;0&#125;, dc/db = &#123;1&#125;, dc/dc = &#123;2&#125;\"</span>.format(a.grad, b.grad, c.grad))</span><br></pre></td></tr></table></figure>\n<p>output: <code>dc/da = [ 6.], dc/db = [ 8.], dc/dc = [ 1.]</code></p>\n<h2 id=\"II-在Chainer中做线性回归\"><a href=\"#II-在Chainer中做线性回归\" class=\"headerlink\" title=\"II. 在Chainer中做线性回归\"></a>II. 在Chainer中做线性回归</h2><p>现在我们知道了一点关于基础的Chainer在做什么，让我们用它来训练最基本的神经网络、线性回归网络。当然，这里所涉及的最小二乘优化的解决方案，通过正常的等式计算分析可能更有效，但这个过程将展示每个网络的基本组成部分，你可以直接进行训练。</p>\n<p>这个网络没有隐藏的节点，只涉及一个输入节点，一个输出节点，和一个连接他们两个的线性函数。</p>\n<p>我们将要进行下列步骤：</p>\n<ol>\n<li>生成随机的线性数据集</li>\n<li>通过Chainer Link构造一个前向的网络</li>\n<li>构造一个函数来进行网络训练</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Generate linearly related datasets x and y.</span></span><br><span class=\"line\"></span><br><span class=\"line\">x = <span class=\"number\">30</span>*np.random.rand(<span class=\"number\">1000</span>).astype(np.float32)</span><br><span class=\"line\">y = <span class=\"number\">7</span>*x+<span class=\"number\">10</span></span><br><span class=\"line\">y += <span class=\"number\">10</span>*np.random.randn(<span class=\"number\">1000</span>).astype(np.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(x,y)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809233029.png\" alt=\"\"></p>\n<p>一般来说，在Chainer中想要让你的结构保持共同的神经网络，你需要构造一个forward函数，这个函数会带入你的不同的带参的link函数并在序列中把所有数据运行一遍。</p>\n<p>然后，我们需要写一个train函数，它将在你的所有的数据上把forward函数运行epochs次。并且在每次forward之后，都调用loss/objective函数，然后通过optimizer和通过backward方法算出的梯度来更新权重。</p>\n<p>Chainer使用者通常会在一开始的时候就定义好Link的层（这里我们只需要一层）。然后他们会通过实例化一个optimizer类来指定要用的优化器。最后，他们会通过调用优化实例的设置方法，告诉optimizer来跟踪和更新指定的模型层的参数，该层将作为一个参数被跟踪。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Setup linear link from one variable to another.</span></span><br><span class=\"line\"></span><br><span class=\"line\">linear_function = L.Linear(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Set x and y as chainer variables, make sure to reshape</span></span><br><span class=\"line\"><span class=\"comment\"># them to give one value at a time.</span></span><br><span class=\"line\">x_var = Variable(x.reshape(<span class=\"number\">1000</span>,<span class=\"number\">-1</span>))</span><br><span class=\"line\">y_var = Variable(y.reshape(<span class=\"number\">1000</span>,<span class=\"number\">-1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Setup the optimizer.</span></span><br><span class=\"line\">optimizer = optimizers.MomentumSGD(lr=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">optimizer.setup(linear_function)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Define a forward pass function taking the data as input.</span></span><br><span class=\"line\"><span class=\"comment\"># and the linear function as output.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(data)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> linear_function(data)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Define a training function given the input data, target data,</span></span><br><span class=\"line\"><span class=\"comment\"># and number of epochs to train over.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_train</span><span class=\"params\">(train_data, train_target,n_epochs=<span class=\"number\">200</span>)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(n_epochs):</span><br><span class=\"line\">        <span class=\"comment\"># Get the result of the forward pass.    </span></span><br><span class=\"line\">        output = linear_forward(train_data)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate the loss between the training data and target data.</span></span><br><span class=\"line\">        loss = F.mean_squared_error(train_target,output)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Zero all gradients before updating them.</span></span><br><span class=\"line\">        linear_function.zerograds()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate and update all gradients.</span></span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Use the optmizer to move all parameters of the network</span></span><br><span class=\"line\">        <span class=\"comment\"># to values which will reduce the loss.</span></span><br><span class=\"line\">        optimizer.update()</span><br></pre></td></tr></table></figure>\n<h3 id=\"绘制训练结果\"><a href=\"#绘制训练结果\" class=\"headerlink\" title=\"绘制训练结果\"></a>绘制训练结果</h3><p>下面的代码将会把此模型每次训练5遍，并绘制线性链接中当前的参数下的线。你将会看到模型是如何从蓝色的线收敛到红色的线（最终状态）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This code is supplied to visualize your results.</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(x,y, alpha =<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">150</span>):    </span><br><span class=\"line\">    linear_train(x_var, y_var, n_epochs=<span class=\"number\">5</span>)</span><br><span class=\"line\">    y_pred = linear_forward(x_var).data</span><br><span class=\"line\">    plt.plot(x, y_pred, color=plt.cm.cool(i/<span class=\"number\">150.</span>), alpha = <span class=\"number\">0.4</span>, lw =<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">slope = linear_function.W.data[<span class=\"number\">0</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\">intercept = linear_function.b.data[<span class=\"number\">0</span>]</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Final Line: &#123;0:.3&#125;x + &#123;1:.3&#125;\"</span>.format(slope, intercept))</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">'x'</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">'y'</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809233133.png\" alt=\"\"></p>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note:\"></a>Note:</h3><p>本文译自：<a href=\"http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/\" target=\"_blank\" rel=\"noopener\">Introduction to Chainer: Neural Networks in Python</a></p>\n<h3 id=\"每日一句\"><a href=\"#每日一句\" class=\"headerlink\" title=\"每日一句\"></a>每日一句</h3><p>Oh là là ! C’est incroyable !（艾玛，真是令人难以置信！）</p>"},{"title":"Mac如何卸载pandoc","date":"2018-09-28T01:44:34.000Z","_content":"\n# 将以下脚本保存到本地，命名为**uninstall-pandoc.pl**\n\n```\n#!/usr/bin/perl\n\n# Script to remove all files installed by the macOS pandoc installer\n# and unregister the package.  Modified from a script contributed\n# by Daniel T. Staal.\n\nuse warnings;\nuse strict;\n\nuse File::Spec;\n\n# The main info: this is the list of files to remove and the pkg_id.\nmy $pkg_id    = 'net.johnmacfarlane.pandoc';\n\n# Find which, if any, volume Pandoc is installed on.\nmy $volume;\n\n# First check /, then other volumes on the box.\nmy $cur_test = `pkgutil --pkgs=$pkg_id`;\nif ( $cur_test =~ m/$pkg_id/ ) {\n    $volume = '/';\n} else {\n    opendir( my $dh, '/Volumes' ) or die \"Can't list Volumes: $!\\n\";\n    foreach my $dir ( readdir($dh) ) {\n      next if $dir =~ m/^\\./;    # Skip dotfiles.\n\n      my $path = File::Spec->rel2abs( $dir, '/Volumes' );\n      next if !( -d $path );     # Skip anything that isn't a directory.\n\n      my $cur_test = `pkgutil --pkgs=$pkg_id --volume '$path'`;\n      if ( $cur_test =~ m/$pkg_id/ ) {\n          $volume = $path;\n          last;\n      }\n    }\n}\n\ndie \"Pandoc not installed.\\n\" if !( defined($volume) );\n\n# Get the list of files to remove.\nmy @pkg_files = `pkgutil --volume '$volume' --only-files --files '$pkg_id'`;\n@pkg_files = map { chomp; File::Spec->rel2abs($_, $volume) } @pkg_files;\n\n# Confirm uninistall with the user.\nprint \"The following files will be deleted:\\n\\n\";\nprint join(\"\\n\", @pkg_files);\nprint \"\\n\\n\";\nprint \"Do you want to proceed and uninstall pandoc (Y/N)?\";\nmy $input = <STDIN>;\n\nif ($input =~ m/^[Yy]/) {\n\n    # Actually remove the files.\n    foreach my $file (@pkg_files) {\n        if ( -e $file ) {\n            if ( system( 'sudo', 'rm', $file ) == 0 ) {\n                warn \"Deleted $file\\n\";\n            } else {\n                warn \"Unable to delete $file: $?\\n\";\n                die \"Aborting Uninstall.\\n\";\n            }\n        }  else {\n            warn \"File $file does not exist.  Skipping.\\n\";\n        }\n    }\n\n    # Clean up the install.\n    if (system('sudo', 'pkgutil', '--forget', $pkg_id, '--volume', $volume) != 0) {\n        die \"Unable to clean up install: $?\\n\";\n    }\n\n} else {\n\n   print \"OK, aborting uninstall.\\n\";\n   exit;\n}\n\nprint \"Pandoc has been successfully uninstalled.\\n\";\nexit;\n```\n\n# 在uninstall-pandoc.pl目录下运行\n\n> perl uninstall-pandoc.pl\n\n# 附：官方链接和截图\n\n官方链接：http://pandoc.org/installing.html#macos\n\n截图：\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180928095031.png)","source":"_posts/Mac如何卸载pandoc.md","raw":"---\ntitle: Mac如何卸载pandoc\ndate: 2018-09-28 09:44:34\ntags: [mac, 卸载, pandoc]\n---\n\n# 将以下脚本保存到本地，命名为**uninstall-pandoc.pl**\n\n```\n#!/usr/bin/perl\n\n# Script to remove all files installed by the macOS pandoc installer\n# and unregister the package.  Modified from a script contributed\n# by Daniel T. Staal.\n\nuse warnings;\nuse strict;\n\nuse File::Spec;\n\n# The main info: this is the list of files to remove and the pkg_id.\nmy $pkg_id    = 'net.johnmacfarlane.pandoc';\n\n# Find which, if any, volume Pandoc is installed on.\nmy $volume;\n\n# First check /, then other volumes on the box.\nmy $cur_test = `pkgutil --pkgs=$pkg_id`;\nif ( $cur_test =~ m/$pkg_id/ ) {\n    $volume = '/';\n} else {\n    opendir( my $dh, '/Volumes' ) or die \"Can't list Volumes: $!\\n\";\n    foreach my $dir ( readdir($dh) ) {\n      next if $dir =~ m/^\\./;    # Skip dotfiles.\n\n      my $path = File::Spec->rel2abs( $dir, '/Volumes' );\n      next if !( -d $path );     # Skip anything that isn't a directory.\n\n      my $cur_test = `pkgutil --pkgs=$pkg_id --volume '$path'`;\n      if ( $cur_test =~ m/$pkg_id/ ) {\n          $volume = $path;\n          last;\n      }\n    }\n}\n\ndie \"Pandoc not installed.\\n\" if !( defined($volume) );\n\n# Get the list of files to remove.\nmy @pkg_files = `pkgutil --volume '$volume' --only-files --files '$pkg_id'`;\n@pkg_files = map { chomp; File::Spec->rel2abs($_, $volume) } @pkg_files;\n\n# Confirm uninistall with the user.\nprint \"The following files will be deleted:\\n\\n\";\nprint join(\"\\n\", @pkg_files);\nprint \"\\n\\n\";\nprint \"Do you want to proceed and uninstall pandoc (Y/N)?\";\nmy $input = <STDIN>;\n\nif ($input =~ m/^[Yy]/) {\n\n    # Actually remove the files.\n    foreach my $file (@pkg_files) {\n        if ( -e $file ) {\n            if ( system( 'sudo', 'rm', $file ) == 0 ) {\n                warn \"Deleted $file\\n\";\n            } else {\n                warn \"Unable to delete $file: $?\\n\";\n                die \"Aborting Uninstall.\\n\";\n            }\n        }  else {\n            warn \"File $file does not exist.  Skipping.\\n\";\n        }\n    }\n\n    # Clean up the install.\n    if (system('sudo', 'pkgutil', '--forget', $pkg_id, '--volume', $volume) != 0) {\n        die \"Unable to clean up install: $?\\n\";\n    }\n\n} else {\n\n   print \"OK, aborting uninstall.\\n\";\n   exit;\n}\n\nprint \"Pandoc has been successfully uninstalled.\\n\";\nexit;\n```\n\n# 在uninstall-pandoc.pl目录下运行\n\n> perl uninstall-pandoc.pl\n\n# 附：官方链接和截图\n\n官方链接：http://pandoc.org/installing.html#macos\n\n截图：\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180928095031.png)","slug":"Mac如何卸载pandoc","published":1,"updated":"2018-09-28T01:50:55.195Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiq00001t7x04bjcai3i","content":"<h1 id=\"将以下脚本保存到本地，命名为uninstall-pandoc-pl\"><a href=\"#将以下脚本保存到本地，命名为uninstall-pandoc-pl\" class=\"headerlink\" title=\"将以下脚本保存到本地，命名为uninstall-pandoc.pl\"></a>将以下脚本保存到本地，命名为<strong>uninstall-pandoc.pl</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/usr/bin/perl</span><br><span class=\"line\"></span><br><span class=\"line\"># Script to remove all files installed by the macOS pandoc installer</span><br><span class=\"line\"># and unregister the package.  Modified from a script contributed</span><br><span class=\"line\"># by Daniel T. Staal.</span><br><span class=\"line\"></span><br><span class=\"line\">use warnings;</span><br><span class=\"line\">use strict;</span><br><span class=\"line\"></span><br><span class=\"line\">use File::Spec;</span><br><span class=\"line\"></span><br><span class=\"line\"># The main info: this is the list of files to remove and the pkg_id.</span><br><span class=\"line\">my $pkg_id    = &apos;net.johnmacfarlane.pandoc&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\"># Find which, if any, volume Pandoc is installed on.</span><br><span class=\"line\">my $volume;</span><br><span class=\"line\"></span><br><span class=\"line\"># First check /, then other volumes on the box.</span><br><span class=\"line\">my $cur_test = `pkgutil --pkgs=$pkg_id`;</span><br><span class=\"line\">if ( $cur_test =~ m/$pkg_id/ ) &#123;</span><br><span class=\"line\">    $volume = &apos;/&apos;;</span><br><span class=\"line\">&#125; else &#123;</span><br><span class=\"line\">    opendir( my $dh, &apos;/Volumes&apos; ) or die &quot;Can&apos;t list Volumes: $!\\n&quot;;</span><br><span class=\"line\">    foreach my $dir ( readdir($dh) ) &#123;</span><br><span class=\"line\">      next if $dir =~ m/^\\./;    # Skip dotfiles.</span><br><span class=\"line\"></span><br><span class=\"line\">      my $path = File::Spec-&gt;rel2abs( $dir, &apos;/Volumes&apos; );</span><br><span class=\"line\">      next if !( -d $path );     # Skip anything that isn&apos;t a directory.</span><br><span class=\"line\"></span><br><span class=\"line\">      my $cur_test = `pkgutil --pkgs=$pkg_id --volume &apos;$path&apos;`;</span><br><span class=\"line\">      if ( $cur_test =~ m/$pkg_id/ ) &#123;</span><br><span class=\"line\">          $volume = $path;</span><br><span class=\"line\">          last;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">die &quot;Pandoc not installed.\\n&quot; if !( defined($volume) );</span><br><span class=\"line\"></span><br><span class=\"line\"># Get the list of files to remove.</span><br><span class=\"line\">my @pkg_files = `pkgutil --volume &apos;$volume&apos; --only-files --files &apos;$pkg_id&apos;`;</span><br><span class=\"line\">@pkg_files = map &#123; chomp; File::Spec-&gt;rel2abs($_, $volume) &#125; @pkg_files;</span><br><span class=\"line\"></span><br><span class=\"line\"># Confirm uninistall with the user.</span><br><span class=\"line\">print &quot;The following files will be deleted:\\n\\n&quot;;</span><br><span class=\"line\">print join(&quot;\\n&quot;, @pkg_files);</span><br><span class=\"line\">print &quot;\\n\\n&quot;;</span><br><span class=\"line\">print &quot;Do you want to proceed and uninstall pandoc (Y/N)?&quot;;</span><br><span class=\"line\">my $input = &lt;STDIN&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">if ($input =~ m/^[Yy]/) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    # Actually remove the files.</span><br><span class=\"line\">    foreach my $file (@pkg_files) &#123;</span><br><span class=\"line\">        if ( -e $file ) &#123;</span><br><span class=\"line\">            if ( system( &apos;sudo&apos;, &apos;rm&apos;, $file ) == 0 ) &#123;</span><br><span class=\"line\">                warn &quot;Deleted $file\\n&quot;;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                warn &quot;Unable to delete $file: $?\\n&quot;;</span><br><span class=\"line\">                die &quot;Aborting Uninstall.\\n&quot;;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;  else &#123;</span><br><span class=\"line\">            warn &quot;File $file does not exist.  Skipping.\\n&quot;;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    # Clean up the install.</span><br><span class=\"line\">    if (system(&apos;sudo&apos;, &apos;pkgutil&apos;, &apos;--forget&apos;, $pkg_id, &apos;--volume&apos;, $volume) != 0) &#123;</span><br><span class=\"line\">        die &quot;Unable to clean up install: $?\\n&quot;;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; else &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   print &quot;OK, aborting uninstall.\\n&quot;;</span><br><span class=\"line\">   exit;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">print &quot;Pandoc has been successfully uninstalled.\\n&quot;;</span><br><span class=\"line\">exit;</span><br></pre></td></tr></table></figure>\n<h1 id=\"在uninstall-pandoc-pl目录下运行\"><a href=\"#在uninstall-pandoc-pl目录下运行\" class=\"headerlink\" title=\"在uninstall-pandoc.pl目录下运行\"></a>在uninstall-pandoc.pl目录下运行</h1><blockquote>\n<p>perl uninstall-pandoc.pl</p>\n</blockquote>\n<h1 id=\"附：官方链接和截图\"><a href=\"#附：官方链接和截图\" class=\"headerlink\" title=\"附：官方链接和截图\"></a>附：官方链接和截图</h1><p>官方链接：<a href=\"http://pandoc.org/installing.html#macos\" target=\"_blank\" rel=\"noopener\">http://pandoc.org/installing.html#macos</a></p>\n<p>截图：<br><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180928095031.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"将以下脚本保存到本地，命名为uninstall-pandoc-pl\"><a href=\"#将以下脚本保存到本地，命名为uninstall-pandoc-pl\" class=\"headerlink\" title=\"将以下脚本保存到本地，命名为uninstall-pandoc.pl\"></a>将以下脚本保存到本地，命名为<strong>uninstall-pandoc.pl</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/usr/bin/perl</span><br><span class=\"line\"></span><br><span class=\"line\"># Script to remove all files installed by the macOS pandoc installer</span><br><span class=\"line\"># and unregister the package.  Modified from a script contributed</span><br><span class=\"line\"># by Daniel T. Staal.</span><br><span class=\"line\"></span><br><span class=\"line\">use warnings;</span><br><span class=\"line\">use strict;</span><br><span class=\"line\"></span><br><span class=\"line\">use File::Spec;</span><br><span class=\"line\"></span><br><span class=\"line\"># The main info: this is the list of files to remove and the pkg_id.</span><br><span class=\"line\">my $pkg_id    = &apos;net.johnmacfarlane.pandoc&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\"># Find which, if any, volume Pandoc is installed on.</span><br><span class=\"line\">my $volume;</span><br><span class=\"line\"></span><br><span class=\"line\"># First check /, then other volumes on the box.</span><br><span class=\"line\">my $cur_test = `pkgutil --pkgs=$pkg_id`;</span><br><span class=\"line\">if ( $cur_test =~ m/$pkg_id/ ) &#123;</span><br><span class=\"line\">    $volume = &apos;/&apos;;</span><br><span class=\"line\">&#125; else &#123;</span><br><span class=\"line\">    opendir( my $dh, &apos;/Volumes&apos; ) or die &quot;Can&apos;t list Volumes: $!\\n&quot;;</span><br><span class=\"line\">    foreach my $dir ( readdir($dh) ) &#123;</span><br><span class=\"line\">      next if $dir =~ m/^\\./;    # Skip dotfiles.</span><br><span class=\"line\"></span><br><span class=\"line\">      my $path = File::Spec-&gt;rel2abs( $dir, &apos;/Volumes&apos; );</span><br><span class=\"line\">      next if !( -d $path );     # Skip anything that isn&apos;t a directory.</span><br><span class=\"line\"></span><br><span class=\"line\">      my $cur_test = `pkgutil --pkgs=$pkg_id --volume &apos;$path&apos;`;</span><br><span class=\"line\">      if ( $cur_test =~ m/$pkg_id/ ) &#123;</span><br><span class=\"line\">          $volume = $path;</span><br><span class=\"line\">          last;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">die &quot;Pandoc not installed.\\n&quot; if !( defined($volume) );</span><br><span class=\"line\"></span><br><span class=\"line\"># Get the list of files to remove.</span><br><span class=\"line\">my @pkg_files = `pkgutil --volume &apos;$volume&apos; --only-files --files &apos;$pkg_id&apos;`;</span><br><span class=\"line\">@pkg_files = map &#123; chomp; File::Spec-&gt;rel2abs($_, $volume) &#125; @pkg_files;</span><br><span class=\"line\"></span><br><span class=\"line\"># Confirm uninistall with the user.</span><br><span class=\"line\">print &quot;The following files will be deleted:\\n\\n&quot;;</span><br><span class=\"line\">print join(&quot;\\n&quot;, @pkg_files);</span><br><span class=\"line\">print &quot;\\n\\n&quot;;</span><br><span class=\"line\">print &quot;Do you want to proceed and uninstall pandoc (Y/N)?&quot;;</span><br><span class=\"line\">my $input = &lt;STDIN&gt;;</span><br><span class=\"line\"></span><br><span class=\"line\">if ($input =~ m/^[Yy]/) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    # Actually remove the files.</span><br><span class=\"line\">    foreach my $file (@pkg_files) &#123;</span><br><span class=\"line\">        if ( -e $file ) &#123;</span><br><span class=\"line\">            if ( system( &apos;sudo&apos;, &apos;rm&apos;, $file ) == 0 ) &#123;</span><br><span class=\"line\">                warn &quot;Deleted $file\\n&quot;;</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                warn &quot;Unable to delete $file: $?\\n&quot;;</span><br><span class=\"line\">                die &quot;Aborting Uninstall.\\n&quot;;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;  else &#123;</span><br><span class=\"line\">            warn &quot;File $file does not exist.  Skipping.\\n&quot;;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    # Clean up the install.</span><br><span class=\"line\">    if (system(&apos;sudo&apos;, &apos;pkgutil&apos;, &apos;--forget&apos;, $pkg_id, &apos;--volume&apos;, $volume) != 0) &#123;</span><br><span class=\"line\">        die &quot;Unable to clean up install: $?\\n&quot;;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125; else &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">   print &quot;OK, aborting uninstall.\\n&quot;;</span><br><span class=\"line\">   exit;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">print &quot;Pandoc has been successfully uninstalled.\\n&quot;;</span><br><span class=\"line\">exit;</span><br></pre></td></tr></table></figure>\n<h1 id=\"在uninstall-pandoc-pl目录下运行\"><a href=\"#在uninstall-pandoc-pl目录下运行\" class=\"headerlink\" title=\"在uninstall-pandoc.pl目录下运行\"></a>在uninstall-pandoc.pl目录下运行</h1><blockquote>\n<p>perl uninstall-pandoc.pl</p>\n</blockquote>\n<h1 id=\"附：官方链接和截图\"><a href=\"#附：官方链接和截图\" class=\"headerlink\" title=\"附：官方链接和截图\"></a>附：官方链接和截图</h1><p>官方链接：<a href=\"http://pandoc.org/installing.html#macos\" target=\"_blank\" rel=\"noopener\">http://pandoc.org/installing.html#macos</a></p>\n<p>截图：<br><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180928095031.png\" alt=\"\"></p>\n"},{"title":"[Python]对dict字典进行排序","date":"2018-08-08T01:33:09.000Z","_content":"\n# 代码\n```python\n#定义字典\ndict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}\n#根据key进行排序\ndict_sorted_by_key = sorted(dict.items(), key=lambda d: d[0])\n#根据key进行反向排序\ndict_sorted_by_key_reverse = sorted(dict.items(), key=lambda d: d[0], reverse=True)\n#根据value进行排序\ndict_sorted_by_value = sorted(dict.items(), key=lambda d: d[1])\n#根据value进行反向排序\ndict_sorted_by_value_reverse = sorted(dict.items(), key=lambda d: d[1], reverse=True)\n```\n# 示例\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180808100756.png)","source":"_posts/Python-对dict字典进行排序.md","raw":"---\ntitle: '[Python]对dict字典进行排序'\ndate: 2018-08-08 09:33:09\ntags: [python, dict, 字典, python入门]\n---\n\n# 代码\n```python\n#定义字典\ndict = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5}\n#根据key进行排序\ndict_sorted_by_key = sorted(dict.items(), key=lambda d: d[0])\n#根据key进行反向排序\ndict_sorted_by_key_reverse = sorted(dict.items(), key=lambda d: d[0], reverse=True)\n#根据value进行排序\ndict_sorted_by_value = sorted(dict.items(), key=lambda d: d[1])\n#根据value进行反向排序\ndict_sorted_by_value_reverse = sorted(dict.items(), key=lambda d: d[1], reverse=True)\n```\n# 示例\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180808100756.png)","slug":"Python-对dict字典进行排序","published":1,"updated":"2018-08-08T02:12:50.893Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiq30003t7x0n4hfbmcd","content":"<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#定义字典</span></span><br><span class=\"line\">dict = &#123;<span class=\"string\">'a'</span>:<span class=\"number\">1</span>, <span class=\"string\">'b'</span>:<span class=\"number\">2</span>, <span class=\"string\">'c'</span>:<span class=\"number\">3</span>, <span class=\"string\">'d'</span>:<span class=\"number\">4</span>, <span class=\"string\">'e'</span>:<span class=\"number\">5</span>&#125;</span><br><span class=\"line\"><span class=\"comment\">#根据key进行排序</span></span><br><span class=\"line\">dict_sorted_by_key = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\">#根据key进行反向排序</span></span><br><span class=\"line\">dict_sorted_by_key_reverse = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">0</span>], reverse=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#根据value进行排序</span></span><br><span class=\"line\">dict_sorted_by_value = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\">#根据value进行反向排序</span></span><br><span class=\"line\">dict_sorted_by_value_reverse = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">1</span>], reverse=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h1><p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180808100756.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#定义字典</span></span><br><span class=\"line\">dict = &#123;<span class=\"string\">'a'</span>:<span class=\"number\">1</span>, <span class=\"string\">'b'</span>:<span class=\"number\">2</span>, <span class=\"string\">'c'</span>:<span class=\"number\">3</span>, <span class=\"string\">'d'</span>:<span class=\"number\">4</span>, <span class=\"string\">'e'</span>:<span class=\"number\">5</span>&#125;</span><br><span class=\"line\"><span class=\"comment\">#根据key进行排序</span></span><br><span class=\"line\">dict_sorted_by_key = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\">#根据key进行反向排序</span></span><br><span class=\"line\">dict_sorted_by_key_reverse = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">0</span>], reverse=<span class=\"keyword\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#根据value进行排序</span></span><br><span class=\"line\">dict_sorted_by_value = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">1</span>])</span><br><span class=\"line\"><span class=\"comment\">#根据value进行反向排序</span></span><br><span class=\"line\">dict_sorted_by_value_reverse = sorted(dict.items(), key=<span class=\"keyword\">lambda</span> d: d[<span class=\"number\">1</span>], reverse=<span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h1><p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180808100756.png\" alt=\"\"></p>\n"},{"title":"[Python]通过threading开启多线程","date":"2018-08-09T14:42:08.000Z","_content":"\n# 构造方法： \nThread(group=None, target=None, name=None, args=(), kwargs={}) \n\n- group: 线程组，目前还没有实现，库引用中提示必须是None； \n- target: 要执行的方法； \n- name: 线程名； \n- args/kwargs: 要传入方法的参数。\n\n# 实例方法：\n\n- isAlive(): 返回线程是否在运行。正在运行指启动后、终止前。 \n- get/setName(name): 获取/设置线程名。 \n- start():  线程准备就绪，等待CPU调度\n- is/setDaemon(bool): 获取/设置是后台线程（默认前台线程（False））。（在start之前设置）\n    - 如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，主线程和后台线程均停止\n    - 如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止\n- start(): 启动线程。 \n- join([timeout]): 阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout（可选参数）。\n\n# 示例代码\n```python\n# Split items and run function through n threads\n# func形如func(arg_list[0], ..., arg_list[n], items),run_through_threads 可以把items分为num份分配给num个线程运行\ndef run_through_threads(func, arg_list, items, num=4):\n    threads = []\n    item_len = len(items)\n    for i in range(num):\n        threads.append(threading.Thread(target=func, args=(*arg_list, items[int(i*item_len/num):int((i+1)*item_len/num)])))\n    for t in threads:\n        t.setDaemon(True)\n        t.start()\n    for t in threads:\n        t.join()\n```","source":"_posts/Python-python通过threading开启多线程.md","raw":"---\ntitle: '[Python]通过threading开启多线程'\ndate: 2018-08-09 22:42:08\ntags: [python, threading, 多线程, python入门]\n---\n\n# 构造方法： \nThread(group=None, target=None, name=None, args=(), kwargs={}) \n\n- group: 线程组，目前还没有实现，库引用中提示必须是None； \n- target: 要执行的方法； \n- name: 线程名； \n- args/kwargs: 要传入方法的参数。\n\n# 实例方法：\n\n- isAlive(): 返回线程是否在运行。正在运行指启动后、终止前。 \n- get/setName(name): 获取/设置线程名。 \n- start():  线程准备就绪，等待CPU调度\n- is/setDaemon(bool): 获取/设置是后台线程（默认前台线程（False））。（在start之前设置）\n    - 如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，主线程和后台线程均停止\n    - 如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止\n- start(): 启动线程。 \n- join([timeout]): 阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout（可选参数）。\n\n# 示例代码\n```python\n# Split items and run function through n threads\n# func形如func(arg_list[0], ..., arg_list[n], items),run_through_threads 可以把items分为num份分配给num个线程运行\ndef run_through_threads(func, arg_list, items, num=4):\n    threads = []\n    item_len = len(items)\n    for i in range(num):\n        threads.append(threading.Thread(target=func, args=(*arg_list, items[int(i*item_len/num):int((i+1)*item_len/num)])))\n    for t in threads:\n        t.setDaemon(True)\n        t.start()\n    for t in threads:\n        t.join()\n```","slug":"Python-python通过threading开启多线程","published":1,"updated":"2018-08-09T15:04:45.384Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiq40004t7x0jzvdphte","content":"<h1 id=\"构造方法：\"><a href=\"#构造方法：\" class=\"headerlink\" title=\"构造方法：\"></a>构造方法：</h1><p>Thread(group=None, target=None, name=None, args=(), kwargs={}) </p>\n<ul>\n<li>group: 线程组，目前还没有实现，库引用中提示必须是None； </li>\n<li>target: 要执行的方法； </li>\n<li>name: 线程名； </li>\n<li>args/kwargs: 要传入方法的参数。</li>\n</ul>\n<h1 id=\"实例方法：\"><a href=\"#实例方法：\" class=\"headerlink\" title=\"实例方法：\"></a>实例方法：</h1><ul>\n<li>isAlive(): 返回线程是否在运行。正在运行指启动后、终止前。 </li>\n<li>get/setName(name): 获取/设置线程名。 </li>\n<li>start():  线程准备就绪，等待CPU调度</li>\n<li>is/setDaemon(bool): 获取/设置是后台线程（默认前台线程（False））。（在start之前设置）<ul>\n<li>如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，主线程和后台线程均停止</li>\n<li>如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止</li>\n</ul>\n</li>\n<li>start(): 启动线程。 </li>\n<li>join([timeout]): 阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout（可选参数）。</li>\n</ul>\n<h1 id=\"示例代码\"><a href=\"#示例代码\" class=\"headerlink\" title=\"示例代码\"></a>示例代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Split items and run function through n threads</span></span><br><span class=\"line\"><span class=\"comment\"># func形如func(arg_list[0], ..., arg_list[n], items),run_through_threads 可以把items分为num份分配给num个线程运行</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_through_threads</span><span class=\"params\">(func, arg_list, items, num=<span class=\"number\">4</span>)</span>:</span></span><br><span class=\"line\">    threads = []</span><br><span class=\"line\">    item_len = len(items)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num):</span><br><span class=\"line\">        threads.append(threading.Thread(target=func, args=(*arg_list, items[int(i*item_len/num):int((i+<span class=\"number\">1</span>)*item_len/num)])))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> threads:</span><br><span class=\"line\">        t.setDaemon(<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        t.start()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> threads:</span><br><span class=\"line\">        t.join()</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"构造方法：\"><a href=\"#构造方法：\" class=\"headerlink\" title=\"构造方法：\"></a>构造方法：</h1><p>Thread(group=None, target=None, name=None, args=(), kwargs={}) </p>\n<ul>\n<li>group: 线程组，目前还没有实现，库引用中提示必须是None； </li>\n<li>target: 要执行的方法； </li>\n<li>name: 线程名； </li>\n<li>args/kwargs: 要传入方法的参数。</li>\n</ul>\n<h1 id=\"实例方法：\"><a href=\"#实例方法：\" class=\"headerlink\" title=\"实例方法：\"></a>实例方法：</h1><ul>\n<li>isAlive(): 返回线程是否在运行。正在运行指启动后、终止前。 </li>\n<li>get/setName(name): 获取/设置线程名。 </li>\n<li>start():  线程准备就绪，等待CPU调度</li>\n<li>is/setDaemon(bool): 获取/设置是后台线程（默认前台线程（False））。（在start之前设置）<ul>\n<li>如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，主线程和后台线程均停止</li>\n<li>如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止</li>\n</ul>\n</li>\n<li>start(): 启动线程。 </li>\n<li>join([timeout]): 阻塞当前上下文环境的线程，直到调用此方法的线程终止或到达指定的timeout（可选参数）。</li>\n</ul>\n<h1 id=\"示例代码\"><a href=\"#示例代码\" class=\"headerlink\" title=\"示例代码\"></a>示例代码</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Split items and run function through n threads</span></span><br><span class=\"line\"><span class=\"comment\"># func形如func(arg_list[0], ..., arg_list[n], items),run_through_threads 可以把items分为num份分配给num个线程运行</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_through_threads</span><span class=\"params\">(func, arg_list, items, num=<span class=\"number\">4</span>)</span>:</span></span><br><span class=\"line\">    threads = []</span><br><span class=\"line\">    item_len = len(items)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num):</span><br><span class=\"line\">        threads.append(threading.Thread(target=func, args=(*arg_list, items[int(i*item_len/num):int((i+<span class=\"number\">1</span>)*item_len/num)])))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> threads:</span><br><span class=\"line\">        t.setDaemon(<span class=\"keyword\">True</span>)</span><br><span class=\"line\">        t.start()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> threads:</span><br><span class=\"line\">        t.join()</span><br></pre></td></tr></table></figure>"},{"title":"[Trick]-bash: tensorboard: 未找到命令","date":"2018-09-06T02:51:44.000Z","comments":1,"_content":"\n# 原因\n\ntensorboard命令不在环境变量中\n\n# 解决思路\n\n找到tensorboard脚本路径，然后运行\n\n# 解决方法\n\n\n```bash\npython3 /home/USERNAME/.local/lib/python3.6/site-packages/tensorboard/main.py --logdir=LOGDIR\n```\n\n***NOTE：***\n- USERNAME是指用户名称\n- LOGDIR是指log文件存放的相对或绝对目录","source":"_posts/Trick-bash-tensorboard-未找到命令.md","raw":"---\ntitle: '[Trick]-bash: tensorboard: 未找到命令'\ndate: 2018-09-06 10:51:44\ntags: [linux, trick, tensorboard]\ncomments: true\n---\n\n# 原因\n\ntensorboard命令不在环境变量中\n\n# 解决思路\n\n找到tensorboard脚本路径，然后运行\n\n# 解决方法\n\n\n```bash\npython3 /home/USERNAME/.local/lib/python3.6/site-packages/tensorboard/main.py --logdir=LOGDIR\n```\n\n***NOTE：***\n- USERNAME是指用户名称\n- LOGDIR是指log文件存放的相对或绝对目录","slug":"Trick-bash-tensorboard-未找到命令","published":1,"updated":"2018-09-06T02:59:59.912Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiq50005t7x08ej5izpb","content":"<h1 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h1><p>tensorboard命令不在环境变量中</p>\n<h1 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h1><p>找到tensorboard脚本路径，然后运行</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 /home/USERNAME/.<span class=\"built_in\">local</span>/lib/python3.6/site-packages/tensorboard/main.py --logdir=LOGDIR</span><br></pre></td></tr></table></figure>\n<p><strong><em>NOTE：</em></strong></p>\n<ul>\n<li>USERNAME是指用户名称</li>\n<li>LOGDIR是指log文件存放的相对或绝对目录</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h1><p>tensorboard命令不在环境变量中</p>\n<h1 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h1><p>找到tensorboard脚本路径，然后运行</p>\n<h1 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 /home/USERNAME/.<span class=\"built_in\">local</span>/lib/python3.6/site-packages/tensorboard/main.py --logdir=LOGDIR</span><br></pre></td></tr></table></figure>\n<p><strong><em>NOTE：</em></strong></p>\n<ul>\n<li>USERNAME是指用户名称</li>\n<li>LOGDIR是指log文件存放的相对或绝对目录</li>\n</ul>\n"},{"title":"[Trick]git pull 强制覆盖本地文件","date":"2018-07-31T04:50:16.000Z","comments":1,"_content":"\n```git\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\nnote：出错的话就再试一次，说不定就可以了","source":"_posts/Trick-git-pull-强制覆盖本地文件.md","raw":"---\ntitle: '[Trick]git pull 强制覆盖本地文件'\ndate: 2018-07-31 12:50:16\ntags: [git, trick]\ncomments: true\n---\n\n```git\ngit fetch --all \ngit reset --hard origin/master\ngit pull\n```\n\nnote：出错的话就再试一次，说不定就可以了","slug":"Trick-git-pull-强制覆盖本地文件","published":1,"updated":"2018-07-31T05:55:49.383Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiq70007t7x07084hr2d","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch --all </span><br><span class=\"line\">git reset --hard origin/master</span><br><span class=\"line\">git pull</span><br></pre></td></tr></table></figure>\n<p>note：出错的话就再试一次，说不定就可以了</p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch --all </span><br><span class=\"line\">git reset --hard origin/master</span><br><span class=\"line\">git pull</span><br></pre></td></tr></table></figure>\n<p>note：出错的话就再试一次，说不定就可以了</p>\n"},{"title":"Python入门：pickle模块简介","date":"2017-04-23T18:45:19.000Z","comments":1,"_content":"\n## 概要\n这篇文章介绍了何为pickle以及pickle模块的简单使用方法，即如何使用pickle进行存储存储以及数据的提取，关于pickle模块的其他更加详细的介绍可以参看[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)\n\n## pickle简介\n\npickle模块是python中用来持久化对象的一个模块。所谓对对象进行持久化，即将对象的数据类型、存储结构、存储内容等所有信息作为文件保存下来以便下次使用。\n\n就比如说你通过pickle将一个数组保存成了文件，那么当你下次通过pickle将这个文件读取出来的时候，你读取到的依然是一个数组，而不是一个看起来长得像数组的字符串。\n\n## 用pickle保存对象到文件\n\n```python\n#导入pickle模块\nimport pickle\n\n#创建一个名为data1的对象\ndata1 = {'a': '123', 'b': [1, 2, 3, 4]}\n\n#打开(或创建)一个名为data1.pkl的文件，打开方式为二进制写入(参数‘wb’)\nfile_to_save = open(\"data1.pkl\", \"wb\")\n\n#通过pickle模块中的dump函数将data1保存到data1.pkl文件中。\n#第一个参数是要保存的对象名\n#第二个参数是写入到的类文件对象file。file必须有write()接口， file可以是一个以'w'方式打开的文件或者一个StringIO对象或者其他任何实现write()接口的对象。如果protocol>=1，文件对象需要是二进制模式打开的。\n#第三个参数为序列化使用的协议版本，0：ASCII协议，所序列化的对象使用可打印的ASCII码表示；1：老式的二进制协议；2：2.3版本引入的新二进制协议，较以前的更高效；-1：使用当前版本支持的最高协议。其中协议0和1兼容老版本的python。protocol默认值为0。\npickle.dump(data1, file_to_save, -1)\n\n#关闭文件对象\nfile_to_save.close()\n```\n\n## 用pickle从文件中读取对象\n\n(请接着上一个脚本运行)\n```python\n#导入pickle模块\nimport pickle\n\n#打开一个名为data1.pkl的文件，打开方式为二进制读取(参数‘rb’)\nfile_to_read = open('data1.pkl', 'rb')\n\n#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2\ndata2 = pickle.load(file_to_read)\n\n#打印data2\nprint data2\n\n#关闭文件对象\nfile_to_read.close()\n```\n\n\n\n","source":"_posts/Python入门：pickle模块简介.md","raw":"---\ntitle: Python入门：pickle模块简介\ndate: 2017-04-24 02:45:19\ntags: [python, pickle, python入门]\ncomments: true\n---\n\n## 概要\n这篇文章介绍了何为pickle以及pickle模块的简单使用方法，即如何使用pickle进行存储存储以及数据的提取，关于pickle模块的其他更加详细的介绍可以参看[https://docs.python.org/2/library/pickle.html](https://docs.python.org/2/library/pickle.html)\n\n## pickle简介\n\npickle模块是python中用来持久化对象的一个模块。所谓对对象进行持久化，即将对象的数据类型、存储结构、存储内容等所有信息作为文件保存下来以便下次使用。\n\n就比如说你通过pickle将一个数组保存成了文件，那么当你下次通过pickle将这个文件读取出来的时候，你读取到的依然是一个数组，而不是一个看起来长得像数组的字符串。\n\n## 用pickle保存对象到文件\n\n```python\n#导入pickle模块\nimport pickle\n\n#创建一个名为data1的对象\ndata1 = {'a': '123', 'b': [1, 2, 3, 4]}\n\n#打开(或创建)一个名为data1.pkl的文件，打开方式为二进制写入(参数‘wb’)\nfile_to_save = open(\"data1.pkl\", \"wb\")\n\n#通过pickle模块中的dump函数将data1保存到data1.pkl文件中。\n#第一个参数是要保存的对象名\n#第二个参数是写入到的类文件对象file。file必须有write()接口， file可以是一个以'w'方式打开的文件或者一个StringIO对象或者其他任何实现write()接口的对象。如果protocol>=1，文件对象需要是二进制模式打开的。\n#第三个参数为序列化使用的协议版本，0：ASCII协议，所序列化的对象使用可打印的ASCII码表示；1：老式的二进制协议；2：2.3版本引入的新二进制协议，较以前的更高效；-1：使用当前版本支持的最高协议。其中协议0和1兼容老版本的python。protocol默认值为0。\npickle.dump(data1, file_to_save, -1)\n\n#关闭文件对象\nfile_to_save.close()\n```\n\n## 用pickle从文件中读取对象\n\n(请接着上一个脚本运行)\n```python\n#导入pickle模块\nimport pickle\n\n#打开一个名为data1.pkl的文件，打开方式为二进制读取(参数‘rb’)\nfile_to_read = open('data1.pkl', 'rb')\n\n#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2\ndata2 = pickle.load(file_to_read)\n\n#打印data2\nprint data2\n\n#关闭文件对象\nfile_to_read.close()\n```\n\n\n\n","slug":"Python入门：pickle模块简介","published":1,"updated":"2017-04-24T05:20:02.000Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiq80008t7x0i3d4ncj7","content":"<h2 id=\"概要\"><a href=\"#概要\" class=\"headerlink\" title=\"概要\"></a>概要</h2><p>这篇文章介绍了何为pickle以及pickle模块的简单使用方法，即如何使用pickle进行存储存储以及数据的提取，关于pickle模块的其他更加详细的介绍可以参看<a href=\"https://docs.python.org/2/library/pickle.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/2/library/pickle.html</a></p>\n<h2 id=\"pickle简介\"><a href=\"#pickle简介\" class=\"headerlink\" title=\"pickle简介\"></a>pickle简介</h2><p>pickle模块是python中用来持久化对象的一个模块。所谓对对象进行持久化，即将对象的数据类型、存储结构、存储内容等所有信息作为文件保存下来以便下次使用。</p>\n<p>就比如说你通过pickle将一个数组保存成了文件，那么当你下次通过pickle将这个文件读取出来的时候，你读取到的依然是一个数组，而不是一个看起来长得像数组的字符串。</p>\n<h2 id=\"用pickle保存对象到文件\"><a href=\"#用pickle保存对象到文件\" class=\"headerlink\" title=\"用pickle保存对象到文件\"></a>用pickle保存对象到文件</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#导入pickle模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建一个名为data1的对象</span></span><br><span class=\"line\">data1 = &#123;<span class=\"string\">'a'</span>: <span class=\"string\">'123'</span>, <span class=\"string\">'b'</span>: [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打开(或创建)一个名为data1.pkl的文件，打开方式为二进制写入(参数‘wb’)</span></span><br><span class=\"line\">file_to_save = open(<span class=\"string\">\"data1.pkl\"</span>, <span class=\"string\">\"wb\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过pickle模块中的dump函数将data1保存到data1.pkl文件中。</span></span><br><span class=\"line\"><span class=\"comment\">#第一个参数是要保存的对象名</span></span><br><span class=\"line\"><span class=\"comment\">#第二个参数是写入到的类文件对象file。file必须有write()接口， file可以是一个以'w'方式打开的文件或者一个StringIO对象或者其他任何实现write()接口的对象。如果protocol&gt;=1，文件对象需要是二进制模式打开的。</span></span><br><span class=\"line\"><span class=\"comment\">#第三个参数为序列化使用的协议版本，0：ASCII协议，所序列化的对象使用可打印的ASCII码表示；1：老式的二进制协议；2：2.3版本引入的新二进制协议，较以前的更高效；-1：使用当前版本支持的最高协议。其中协议0和1兼容老版本的python。protocol默认值为0。</span></span><br><span class=\"line\">pickle.dump(data1, file_to_save, <span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#关闭文件对象</span></span><br><span class=\"line\">file_to_save.close()</span><br></pre></td></tr></table></figure>\n<h2 id=\"用pickle从文件中读取对象\"><a href=\"#用pickle从文件中读取对象\" class=\"headerlink\" title=\"用pickle从文件中读取对象\"></a>用pickle从文件中读取对象</h2><p>(请接着上一个脚本运行)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#导入pickle模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打开一个名为data1.pkl的文件，打开方式为二进制读取(参数‘rb’)</span></span><br><span class=\"line\">file_to_read = open(<span class=\"string\">'data1.pkl'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2</span></span><br><span class=\"line\">data2 = pickle.load(file_to_read)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打印data2</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> data2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#关闭文件对象</span></span><br><span class=\"line\">file_to_read.close()</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"概要\"><a href=\"#概要\" class=\"headerlink\" title=\"概要\"></a>概要</h2><p>这篇文章介绍了何为pickle以及pickle模块的简单使用方法，即如何使用pickle进行存储存储以及数据的提取，关于pickle模块的其他更加详细的介绍可以参看<a href=\"https://docs.python.org/2/library/pickle.html\" target=\"_blank\" rel=\"noopener\">https://docs.python.org/2/library/pickle.html</a></p>\n<h2 id=\"pickle简介\"><a href=\"#pickle简介\" class=\"headerlink\" title=\"pickle简介\"></a>pickle简介</h2><p>pickle模块是python中用来持久化对象的一个模块。所谓对对象进行持久化，即将对象的数据类型、存储结构、存储内容等所有信息作为文件保存下来以便下次使用。</p>\n<p>就比如说你通过pickle将一个数组保存成了文件，那么当你下次通过pickle将这个文件读取出来的时候，你读取到的依然是一个数组，而不是一个看起来长得像数组的字符串。</p>\n<h2 id=\"用pickle保存对象到文件\"><a href=\"#用pickle保存对象到文件\" class=\"headerlink\" title=\"用pickle保存对象到文件\"></a>用pickle保存对象到文件</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#导入pickle模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#创建一个名为data1的对象</span></span><br><span class=\"line\">data1 = &#123;<span class=\"string\">'a'</span>: <span class=\"string\">'123'</span>, <span class=\"string\">'b'</span>: [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打开(或创建)一个名为data1.pkl的文件，打开方式为二进制写入(参数‘wb’)</span></span><br><span class=\"line\">file_to_save = open(<span class=\"string\">\"data1.pkl\"</span>, <span class=\"string\">\"wb\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过pickle模块中的dump函数将data1保存到data1.pkl文件中。</span></span><br><span class=\"line\"><span class=\"comment\">#第一个参数是要保存的对象名</span></span><br><span class=\"line\"><span class=\"comment\">#第二个参数是写入到的类文件对象file。file必须有write()接口， file可以是一个以'w'方式打开的文件或者一个StringIO对象或者其他任何实现write()接口的对象。如果protocol&gt;=1，文件对象需要是二进制模式打开的。</span></span><br><span class=\"line\"><span class=\"comment\">#第三个参数为序列化使用的协议版本，0：ASCII协议，所序列化的对象使用可打印的ASCII码表示；1：老式的二进制协议；2：2.3版本引入的新二进制协议，较以前的更高效；-1：使用当前版本支持的最高协议。其中协议0和1兼容老版本的python。protocol默认值为0。</span></span><br><span class=\"line\">pickle.dump(data1, file_to_save, <span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#关闭文件对象</span></span><br><span class=\"line\">file_to_save.close()</span><br></pre></td></tr></table></figure>\n<h2 id=\"用pickle从文件中读取对象\"><a href=\"#用pickle从文件中读取对象\" class=\"headerlink\" title=\"用pickle从文件中读取对象\"></a>用pickle从文件中读取对象</h2><p>(请接着上一个脚本运行)<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#导入pickle模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打开一个名为data1.pkl的文件，打开方式为二进制读取(参数‘rb’)</span></span><br><span class=\"line\">file_to_read = open(<span class=\"string\">'data1.pkl'</span>, <span class=\"string\">'rb'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过pickle的load函数读取data1.pkl中的对象，并赋值给data2</span></span><br><span class=\"line\">data2 = pickle.load(file_to_read)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#打印data2</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> data2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#关闭文件对象</span></span><br><span class=\"line\">file_to_read.close()</span><br></pre></td></tr></table></figure></p>\n"},{"title":"[Reading Notes]Privacy-CNH: A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features","date":"2016-12-10T08:44:19.000Z","comments":1,"_content":"\n## Motivation\n\nMobile devices have revolutionized how people share photos with each other on social networks with a single click of a button.\n\nThe content of the photos people shared to the internet are rarely analyzed by the websites before the photos are made available to view. After the photos are posted on the social network to the public to view, it is close to impossible to permanently delete the uploaded photos.\n\nPhoto leakage, regretion after posting and malicious posting happens from time to time.\n<!-- more-->\n## Related Works\n\nExisting works on photo privacy detection, which rely on low-level vision features, are non-informative to the users regarding what privacy information is leaked from their photos.\n\n## Detailed Design\n\n![framework](http://ohykn376o.bkt.clouddn.com/20161210PCNH1.jpg)\n\nPCNH is a combination of PCNN and PONN. Given the image features in the input layer, the object features learning pipeline processes the features using hi(x) as the activation functions and the param eters of network structure is encoded as Vi, finally obtaining the photo privacy detection result in the output layer. The convolutional features learning pipeline processes the features using j(x) as the activation function and the parameters of network structure is encoded as Ws, finally obtaining the photo privacy detection result in the output layer. hi(x), j(x) are activation functions, which map from a vector to a scalar.\n\n## Evaluation\n\n![Evaluation Chart](http://ohykn376o.bkt.clouddn.com/20161210PCNH2.jpg)\n\n","source":"_posts/[Reading Notes] Privacy-CNH- A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features.md","raw":"---\ntitle: '[Reading Notes]Privacy-CNH: A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features'\ndate: 2016-12-10 16:44:19\ntags: [PCNH, CNN, Privacy Detect, Reading Notes]\ncomments: true\n---\n\n## Motivation\n\nMobile devices have revolutionized how people share photos with each other on social networks with a single click of a button.\n\nThe content of the photos people shared to the internet are rarely analyzed by the websites before the photos are made available to view. After the photos are posted on the social network to the public to view, it is close to impossible to permanently delete the uploaded photos.\n\nPhoto leakage, regretion after posting and malicious posting happens from time to time.\n<!-- more-->\n## Related Works\n\nExisting works on photo privacy detection, which rely on low-level vision features, are non-informative to the users regarding what privacy information is leaked from their photos.\n\n## Detailed Design\n\n![framework](http://ohykn376o.bkt.clouddn.com/20161210PCNH1.jpg)\n\nPCNH is a combination of PCNN and PONN. Given the image features in the input layer, the object features learning pipeline processes the features using hi(x) as the activation functions and the param eters of network structure is encoded as Vi, finally obtaining the photo privacy detection result in the output layer. The convolutional features learning pipeline processes the features using j(x) as the activation function and the parameters of network structure is encoded as Ws, finally obtaining the photo privacy detection result in the output layer. hi(x), j(x) are activation functions, which map from a vector to a scalar.\n\n## Evaluation\n\n![Evaluation Chart](http://ohykn376o.bkt.clouddn.com/20161210PCNH2.jpg)\n\n","slug":"[Reading Notes] Privacy-CNH- A Framework to Detect Photo Privacy with Convolutional Neural Network using Hierarchical Features","published":1,"updated":"2016-12-10T16:14:34.000Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiq9000at7x0wsnuv7an","content":"<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>Mobile devices have revolutionized how people share photos with each other on social networks with a single click of a button.</p>\n<p>The content of the photos people shared to the internet are rarely analyzed by the websites before the photos are made available to view. After the photos are posted on the social network to the public to view, it is close to impossible to permanently delete the uploaded photos.</p>\n<p>Photo leakage, regretion after posting and malicious posting happens from time to time.<br><a id=\"more\"></a></p>\n<h2 id=\"Related-Works\"><a href=\"#Related-Works\" class=\"headerlink\" title=\"Related Works\"></a>Related Works</h2><p>Existing works on photo privacy detection, which rely on low-level vision features, are non-informative to the users regarding what privacy information is leaked from their photos.</p>\n<h2 id=\"Detailed-Design\"><a href=\"#Detailed-Design\" class=\"headerlink\" title=\"Detailed Design\"></a>Detailed Design</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/20161210PCNH1.jpg\" alt=\"framework\"></p>\n<p>PCNH is a combination of PCNN and PONN. Given the image features in the input layer, the object features learning pipeline processes the features using hi(x) as the activation functions and the param eters of network structure is encoded as Vi, finally obtaining the photo privacy detection result in the output layer. The convolutional features learning pipeline processes the features using j(x) as the activation function and the parameters of network structure is encoded as Ws, finally obtaining the photo privacy detection result in the output layer. hi(x), j(x) are activation functions, which map from a vector to a scalar.</p>\n<h2 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/20161210PCNH2.jpg\" alt=\"Evaluation Chart\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>Mobile devices have revolutionized how people share photos with each other on social networks with a single click of a button.</p>\n<p>The content of the photos people shared to the internet are rarely analyzed by the websites before the photos are made available to view. After the photos are posted on the social network to the public to view, it is close to impossible to permanently delete the uploaded photos.</p>\n<p>Photo leakage, regretion after posting and malicious posting happens from time to time.<br>","more":"</p>\n<h2 id=\"Related-Works\"><a href=\"#Related-Works\" class=\"headerlink\" title=\"Related Works\"></a>Related Works</h2><p>Existing works on photo privacy detection, which rely on low-level vision features, are non-informative to the users regarding what privacy information is leaked from their photos.</p>\n<h2 id=\"Detailed-Design\"><a href=\"#Detailed-Design\" class=\"headerlink\" title=\"Detailed Design\"></a>Detailed Design</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/20161210PCNH1.jpg\" alt=\"framework\"></p>\n<p>PCNH is a combination of PCNN and PONN. Given the image features in the input layer, the object features learning pipeline processes the features using hi(x) as the activation functions and the param eters of network structure is encoded as Vi, finally obtaining the photo privacy detection result in the output layer. The convolutional features learning pipeline processes the features using j(x) as the activation function and the parameters of network structure is encoded as Ws, finally obtaining the photo privacy detection result in the output layer. hi(x), j(x) are activation functions, which map from a vector to a scalar.</p>\n<h2 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/20161210PCNH2.jpg\" alt=\"Evaluation Chart\"></p>"},{"title":"[Reading Notes] UniCrawl: A Practical Geographically Distributed Web Crawler","date":"2016-12-10T16:13:19.000Z","comments":1,"_content":"\n## Abstract\n\nCause the wealth of information available on the web keeps growing, we want to use web crawler to get them. But the traditional method has a fatal limit of its large infrastructure cost. To reduce it, we developed this method, unicrawl, which can show a performance improvement of 93.6% in terms of network bandwidth consumption, and a speedup factor of 1.75.\n\n## I. introduction\n\nNowadays, it's common that to use parallel process on a large number of machines to achieve a reasonable collection time. While this method requires large computing infrastructures. Like Google and Bing, who rely on big data centers. \n<!-- more-->\nAs for the public crawl repositories, they require externalizing computation and data hosting to a commercial cloud provider. Which may pose the problem of data availability in the mid-long term. And cause there are large amounts of data unnecessary, postprocessing is needed.\n\nA solution to those problems is to distribute the crawling effort over several geographically distributed locations. For instance, by allowing several small companies to mutualize their crawling infrastructures. In addition, such an approach leverages data locality as sites can crawl web servers that are geographically nearby. But in this way, the synchronization between the crawler at the different sites is a new problem. Our goal is to reduce such communication costs.\n\nUniCrawl is an efficient geo-distributed crawler that aims at minimizing inter-site communication costs. Our design is both practical and scalable. We assess this claim with a detailed evaluation of UniCrawl in a controlled environment using the ClueWeb12 dataset, as well as in geo-distributed setting using 3 distinct sites located in Germany.\n\n### outline:\nSection II is related work. Section III introduces the crawler architecture, refining it from existing well-founded central disigns. Section IV is the details about the internal implementaion. Section V presents the experimental results, both in-vitro, and in-vivo over multiple geographical locations in Germany. We discuss out results and future work in Section V. We conclude the paper inSection VII.\n\n## II. Related work\n\nThere are several problems for every crawler to solve:\n\n1. since the amount of information to parse is huge, a crawler must scale\n2. a crawler should select which information to download first, and which information to refresh over time\n3. a crawler should not be a burden for the web sites that host the content\n4. adversaries, e.g., spider traps, need to be avoided with care\n\nMercator/Polybot/IBM WebFountain/Ubicrawl and etc..\n\n## III. Distributed crawler architecture\n\n### A. Single site Design\n\n1. Map-reduce:\n - spill\n - shuffle\n - reduce\n2. site storage\n - In UniCrawl, the crawl database of a site is implemented as a single distributed map structure. This map contains for each page its URL, content, and outlinks.\n - INFINISPAN, a distributed key-value store stat supports the following features: \n     - Routing: Notes are organized in a ring\n     - Elasticity\n     - Storage\n     - Reliability\n     - Interface\n     - Consistency\n     - Querying\n3. Detail of Phases\n - Generate: The goal of the generate phase is to select a set of pages to process during the round.\n - Fetch: During the fetch phase, the map step first groups by host the pages that were generated in the previous phase.\n - Parse: Once the pages are fetched, they are analyzed during the parse phase.\n - Update: The goal of the update phase is to refresh the scores of pages that belong to the frontier in order to prioritize them.\n\n### B. Multi-site Operations\n\nSeveral key ideas allow UniCrawl to be practical in this setting:\n1. Each site is independent and crawls the web autonomously\n2. We unite all the site data stores\n3. Sites exchanges dynamically the URLs they discover over the course of the crawl\n\n1. Federating the storage: One of the key design concerns of UniCrawl is to bring small monifications to the site code base in order be usable over multiple geographical locations.\n2. collaboration between sites: Following the approach advocated by Cho and Garcia-Molina. UniCrawl exchanges newly discovered URLs over time. This exchange occurs at the end of the update phase.\n - We implement the crawl database as a distributed ensemble map that span all the sites. This map operates in frontier mode with a replication factor of one.\n3. Crawl quality and cost: The quality of the crawling operation is not only measured by means of pure web-graph exploration but also by the rounds it takes to discover the most interesting pages.\n\n## IV. Implementation\n\nWe implemented UniCrawl inJava, starting from the code base of Nutch version 2.5.3.\n\nNutch makes use of Apache Gora, an open-source framework that provides an in-memory and persistent data model for big data.\n\nIntotal, our contribution accounts for about a dozen thousands lines of code (LOC) split as follows: 9.4 kLOC for Ensemble, 1.1 kLOC for Gora and 2.3 kLOC patch for Nutch\n\n### A. Merging phases\n\nCause each new map-reduce job creation is expensive as it requires to start a dedicated Java virtual machine, and deploy the appropriate jars. To lower this cost, we merge the fetch and parse phases in out UniCrawl implementation. This means that whenever a reducer fetches a new page, it parses its content and extract the out-links. These links are then directly inserted in the crawl database together with the fetched page.\n\n### B. Caching\n\nTo avoid sending out an URL multiple times across sites, we use a distributed solution. In more details, this cache is a bounded ENSENMBLE map C local to each site and replicated at all nodes in a site. During the update phase, when a reducer selects a URL in the frontier that is associated to a remote site, it first check locally with C is this URL woa previously sent. If this is the case , the reducer simply skips the call to putIfAbsent. Since C is replicated at all nodes, every map-reduce node is co-located with an INFINISPAN node, and C is in memory, this inclusion test costs less than a millisecond.\n\n## V. Evaluation\n\nEvaluate UniCrawl through several key metrics such as the page processing rate, the memory usage and the network traffic across sites.\n\nTwo parts:\n1. Evaluate our approach in-vitro, by running UniCrawl against the ClueWeb12 benchmark in an emulated multi-site architecture and crawling from a local repository.\n2. Report several experimental results where we deploy UniCrawl at multiple localtions in Germany and access actual web sites.\n\n### A. In-vitro validation\n\n1. Single site performance\n2. Emulating multiple sites\n\n### B. UniCrawl in the wild\n\n1. URL Exchange\n2. Comparison with Nutch\n3. Scalability\n\n","source":"_posts/[Reading Notes] UniCrawl- A Practical Geographically Distributed Web Crawler.md","raw":"---\ntitle: '[Reading Notes] UniCrawl: A Practical Geographically Distributed Web Crawler'\ndate: 2016-12-11 00:13:19\ntags: [web, crawler, unicrawl, Reading Notes]\ncomments: true\n---\n\n## Abstract\n\nCause the wealth of information available on the web keeps growing, we want to use web crawler to get them. But the traditional method has a fatal limit of its large infrastructure cost. To reduce it, we developed this method, unicrawl, which can show a performance improvement of 93.6% in terms of network bandwidth consumption, and a speedup factor of 1.75.\n\n## I. introduction\n\nNowadays, it's common that to use parallel process on a large number of machines to achieve a reasonable collection time. While this method requires large computing infrastructures. Like Google and Bing, who rely on big data centers. \n<!-- more-->\nAs for the public crawl repositories, they require externalizing computation and data hosting to a commercial cloud provider. Which may pose the problem of data availability in the mid-long term. And cause there are large amounts of data unnecessary, postprocessing is needed.\n\nA solution to those problems is to distribute the crawling effort over several geographically distributed locations. For instance, by allowing several small companies to mutualize their crawling infrastructures. In addition, such an approach leverages data locality as sites can crawl web servers that are geographically nearby. But in this way, the synchronization between the crawler at the different sites is a new problem. Our goal is to reduce such communication costs.\n\nUniCrawl is an efficient geo-distributed crawler that aims at minimizing inter-site communication costs. Our design is both practical and scalable. We assess this claim with a detailed evaluation of UniCrawl in a controlled environment using the ClueWeb12 dataset, as well as in geo-distributed setting using 3 distinct sites located in Germany.\n\n### outline:\nSection II is related work. Section III introduces the crawler architecture, refining it from existing well-founded central disigns. Section IV is the details about the internal implementaion. Section V presents the experimental results, both in-vitro, and in-vivo over multiple geographical locations in Germany. We discuss out results and future work in Section V. We conclude the paper inSection VII.\n\n## II. Related work\n\nThere are several problems for every crawler to solve:\n\n1. since the amount of information to parse is huge, a crawler must scale\n2. a crawler should select which information to download first, and which information to refresh over time\n3. a crawler should not be a burden for the web sites that host the content\n4. adversaries, e.g., spider traps, need to be avoided with care\n\nMercator/Polybot/IBM WebFountain/Ubicrawl and etc..\n\n## III. Distributed crawler architecture\n\n### A. Single site Design\n\n1. Map-reduce:\n - spill\n - shuffle\n - reduce\n2. site storage\n - In UniCrawl, the crawl database of a site is implemented as a single distributed map structure. This map contains for each page its URL, content, and outlinks.\n - INFINISPAN, a distributed key-value store stat supports the following features: \n     - Routing: Notes are organized in a ring\n     - Elasticity\n     - Storage\n     - Reliability\n     - Interface\n     - Consistency\n     - Querying\n3. Detail of Phases\n - Generate: The goal of the generate phase is to select a set of pages to process during the round.\n - Fetch: During the fetch phase, the map step first groups by host the pages that were generated in the previous phase.\n - Parse: Once the pages are fetched, they are analyzed during the parse phase.\n - Update: The goal of the update phase is to refresh the scores of pages that belong to the frontier in order to prioritize them.\n\n### B. Multi-site Operations\n\nSeveral key ideas allow UniCrawl to be practical in this setting:\n1. Each site is independent and crawls the web autonomously\n2. We unite all the site data stores\n3. Sites exchanges dynamically the URLs they discover over the course of the crawl\n\n1. Federating the storage: One of the key design concerns of UniCrawl is to bring small monifications to the site code base in order be usable over multiple geographical locations.\n2. collaboration between sites: Following the approach advocated by Cho and Garcia-Molina. UniCrawl exchanges newly discovered URLs over time. This exchange occurs at the end of the update phase.\n - We implement the crawl database as a distributed ensemble map that span all the sites. This map operates in frontier mode with a replication factor of one.\n3. Crawl quality and cost: The quality of the crawling operation is not only measured by means of pure web-graph exploration but also by the rounds it takes to discover the most interesting pages.\n\n## IV. Implementation\n\nWe implemented UniCrawl inJava, starting from the code base of Nutch version 2.5.3.\n\nNutch makes use of Apache Gora, an open-source framework that provides an in-memory and persistent data model for big data.\n\nIntotal, our contribution accounts for about a dozen thousands lines of code (LOC) split as follows: 9.4 kLOC for Ensemble, 1.1 kLOC for Gora and 2.3 kLOC patch for Nutch\n\n### A. Merging phases\n\nCause each new map-reduce job creation is expensive as it requires to start a dedicated Java virtual machine, and deploy the appropriate jars. To lower this cost, we merge the fetch and parse phases in out UniCrawl implementation. This means that whenever a reducer fetches a new page, it parses its content and extract the out-links. These links are then directly inserted in the crawl database together with the fetched page.\n\n### B. Caching\n\nTo avoid sending out an URL multiple times across sites, we use a distributed solution. In more details, this cache is a bounded ENSENMBLE map C local to each site and replicated at all nodes in a site. During the update phase, when a reducer selects a URL in the frontier that is associated to a remote site, it first check locally with C is this URL woa previously sent. If this is the case , the reducer simply skips the call to putIfAbsent. Since C is replicated at all nodes, every map-reduce node is co-located with an INFINISPAN node, and C is in memory, this inclusion test costs less than a millisecond.\n\n## V. Evaluation\n\nEvaluate UniCrawl through several key metrics such as the page processing rate, the memory usage and the network traffic across sites.\n\nTwo parts:\n1. Evaluate our approach in-vitro, by running UniCrawl against the ClueWeb12 benchmark in an emulated multi-site architecture and crawling from a local repository.\n2. Report several experimental results where we deploy UniCrawl at multiple localtions in Germany and access actual web sites.\n\n### A. In-vitro validation\n\n1. Single site performance\n2. Emulating multiple sites\n\n### B. UniCrawl in the wild\n\n1. URL Exchange\n2. Comparison with Nutch\n3. Scalability\n\n","slug":"[Reading Notes] UniCrawl- A Practical Geographically Distributed Web Crawler","published":1,"updated":"2016-12-10T16:14:50.000Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqb000bt7x0rpp86pgc","content":"<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Cause the wealth of information available on the web keeps growing, we want to use web crawler to get them. But the traditional method has a fatal limit of its large infrastructure cost. To reduce it, we developed this method, unicrawl, which can show a performance improvement of 93.6% in terms of network bandwidth consumption, and a speedup factor of 1.75.</p>\n<h2 id=\"I-introduction\"><a href=\"#I-introduction\" class=\"headerlink\" title=\"I. introduction\"></a>I. introduction</h2><p>Nowadays, it’s common that to use parallel process on a large number of machines to achieve a reasonable collection time. While this method requires large computing infrastructures. Like Google and Bing, who rely on big data centers.<br><a id=\"more\"></a><br>As for the public crawl repositories, they require externalizing computation and data hosting to a commercial cloud provider. Which may pose the problem of data availability in the mid-long term. And cause there are large amounts of data unnecessary, postprocessing is needed.</p>\n<p>A solution to those problems is to distribute the crawling effort over several geographically distributed locations. For instance, by allowing several small companies to mutualize their crawling infrastructures. In addition, such an approach leverages data locality as sites can crawl web servers that are geographically nearby. But in this way, the synchronization between the crawler at the different sites is a new problem. Our goal is to reduce such communication costs.</p>\n<p>UniCrawl is an efficient geo-distributed crawler that aims at minimizing inter-site communication costs. Our design is both practical and scalable. We assess this claim with a detailed evaluation of UniCrawl in a controlled environment using the ClueWeb12 dataset, as well as in geo-distributed setting using 3 distinct sites located in Germany.</p>\n<h3 id=\"outline\"><a href=\"#outline\" class=\"headerlink\" title=\"outline:\"></a>outline:</h3><p>Section II is related work. Section III introduces the crawler architecture, refining it from existing well-founded central disigns. Section IV is the details about the internal implementaion. Section V presents the experimental results, both in-vitro, and in-vivo over multiple geographical locations in Germany. We discuss out results and future work in Section V. We conclude the paper inSection VII.</p>\n<h2 id=\"II-Related-work\"><a href=\"#II-Related-work\" class=\"headerlink\" title=\"II. Related work\"></a>II. Related work</h2><p>There are several problems for every crawler to solve:</p>\n<ol>\n<li>since the amount of information to parse is huge, a crawler must scale</li>\n<li>a crawler should select which information to download first, and which information to refresh over time</li>\n<li>a crawler should not be a burden for the web sites that host the content</li>\n<li>adversaries, e.g., spider traps, need to be avoided with care</li>\n</ol>\n<p>Mercator/Polybot/IBM WebFountain/Ubicrawl and etc..</p>\n<h2 id=\"III-Distributed-crawler-architecture\"><a href=\"#III-Distributed-crawler-architecture\" class=\"headerlink\" title=\"III. Distributed crawler architecture\"></a>III. Distributed crawler architecture</h2><h3 id=\"A-Single-site-Design\"><a href=\"#A-Single-site-Design\" class=\"headerlink\" title=\"A. Single site Design\"></a>A. Single site Design</h3><ol>\n<li>Map-reduce:<ul>\n<li>spill</li>\n<li>shuffle</li>\n<li>reduce</li>\n</ul>\n</li>\n<li>site storage<ul>\n<li>In UniCrawl, the crawl database of a site is implemented as a single distributed map structure. This map contains for each page its URL, content, and outlinks.</li>\n<li>INFINISPAN, a distributed key-value store stat supports the following features: <ul>\n<li>Routing: Notes are organized in a ring</li>\n<li>Elasticity</li>\n<li>Storage</li>\n<li>Reliability</li>\n<li>Interface</li>\n<li>Consistency</li>\n<li>Querying</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Detail of Phases<ul>\n<li>Generate: The goal of the generate phase is to select a set of pages to process during the round.</li>\n<li>Fetch: During the fetch phase, the map step first groups by host the pages that were generated in the previous phase.</li>\n<li>Parse: Once the pages are fetched, they are analyzed during the parse phase.</li>\n<li>Update: The goal of the update phase is to refresh the scores of pages that belong to the frontier in order to prioritize them.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"B-Multi-site-Operations\"><a href=\"#B-Multi-site-Operations\" class=\"headerlink\" title=\"B. Multi-site Operations\"></a>B. Multi-site Operations</h3><p>Several key ideas allow UniCrawl to be practical in this setting:</p>\n<ol>\n<li>Each site is independent and crawls the web autonomously</li>\n<li>We unite all the site data stores</li>\n<li><p>Sites exchanges dynamically the URLs they discover over the course of the crawl</p>\n</li>\n<li><p>Federating the storage: One of the key design concerns of UniCrawl is to bring small monifications to the site code base in order be usable over multiple geographical locations.</p>\n</li>\n<li>collaboration between sites: Following the approach advocated by Cho and Garcia-Molina. UniCrawl exchanges newly discovered URLs over time. This exchange occurs at the end of the update phase.<ul>\n<li>We implement the crawl database as a distributed ensemble map that span all the sites. This map operates in frontier mode with a replication factor of one.</li>\n</ul>\n</li>\n<li>Crawl quality and cost: The quality of the crawling operation is not only measured by means of pure web-graph exploration but also by the rounds it takes to discover the most interesting pages.</li>\n</ol>\n<h2 id=\"IV-Implementation\"><a href=\"#IV-Implementation\" class=\"headerlink\" title=\"IV. Implementation\"></a>IV. Implementation</h2><p>We implemented UniCrawl inJava, starting from the code base of Nutch version 2.5.3.</p>\n<p>Nutch makes use of Apache Gora, an open-source framework that provides an in-memory and persistent data model for big data.</p>\n<p>Intotal, our contribution accounts for about a dozen thousands lines of code (LOC) split as follows: 9.4 kLOC for Ensemble, 1.1 kLOC for Gora and 2.3 kLOC patch for Nutch</p>\n<h3 id=\"A-Merging-phases\"><a href=\"#A-Merging-phases\" class=\"headerlink\" title=\"A. Merging phases\"></a>A. Merging phases</h3><p>Cause each new map-reduce job creation is expensive as it requires to start a dedicated Java virtual machine, and deploy the appropriate jars. To lower this cost, we merge the fetch and parse phases in out UniCrawl implementation. This means that whenever a reducer fetches a new page, it parses its content and extract the out-links. These links are then directly inserted in the crawl database together with the fetched page.</p>\n<h3 id=\"B-Caching\"><a href=\"#B-Caching\" class=\"headerlink\" title=\"B. Caching\"></a>B. Caching</h3><p>To avoid sending out an URL multiple times across sites, we use a distributed solution. In more details, this cache is a bounded ENSENMBLE map C local to each site and replicated at all nodes in a site. During the update phase, when a reducer selects a URL in the frontier that is associated to a remote site, it first check locally with C is this URL woa previously sent. If this is the case , the reducer simply skips the call to putIfAbsent. Since C is replicated at all nodes, every map-reduce node is co-located with an INFINISPAN node, and C is in memory, this inclusion test costs less than a millisecond.</p>\n<h2 id=\"V-Evaluation\"><a href=\"#V-Evaluation\" class=\"headerlink\" title=\"V. Evaluation\"></a>V. Evaluation</h2><p>Evaluate UniCrawl through several key metrics such as the page processing rate, the memory usage and the network traffic across sites.</p>\n<p>Two parts:</p>\n<ol>\n<li>Evaluate our approach in-vitro, by running UniCrawl against the ClueWeb12 benchmark in an emulated multi-site architecture and crawling from a local repository.</li>\n<li>Report several experimental results where we deploy UniCrawl at multiple localtions in Germany and access actual web sites.</li>\n</ol>\n<h3 id=\"A-In-vitro-validation\"><a href=\"#A-In-vitro-validation\" class=\"headerlink\" title=\"A. In-vitro validation\"></a>A. In-vitro validation</h3><ol>\n<li>Single site performance</li>\n<li>Emulating multiple sites</li>\n</ol>\n<h3 id=\"B-UniCrawl-in-the-wild\"><a href=\"#B-UniCrawl-in-the-wild\" class=\"headerlink\" title=\"B. UniCrawl in the wild\"></a>B. UniCrawl in the wild</h3><ol>\n<li>URL Exchange</li>\n<li>Comparison with Nutch</li>\n<li>Scalability</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Cause the wealth of information available on the web keeps growing, we want to use web crawler to get them. But the traditional method has a fatal limit of its large infrastructure cost. To reduce it, we developed this method, unicrawl, which can show a performance improvement of 93.6% in terms of network bandwidth consumption, and a speedup factor of 1.75.</p>\n<h2 id=\"I-introduction\"><a href=\"#I-introduction\" class=\"headerlink\" title=\"I. introduction\"></a>I. introduction</h2><p>Nowadays, it’s common that to use parallel process on a large number of machines to achieve a reasonable collection time. While this method requires large computing infrastructures. Like Google and Bing, who rely on big data centers.<br>","more":"<br>As for the public crawl repositories, they require externalizing computation and data hosting to a commercial cloud provider. Which may pose the problem of data availability in the mid-long term. And cause there are large amounts of data unnecessary, postprocessing is needed.</p>\n<p>A solution to those problems is to distribute the crawling effort over several geographically distributed locations. For instance, by allowing several small companies to mutualize their crawling infrastructures. In addition, such an approach leverages data locality as sites can crawl web servers that are geographically nearby. But in this way, the synchronization between the crawler at the different sites is a new problem. Our goal is to reduce such communication costs.</p>\n<p>UniCrawl is an efficient geo-distributed crawler that aims at minimizing inter-site communication costs. Our design is both practical and scalable. We assess this claim with a detailed evaluation of UniCrawl in a controlled environment using the ClueWeb12 dataset, as well as in geo-distributed setting using 3 distinct sites located in Germany.</p>\n<h3 id=\"outline\"><a href=\"#outline\" class=\"headerlink\" title=\"outline:\"></a>outline:</h3><p>Section II is related work. Section III introduces the crawler architecture, refining it from existing well-founded central disigns. Section IV is the details about the internal implementaion. Section V presents the experimental results, both in-vitro, and in-vivo over multiple geographical locations in Germany. We discuss out results and future work in Section V. We conclude the paper inSection VII.</p>\n<h2 id=\"II-Related-work\"><a href=\"#II-Related-work\" class=\"headerlink\" title=\"II. Related work\"></a>II. Related work</h2><p>There are several problems for every crawler to solve:</p>\n<ol>\n<li>since the amount of information to parse is huge, a crawler must scale</li>\n<li>a crawler should select which information to download first, and which information to refresh over time</li>\n<li>a crawler should not be a burden for the web sites that host the content</li>\n<li>adversaries, e.g., spider traps, need to be avoided with care</li>\n</ol>\n<p>Mercator/Polybot/IBM WebFountain/Ubicrawl and etc..</p>\n<h2 id=\"III-Distributed-crawler-architecture\"><a href=\"#III-Distributed-crawler-architecture\" class=\"headerlink\" title=\"III. Distributed crawler architecture\"></a>III. Distributed crawler architecture</h2><h3 id=\"A-Single-site-Design\"><a href=\"#A-Single-site-Design\" class=\"headerlink\" title=\"A. Single site Design\"></a>A. Single site Design</h3><ol>\n<li>Map-reduce:<ul>\n<li>spill</li>\n<li>shuffle</li>\n<li>reduce</li>\n</ul>\n</li>\n<li>site storage<ul>\n<li>In UniCrawl, the crawl database of a site is implemented as a single distributed map structure. This map contains for each page its URL, content, and outlinks.</li>\n<li>INFINISPAN, a distributed key-value store stat supports the following features: <ul>\n<li>Routing: Notes are organized in a ring</li>\n<li>Elasticity</li>\n<li>Storage</li>\n<li>Reliability</li>\n<li>Interface</li>\n<li>Consistency</li>\n<li>Querying</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Detail of Phases<ul>\n<li>Generate: The goal of the generate phase is to select a set of pages to process during the round.</li>\n<li>Fetch: During the fetch phase, the map step first groups by host the pages that were generated in the previous phase.</li>\n<li>Parse: Once the pages are fetched, they are analyzed during the parse phase.</li>\n<li>Update: The goal of the update phase is to refresh the scores of pages that belong to the frontier in order to prioritize them.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"B-Multi-site-Operations\"><a href=\"#B-Multi-site-Operations\" class=\"headerlink\" title=\"B. Multi-site Operations\"></a>B. Multi-site Operations</h3><p>Several key ideas allow UniCrawl to be practical in this setting:</p>\n<ol>\n<li>Each site is independent and crawls the web autonomously</li>\n<li>We unite all the site data stores</li>\n<li><p>Sites exchanges dynamically the URLs they discover over the course of the crawl</p>\n</li>\n<li><p>Federating the storage: One of the key design concerns of UniCrawl is to bring small monifications to the site code base in order be usable over multiple geographical locations.</p>\n</li>\n<li>collaboration between sites: Following the approach advocated by Cho and Garcia-Molina. UniCrawl exchanges newly discovered URLs over time. This exchange occurs at the end of the update phase.<ul>\n<li>We implement the crawl database as a distributed ensemble map that span all the sites. This map operates in frontier mode with a replication factor of one.</li>\n</ul>\n</li>\n<li>Crawl quality and cost: The quality of the crawling operation is not only measured by means of pure web-graph exploration but also by the rounds it takes to discover the most interesting pages.</li>\n</ol>\n<h2 id=\"IV-Implementation\"><a href=\"#IV-Implementation\" class=\"headerlink\" title=\"IV. Implementation\"></a>IV. Implementation</h2><p>We implemented UniCrawl inJava, starting from the code base of Nutch version 2.5.3.</p>\n<p>Nutch makes use of Apache Gora, an open-source framework that provides an in-memory and persistent data model for big data.</p>\n<p>Intotal, our contribution accounts for about a dozen thousands lines of code (LOC) split as follows: 9.4 kLOC for Ensemble, 1.1 kLOC for Gora and 2.3 kLOC patch for Nutch</p>\n<h3 id=\"A-Merging-phases\"><a href=\"#A-Merging-phases\" class=\"headerlink\" title=\"A. Merging phases\"></a>A. Merging phases</h3><p>Cause each new map-reduce job creation is expensive as it requires to start a dedicated Java virtual machine, and deploy the appropriate jars. To lower this cost, we merge the fetch and parse phases in out UniCrawl implementation. This means that whenever a reducer fetches a new page, it parses its content and extract the out-links. These links are then directly inserted in the crawl database together with the fetched page.</p>\n<h3 id=\"B-Caching\"><a href=\"#B-Caching\" class=\"headerlink\" title=\"B. Caching\"></a>B. Caching</h3><p>To avoid sending out an URL multiple times across sites, we use a distributed solution. In more details, this cache is a bounded ENSENMBLE map C local to each site and replicated at all nodes in a site. During the update phase, when a reducer selects a URL in the frontier that is associated to a remote site, it first check locally with C is this URL woa previously sent. If this is the case , the reducer simply skips the call to putIfAbsent. Since C is replicated at all nodes, every map-reduce node is co-located with an INFINISPAN node, and C is in memory, this inclusion test costs less than a millisecond.</p>\n<h2 id=\"V-Evaluation\"><a href=\"#V-Evaluation\" class=\"headerlink\" title=\"V. Evaluation\"></a>V. Evaluation</h2><p>Evaluate UniCrawl through several key metrics such as the page processing rate, the memory usage and the network traffic across sites.</p>\n<p>Two parts:</p>\n<ol>\n<li>Evaluate our approach in-vitro, by running UniCrawl against the ClueWeb12 benchmark in an emulated multi-site architecture and crawling from a local repository.</li>\n<li>Report several experimental results where we deploy UniCrawl at multiple localtions in Germany and access actual web sites.</li>\n</ol>\n<h3 id=\"A-In-vitro-validation\"><a href=\"#A-In-vitro-validation\" class=\"headerlink\" title=\"A. In-vitro validation\"></a>A. In-vitro validation</h3><ol>\n<li>Single site performance</li>\n<li>Emulating multiple sites</li>\n</ol>\n<h3 id=\"B-UniCrawl-in-the-wild\"><a href=\"#B-UniCrawl-in-the-wild\" class=\"headerlink\" title=\"B. UniCrawl in the wild\"></a>B. UniCrawl in the wild</h3><ol>\n<li>URL Exchange</li>\n<li>Comparison with Nutch</li>\n<li>Scalability</li>\n</ol>"},{"title":"Hello World","comments":1,"_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n<!-- more-->\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncomments: true\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n<!-- more-->\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2016-12-10T15:58:54.000Z","updated":"2018-07-31T04:53:09.314Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqd000dt7x0yd6l7i2p","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a><br><a id=\"more\"></a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a><br>","more":"</p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>"},{"title":"[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model","date":"2016-12-13T14:58:00.000Z","comments":1,"_content":"\n## Abstract\n\nThis article propose an object detection system that relies on a multi-region deep convolutional neural network that also encodes sematic segmentation-aware features. The module aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. They exploit the above properties of their recognition module by intergrating it on an iterative localization mechanism that alternates between socring a box proposal and refgining its location with a deep CNN regression model. And consiquently, they detect objects with very high localization accuracy.\n\n## I. Introduction\n\nThe definition of object detection:\n\n> Given an image return all the instances of one or more type of objects in form of bounding boxes that tightly enclose them.\n\n<!-- more-->\n\nOverfeat:\n\n> Using two CNN models that apply in a sliding window fashion on multiple scales of an image. The first is used to classify if a window contains an object. The second is to predict the true bounding box location of the object. And use **greedy algorithm** in the end to merge them.\n\nR-CNN:\n> Using **Alex Krizhevsky's Net**  to extract features from box proposals provided by selective search and then it classifies them with class specific linear SVMs.\n\nHow to further advance the state-of-the-art on object detection?\n\n> Focusing on object representation and object localization. \n\nObject representation:\n\n> Indeed features matter a lot on object detection. Instead of proposing only a network architecture that is deeper, here they also opt for an architecture of greater width. And that was accomplished at two levels:\n\n> 1. They want their object representation to capture several different aspects of an object. To achieve this, they propose a multi-component CNN model (multi-region CNN). Each component of it is steered to focus on a different region.\n> 2. They wish to enrich the above representation so that it also captures semantic segmentation information\n\nObject localization:\n\n> They attempt to built a more powerful localization system that combines their multi-region CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and reﬁning their coordinates.\n\nTheir Contributions:\n\n> 1. They develop a multi-region CNN recognition model that yields an enriched object representation capable of capturing a diversity of discriminative appearance factors and of exhibiting localization sensitivity that is desired for the task of accurate object localization.\n> 2. They furthermore extend the above model by proposing a uniﬁed neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection.\n> 3. They show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression.\n> 4. Their detection system achieves **mAP** of 78.2% and 73.9% on **VOC2007 and VOC2012** detection challenges respectively.\n\n## II. Multi-Region CNN Model\n\n1. Activation maps module\n - This part of the network gets as input the entire image and outputs activation maps (feature maps) by forwarding it through a sequence of convolutional layers.\n2. Region adaptation module\n - Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer, and then forwards them through a multi-layer network.\n\nThis is the architecture of the Multi-Region CNN model:\n\n![Figure2](http://ohykn376o.bkt.clouddn.com/OBDE2.png)\n\nThere are two aims of that:\n\n> 1. to force the network to capture various complementary aspects of the objects appearance, thus leading to a much richer and more robust object representation\n> 2. to also make the resulting representation more sensitive to inaccurate localization, which is also crucial for object detection\n\n***The regions they deploy:***\n\n> 1. Original candidate box\n> 2. Half boxes\n> 3. Central Regions\n> 4. Border Regions\n> 5. Contextual Region\n\n***Why these regions helps?***\n\n> 1. Discriminative feature diversification\n> 2. Localization-aware representation\n\n## III. Semantic Segmentation-Aware CNN Model\n\n![figure4](http://ohykn376o.bkt.clouddn.com/OBDE1.png)\n\n - Activation maps module for semantic segmentation aware features\n     - ***Weakly supervised training***(see Figure 4)\n     - Activation maps\n - Region adaptation module for semantic segmentation aware features\n \nThey combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating them. The resulting network thus jointly learns deep features of both types during training.\n\n## IV. Object Localization\n\nThere are three main components in this section:\n\n1. CNN region adaptation module for bounding box regression\n\n - It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model, consists of two hidden fully connected layers and one prediction layer that outputs 4 values per category. In order to allow it to predict the location of object instances that are not in the close proximity of any of the initial candidate boxes, we Use as region a box obtained by enlarging the candidate box by a factor of 1.3. This combination offers a significant boost on the detection performance of out system by allowing it to make more accurate predictions and for more distant objects.\n\n2. Iterative Localization\n\n - Their localization scheme starts from the selective search proposals and works by iteratively scoring them and refining their coordinates.\n\n3. Bounding box voting\n\n - Because of the multiple regression steps, the generated boxes will be highly concentrated around the actual objects of interest. They exploit this \"by-product\" of the iterative localization scheme by adding a step of bounding box voting.\n\n## V. Implementation Details\n\n> For all the CNN models involved in their proposed system, we used the publicly available **16-layers VGG model** pre-trained on ImageNet for the task of image classification.\n\nMulti-Region CNN model\n\n> Its activation maps module consistes of the convolustional part of the 16-layers VGG-Net that outputs 512 feature channels. The max-pooling layer right after the last convolutional layer is omitted on this module. Each region adaptation module inherits the fully connected layers of the 16-layers VGG-Net and is fine-tuned separately from the others.\n\nSemantic segmentation-aware CNN model\n\n> The activation maps module architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a Fully Convolutional Network.\n\nClassification SVMs\n\n> The ground truth bounding boxes are used as positive samples and the selective search proposals that overlap with the ground truth boxes by less than 0.3, are used as negative samples.\n\nCNN region adaptation module for bounding box regression\n\n> The region adaptation module for bounding box regression inherits the fully connected hidden layers of the 16-layers VGG-Net. As a loss function they use the euclidean distance between the target values and the network predictions.\n\n## VI. Experimental Evaluation\n\n1. Results on PASCAL VOC2007\n2. Detection error analysis\n3. Results on PASCAL VOC2012\n\n## VII. Conclusion\n\nTwo key factors:\n\n1. the diversification of the discriminative appearance factors that it captures by steering its focus on different regions of the object\n2. the encoding of semantic segmentation-aware features. By using it in the context of a CNN-based localization refinement scheme, they showed that it achieves excellent results that surpass the state-of-the-are by a significant margin","source":"_posts/[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model.md","raw":"---\ntitle: \"[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model\"\ndate: 2016-12-13 22:58:00\ntags: [object detection, CNN, multi-region, sematic segmentation-aware, Reading Notes]\ncomments: true\n---\n\n## Abstract\n\nThis article propose an object detection system that relies on a multi-region deep convolutional neural network that also encodes sematic segmentation-aware features. The module aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. They exploit the above properties of their recognition module by intergrating it on an iterative localization mechanism that alternates between socring a box proposal and refgining its location with a deep CNN regression model. And consiquently, they detect objects with very high localization accuracy.\n\n## I. Introduction\n\nThe definition of object detection:\n\n> Given an image return all the instances of one or more type of objects in form of bounding boxes that tightly enclose them.\n\n<!-- more-->\n\nOverfeat:\n\n> Using two CNN models that apply in a sliding window fashion on multiple scales of an image. The first is used to classify if a window contains an object. The second is to predict the true bounding box location of the object. And use **greedy algorithm** in the end to merge them.\n\nR-CNN:\n> Using **Alex Krizhevsky's Net**  to extract features from box proposals provided by selective search and then it classifies them with class specific linear SVMs.\n\nHow to further advance the state-of-the-art on object detection?\n\n> Focusing on object representation and object localization. \n\nObject representation:\n\n> Indeed features matter a lot on object detection. Instead of proposing only a network architecture that is deeper, here they also opt for an architecture of greater width. And that was accomplished at two levels:\n\n> 1. They want their object representation to capture several different aspects of an object. To achieve this, they propose a multi-component CNN model (multi-region CNN). Each component of it is steered to focus on a different region.\n> 2. They wish to enrich the above representation so that it also captures semantic segmentation information\n\nObject localization:\n\n> They attempt to built a more powerful localization system that combines their multi-region CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and reﬁning their coordinates.\n\nTheir Contributions:\n\n> 1. They develop a multi-region CNN recognition model that yields an enriched object representation capable of capturing a diversity of discriminative appearance factors and of exhibiting localization sensitivity that is desired for the task of accurate object localization.\n> 2. They furthermore extend the above model by proposing a uniﬁed neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection.\n> 3. They show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression.\n> 4. Their detection system achieves **mAP** of 78.2% and 73.9% on **VOC2007 and VOC2012** detection challenges respectively.\n\n## II. Multi-Region CNN Model\n\n1. Activation maps module\n - This part of the network gets as input the entire image and outputs activation maps (feature maps) by forwarding it through a sequence of convolutional layers.\n2. Region adaptation module\n - Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer, and then forwards them through a multi-layer network.\n\nThis is the architecture of the Multi-Region CNN model:\n\n![Figure2](http://ohykn376o.bkt.clouddn.com/OBDE2.png)\n\nThere are two aims of that:\n\n> 1. to force the network to capture various complementary aspects of the objects appearance, thus leading to a much richer and more robust object representation\n> 2. to also make the resulting representation more sensitive to inaccurate localization, which is also crucial for object detection\n\n***The regions they deploy:***\n\n> 1. Original candidate box\n> 2. Half boxes\n> 3. Central Regions\n> 4. Border Regions\n> 5. Contextual Region\n\n***Why these regions helps?***\n\n> 1. Discriminative feature diversification\n> 2. Localization-aware representation\n\n## III. Semantic Segmentation-Aware CNN Model\n\n![figure4](http://ohykn376o.bkt.clouddn.com/OBDE1.png)\n\n - Activation maps module for semantic segmentation aware features\n     - ***Weakly supervised training***(see Figure 4)\n     - Activation maps\n - Region adaptation module for semantic segmentation aware features\n \nThey combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating them. The resulting network thus jointly learns deep features of both types during training.\n\n## IV. Object Localization\n\nThere are three main components in this section:\n\n1. CNN region adaptation module for bounding box regression\n\n - It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model, consists of two hidden fully connected layers and one prediction layer that outputs 4 values per category. In order to allow it to predict the location of object instances that are not in the close proximity of any of the initial candidate boxes, we Use as region a box obtained by enlarging the candidate box by a factor of 1.3. This combination offers a significant boost on the detection performance of out system by allowing it to make more accurate predictions and for more distant objects.\n\n2. Iterative Localization\n\n - Their localization scheme starts from the selective search proposals and works by iteratively scoring them and refining their coordinates.\n\n3. Bounding box voting\n\n - Because of the multiple regression steps, the generated boxes will be highly concentrated around the actual objects of interest. They exploit this \"by-product\" of the iterative localization scheme by adding a step of bounding box voting.\n\n## V. Implementation Details\n\n> For all the CNN models involved in their proposed system, we used the publicly available **16-layers VGG model** pre-trained on ImageNet for the task of image classification.\n\nMulti-Region CNN model\n\n> Its activation maps module consistes of the convolustional part of the 16-layers VGG-Net that outputs 512 feature channels. The max-pooling layer right after the last convolutional layer is omitted on this module. Each region adaptation module inherits the fully connected layers of the 16-layers VGG-Net and is fine-tuned separately from the others.\n\nSemantic segmentation-aware CNN model\n\n> The activation maps module architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a Fully Convolutional Network.\n\nClassification SVMs\n\n> The ground truth bounding boxes are used as positive samples and the selective search proposals that overlap with the ground truth boxes by less than 0.3, are used as negative samples.\n\nCNN region adaptation module for bounding box regression\n\n> The region adaptation module for bounding box regression inherits the fully connected hidden layers of the 16-layers VGG-Net. As a loss function they use the euclidean distance between the target values and the network predictions.\n\n## VI. Experimental Evaluation\n\n1. Results on PASCAL VOC2007\n2. Detection error analysis\n3. Results on PASCAL VOC2012\n\n## VII. Conclusion\n\nTwo key factors:\n\n1. the diversification of the discriminative appearance factors that it captures by steering its focus on different regions of the object\n2. the encoding of semantic segmentation-aware features. By using it in the context of a CNN-based localization refinement scheme, they showed that it achieves excellent results that surpass the state-of-the-are by a significant margin","slug":"[Reading Notes] Object detection via a multi-region & sematic segmentation-aware CNN model","published":1,"updated":"2016-12-16T03:30:10.000Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqf000et7x046atqnb6","content":"<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This article propose an object detection system that relies on a multi-region deep convolutional neural network that also encodes sematic segmentation-aware features. The module aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. They exploit the above properties of their recognition module by intergrating it on an iterative localization mechanism that alternates between socring a box proposal and refgining its location with a deep CNN regression model. And consiquently, they detect objects with very high localization accuracy.</p>\n<h2 id=\"I-Introduction\"><a href=\"#I-Introduction\" class=\"headerlink\" title=\"I. Introduction\"></a>I. Introduction</h2><p>The definition of object detection:</p>\n<blockquote>\n<p>Given an image return all the instances of one or more type of objects in form of bounding boxes that tightly enclose them.</p>\n</blockquote>\n<a id=\"more\"></a>\n<p>Overfeat:</p>\n<blockquote>\n<p>Using two CNN models that apply in a sliding window fashion on multiple scales of an image. The first is used to classify if a window contains an object. The second is to predict the true bounding box location of the object. And use <strong>greedy algorithm</strong> in the end to merge them.</p>\n</blockquote>\n<p>R-CNN:</p>\n<blockquote>\n<p>Using <strong>Alex Krizhevsky’s Net</strong>  to extract features from box proposals provided by selective search and then it classifies them with class specific linear SVMs.</p>\n</blockquote>\n<p>How to further advance the state-of-the-art on object detection?</p>\n<blockquote>\n<p>Focusing on object representation and object localization. </p>\n</blockquote>\n<p>Object representation:</p>\n<blockquote>\n<p>Indeed features matter a lot on object detection. Instead of proposing only a network architecture that is deeper, here they also opt for an architecture of greater width. And that was accomplished at two levels:</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>They want their object representation to capture several different aspects of an object. To achieve this, they propose a multi-component CNN model (multi-region CNN). Each component of it is steered to focus on a different region.</li>\n<li>They wish to enrich the above representation so that it also captures semantic segmentation information</li>\n</ol>\n</blockquote>\n<p>Object localization:</p>\n<blockquote>\n<p>They attempt to built a more powerful localization system that combines their multi-region CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and reﬁning their coordinates.</p>\n</blockquote>\n<p>Their Contributions:</p>\n<blockquote>\n<ol>\n<li>They develop a multi-region CNN recognition model that yields an enriched object representation capable of capturing a diversity of discriminative appearance factors and of exhibiting localization sensitivity that is desired for the task of accurate object localization.</li>\n<li>They furthermore extend the above model by proposing a uniﬁed neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection.</li>\n<li>They show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression.</li>\n<li>Their detection system achieves <strong>mAP</strong> of 78.2% and 73.9% on <strong>VOC2007 and VOC2012</strong> detection challenges respectively.</li>\n</ol>\n</blockquote>\n<h2 id=\"II-Multi-Region-CNN-Model\"><a href=\"#II-Multi-Region-CNN-Model\" class=\"headerlink\" title=\"II. Multi-Region CNN Model\"></a>II. Multi-Region CNN Model</h2><ol>\n<li>Activation maps module<ul>\n<li>This part of the network gets as input the entire image and outputs activation maps (feature maps) by forwarding it through a sequence of convolutional layers.</li>\n</ul>\n</li>\n<li>Region adaptation module<ul>\n<li>Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer, and then forwards them through a multi-layer network.</li>\n</ul>\n</li>\n</ol>\n<p>This is the architecture of the Multi-Region CNN model:</p>\n<p><img src=\"http://ohykn376o.bkt.clouddn.com/OBDE2.png\" alt=\"Figure2\"></p>\n<p>There are two aims of that:</p>\n<blockquote>\n<ol>\n<li>to force the network to capture various complementary aspects of the objects appearance, thus leading to a much richer and more robust object representation</li>\n<li>to also make the resulting representation more sensitive to inaccurate localization, which is also crucial for object detection</li>\n</ol>\n</blockquote>\n<p><strong><em>The regions they deploy:</em></strong></p>\n<blockquote>\n<ol>\n<li>Original candidate box</li>\n<li>Half boxes</li>\n<li>Central Regions</li>\n<li>Border Regions</li>\n<li>Contextual Region</li>\n</ol>\n</blockquote>\n<p><strong><em>Why these regions helps?</em></strong></p>\n<blockquote>\n<ol>\n<li>Discriminative feature diversification</li>\n<li>Localization-aware representation</li>\n</ol>\n</blockquote>\n<h2 id=\"III-Semantic-Segmentation-Aware-CNN-Model\"><a href=\"#III-Semantic-Segmentation-Aware-CNN-Model\" class=\"headerlink\" title=\"III. Semantic Segmentation-Aware CNN Model\"></a>III. Semantic Segmentation-Aware CNN Model</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/OBDE1.png\" alt=\"figure4\"></p>\n<ul>\n<li>Activation maps module for semantic segmentation aware features<ul>\n<li><strong><em>Weakly supervised training</em></strong>(see Figure 4)</li>\n<li>Activation maps</li>\n</ul>\n</li>\n<li>Region adaptation module for semantic segmentation aware features</li>\n</ul>\n<p>They combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating them. The resulting network thus jointly learns deep features of both types during training.</p>\n<h2 id=\"IV-Object-Localization\"><a href=\"#IV-Object-Localization\" class=\"headerlink\" title=\"IV. Object Localization\"></a>IV. Object Localization</h2><p>There are three main components in this section:</p>\n<ol>\n<li><p>CNN region adaptation module for bounding box regression</p>\n<ul>\n<li>It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model, consists of two hidden fully connected layers and one prediction layer that outputs 4 values per category. In order to allow it to predict the location of object instances that are not in the close proximity of any of the initial candidate boxes, we Use as region a box obtained by enlarging the candidate box by a factor of 1.3. This combination offers a significant boost on the detection performance of out system by allowing it to make more accurate predictions and for more distant objects.</li>\n</ul>\n</li>\n<li><p>Iterative Localization</p>\n<ul>\n<li>Their localization scheme starts from the selective search proposals and works by iteratively scoring them and refining their coordinates.</li>\n</ul>\n</li>\n<li><p>Bounding box voting</p>\n<ul>\n<li>Because of the multiple regression steps, the generated boxes will be highly concentrated around the actual objects of interest. They exploit this “by-product” of the iterative localization scheme by adding a step of bounding box voting.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"V-Implementation-Details\"><a href=\"#V-Implementation-Details\" class=\"headerlink\" title=\"V. Implementation Details\"></a>V. Implementation Details</h2><blockquote>\n<p>For all the CNN models involved in their proposed system, we used the publicly available <strong>16-layers VGG model</strong> pre-trained on ImageNet for the task of image classification.</p>\n</blockquote>\n<p>Multi-Region CNN model</p>\n<blockquote>\n<p>Its activation maps module consistes of the convolustional part of the 16-layers VGG-Net that outputs 512 feature channels. The max-pooling layer right after the last convolutional layer is omitted on this module. Each region adaptation module inherits the fully connected layers of the 16-layers VGG-Net and is fine-tuned separately from the others.</p>\n</blockquote>\n<p>Semantic segmentation-aware CNN model</p>\n<blockquote>\n<p>The activation maps module architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a Fully Convolutional Network.</p>\n</blockquote>\n<p>Classification SVMs</p>\n<blockquote>\n<p>The ground truth bounding boxes are used as positive samples and the selective search proposals that overlap with the ground truth boxes by less than 0.3, are used as negative samples.</p>\n</blockquote>\n<p>CNN region adaptation module for bounding box regression</p>\n<blockquote>\n<p>The region adaptation module for bounding box regression inherits the fully connected hidden layers of the 16-layers VGG-Net. As a loss function they use the euclidean distance between the target values and the network predictions.</p>\n</blockquote>\n<h2 id=\"VI-Experimental-Evaluation\"><a href=\"#VI-Experimental-Evaluation\" class=\"headerlink\" title=\"VI. Experimental Evaluation\"></a>VI. Experimental Evaluation</h2><ol>\n<li>Results on PASCAL VOC2007</li>\n<li>Detection error analysis</li>\n<li>Results on PASCAL VOC2012</li>\n</ol>\n<h2 id=\"VII-Conclusion\"><a href=\"#VII-Conclusion\" class=\"headerlink\" title=\"VII. Conclusion\"></a>VII. Conclusion</h2><p>Two key factors:</p>\n<ol>\n<li>the diversification of the discriminative appearance factors that it captures by steering its focus on different regions of the object</li>\n<li>the encoding of semantic segmentation-aware features. By using it in the context of a CNN-based localization refinement scheme, they showed that it achieves excellent results that surpass the state-of-the-are by a significant margin</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This article propose an object detection system that relies on a multi-region deep convolutional neural network that also encodes sematic segmentation-aware features. The module aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. They exploit the above properties of their recognition module by intergrating it on an iterative localization mechanism that alternates between socring a box proposal and refgining its location with a deep CNN regression model. And consiquently, they detect objects with very high localization accuracy.</p>\n<h2 id=\"I-Introduction\"><a href=\"#I-Introduction\" class=\"headerlink\" title=\"I. Introduction\"></a>I. Introduction</h2><p>The definition of object detection:</p>\n<blockquote>\n<p>Given an image return all the instances of one or more type of objects in form of bounding boxes that tightly enclose them.</p>\n</blockquote>","more":"<p>Overfeat:</p>\n<blockquote>\n<p>Using two CNN models that apply in a sliding window fashion on multiple scales of an image. The first is used to classify if a window contains an object. The second is to predict the true bounding box location of the object. And use <strong>greedy algorithm</strong> in the end to merge them.</p>\n</blockquote>\n<p>R-CNN:</p>\n<blockquote>\n<p>Using <strong>Alex Krizhevsky’s Net</strong>  to extract features from box proposals provided by selective search and then it classifies them with class specific linear SVMs.</p>\n</blockquote>\n<p>How to further advance the state-of-the-art on object detection?</p>\n<blockquote>\n<p>Focusing on object representation and object localization. </p>\n</blockquote>\n<p>Object representation:</p>\n<blockquote>\n<p>Indeed features matter a lot on object detection. Instead of proposing only a network architecture that is deeper, here they also opt for an architecture of greater width. And that was accomplished at two levels:</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>They want their object representation to capture several different aspects of an object. To achieve this, they propose a multi-component CNN model (multi-region CNN). Each component of it is steered to focus on a different region.</li>\n<li>They wish to enrich the above representation so that it also captures semantic segmentation information</li>\n</ol>\n</blockquote>\n<p>Object localization:</p>\n<blockquote>\n<p>They attempt to built a more powerful localization system that combines their multi-region CNN model with a CNN-model for bounding box regression, which are used within an iterative scheme that alternates between scoring candidate boxes and reﬁning their coordinates.</p>\n</blockquote>\n<p>Their Contributions:</p>\n<blockquote>\n<ol>\n<li>They develop a multi-region CNN recognition model that yields an enriched object representation capable of capturing a diversity of discriminative appearance factors and of exhibiting localization sensitivity that is desired for the task of accurate object localization.</li>\n<li>They furthermore extend the above model by proposing a uniﬁed neural network architecture that also learns semantic segmentation-aware CNN features for the task of object detection.</li>\n<li>They show how to significantly improve the localization capability by coupling the aforementioned CNN recognition model with a CNN model for bounding box regression.</li>\n<li>Their detection system achieves <strong>mAP</strong> of 78.2% and 73.9% on <strong>VOC2007 and VOC2012</strong> detection challenges respectively.</li>\n</ol>\n</blockquote>\n<h2 id=\"II-Multi-Region-CNN-Model\"><a href=\"#II-Multi-Region-CNN-Model\" class=\"headerlink\" title=\"II. Multi-Region CNN Model\"></a>II. Multi-Region CNN Model</h2><ol>\n<li>Activation maps module<ul>\n<li>This part of the network gets as input the entire image and outputs activation maps (feature maps) by forwarding it through a sequence of convolutional layers.</li>\n</ul>\n</li>\n<li>Region adaptation module<ul>\n<li>Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer, and then forwards them through a multi-layer network.</li>\n</ul>\n</li>\n</ol>\n<p>This is the architecture of the Multi-Region CNN model:</p>\n<p><img src=\"http://ohykn376o.bkt.clouddn.com/OBDE2.png\" alt=\"Figure2\"></p>\n<p>There are two aims of that:</p>\n<blockquote>\n<ol>\n<li>to force the network to capture various complementary aspects of the objects appearance, thus leading to a much richer and more robust object representation</li>\n<li>to also make the resulting representation more sensitive to inaccurate localization, which is also crucial for object detection</li>\n</ol>\n</blockquote>\n<p><strong><em>The regions they deploy:</em></strong></p>\n<blockquote>\n<ol>\n<li>Original candidate box</li>\n<li>Half boxes</li>\n<li>Central Regions</li>\n<li>Border Regions</li>\n<li>Contextual Region</li>\n</ol>\n</blockquote>\n<p><strong><em>Why these regions helps?</em></strong></p>\n<blockquote>\n<ol>\n<li>Discriminative feature diversification</li>\n<li>Localization-aware representation</li>\n</ol>\n</blockquote>\n<h2 id=\"III-Semantic-Segmentation-Aware-CNN-Model\"><a href=\"#III-Semantic-Segmentation-Aware-CNN-Model\" class=\"headerlink\" title=\"III. Semantic Segmentation-Aware CNN Model\"></a>III. Semantic Segmentation-Aware CNN Model</h2><p><img src=\"http://ohykn376o.bkt.clouddn.com/OBDE1.png\" alt=\"figure4\"></p>\n<ul>\n<li>Activation maps module for semantic segmentation aware features<ul>\n<li><strong><em>Weakly supervised training</em></strong>(see Figure 4)</li>\n<li>Activation maps</li>\n</ul>\n</li>\n<li>Region adaptation module for semantic segmentation aware features</li>\n</ul>\n<p>They combine the Multi-Region CNN features and the semantic segmentation aware CNN features by concatenating them. The resulting network thus jointly learns deep features of both types during training.</p>\n<h2 id=\"IV-Object-Localization\"><a href=\"#IV-Object-Localization\" class=\"headerlink\" title=\"IV. Object Localization\"></a>IV. Object Localization</h2><p>There are three main components in this section:</p>\n<ol>\n<li><p>CNN region adaptation module for bounding box regression</p>\n<ul>\n<li>It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model, consists of two hidden fully connected layers and one prediction layer that outputs 4 values per category. In order to allow it to predict the location of object instances that are not in the close proximity of any of the initial candidate boxes, we Use as region a box obtained by enlarging the candidate box by a factor of 1.3. This combination offers a significant boost on the detection performance of out system by allowing it to make more accurate predictions and for more distant objects.</li>\n</ul>\n</li>\n<li><p>Iterative Localization</p>\n<ul>\n<li>Their localization scheme starts from the selective search proposals and works by iteratively scoring them and refining their coordinates.</li>\n</ul>\n</li>\n<li><p>Bounding box voting</p>\n<ul>\n<li>Because of the multiple regression steps, the generated boxes will be highly concentrated around the actual objects of interest. They exploit this “by-product” of the iterative localization scheme by adding a step of bounding box voting.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"V-Implementation-Details\"><a href=\"#V-Implementation-Details\" class=\"headerlink\" title=\"V. Implementation Details\"></a>V. Implementation Details</h2><blockquote>\n<p>For all the CNN models involved in their proposed system, we used the publicly available <strong>16-layers VGG model</strong> pre-trained on ImageNet for the task of image classification.</p>\n</blockquote>\n<p>Multi-Region CNN model</p>\n<blockquote>\n<p>Its activation maps module consistes of the convolustional part of the 16-layers VGG-Net that outputs 512 feature channels. The max-pooling layer right after the last convolutional layer is omitted on this module. Each region adaptation module inherits the fully connected layers of the 16-layers VGG-Net and is fine-tuned separately from the others.</p>\n</blockquote>\n<p>Semantic segmentation-aware CNN model</p>\n<blockquote>\n<p>The activation maps module architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a Fully Convolutional Network.</p>\n</blockquote>\n<p>Classification SVMs</p>\n<blockquote>\n<p>The ground truth bounding boxes are used as positive samples and the selective search proposals that overlap with the ground truth boxes by less than 0.3, are used as negative samples.</p>\n</blockquote>\n<p>CNN region adaptation module for bounding box regression</p>\n<blockquote>\n<p>The region adaptation module for bounding box regression inherits the fully connected hidden layers of the 16-layers VGG-Net. As a loss function they use the euclidean distance between the target values and the network predictions.</p>\n</blockquote>\n<h2 id=\"VI-Experimental-Evaluation\"><a href=\"#VI-Experimental-Evaluation\" class=\"headerlink\" title=\"VI. Experimental Evaluation\"></a>VI. Experimental Evaluation</h2><ol>\n<li>Results on PASCAL VOC2007</li>\n<li>Detection error analysis</li>\n<li>Results on PASCAL VOC2012</li>\n</ol>\n<h2 id=\"VII-Conclusion\"><a href=\"#VII-Conclusion\" class=\"headerlink\" title=\"VII. Conclusion\"></a>VII. Conclusion</h2><p>Two key factors:</p>\n<ol>\n<li>the diversification of the discriminative appearance factors that it captures by steering its focus on different regions of the object</li>\n<li>the encoding of semantic segmentation-aware features. By using it in the context of a CNN-based localization refinement scheme, they showed that it achieves excellent results that surpass the state-of-the-are by a significant margin</li>\n</ol>"},{"title":"jupyter配置远程访问","date":"2018-09-19T03:10:45.000Z","_content":"\n# 安装jupyter\n\n> pip install ipython\n> pip install jupyter\n\n# 生成jupyter配置文件\n\n> jupyter notebook --generate-config\n\n```\nxm@r2d2:~$ jupyter notebook --generate-config\nWriting default config to: /home/xm/.jupyter/jupyter_notebook_config.py\n```\n\n# 自动生成密码\n\n> jupyter notebook password\n\n```\nxm@r2d2:~$ jupyter notebook password\nEnter password: # 这里输入密码不会显示字符的\nVerify password: \n[NotebookPasswordApp] Wrote hashed password to /home/xm/.jupyter/jupyter_notebook_config.json\n# 密码已经被加密记录到这个文件中了\n```\n\n# 获取密码\n\n> cat /home/xm/.jupyter/jupyter_notebook_config.json\n\n```\nxm@r2d2:~$ cat /home/xm/.jupyter/jupyter_notebook_config.json\n{\n  \"NotebookApp\": {\n    \"password\": \"这是你的密码，一整段都复制\b下来\"\n  }\n}\n```\n\n# 修改配置文件\n\n> vim /home/xm/.jupyter/jupyter_notebook_config.py\n\n```\n#懒得找对应配置项的朋友，直接把这四项配置写到文件开头就可以了\nc.NotebookApp.ip = '*'\nc.NotebookApp.password = 'sha:ce...刚才复制的那个密文'\nc.NotebookApp.open_browser = False\nc.NotebookApp.port = 8888 #可自行指定一个端口, 访问时使用该端口\n```","source":"_posts/jupyter配置远程访问.md","raw":"---\ntitle: jupyter配置远程访问\ndate: 2018-09-19 11:10:45\ntags: [jupyter,远程访问,ipython,notebook]\n---\n\n# 安装jupyter\n\n> pip install ipython\n> pip install jupyter\n\n# 生成jupyter配置文件\n\n> jupyter notebook --generate-config\n\n```\nxm@r2d2:~$ jupyter notebook --generate-config\nWriting default config to: /home/xm/.jupyter/jupyter_notebook_config.py\n```\n\n# 自动生成密码\n\n> jupyter notebook password\n\n```\nxm@r2d2:~$ jupyter notebook password\nEnter password: # 这里输入密码不会显示字符的\nVerify password: \n[NotebookPasswordApp] Wrote hashed password to /home/xm/.jupyter/jupyter_notebook_config.json\n# 密码已经被加密记录到这个文件中了\n```\n\n# 获取密码\n\n> cat /home/xm/.jupyter/jupyter_notebook_config.json\n\n```\nxm@r2d2:~$ cat /home/xm/.jupyter/jupyter_notebook_config.json\n{\n  \"NotebookApp\": {\n    \"password\": \"这是你的密码，一整段都复制\b下来\"\n  }\n}\n```\n\n# 修改配置文件\n\n> vim /home/xm/.jupyter/jupyter_notebook_config.py\n\n```\n#懒得找对应配置项的朋友，直接把这四项配置写到文件开头就可以了\nc.NotebookApp.ip = '*'\nc.NotebookApp.password = 'sha:ce...刚才复制的那个密文'\nc.NotebookApp.open_browser = False\nc.NotebookApp.port = 8888 #可自行指定一个端口, 访问时使用该端口\n```","slug":"jupyter配置远程访问","published":1,"updated":"2018-09-19T03:28:23.056Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiqg000gt7x0ci9yicgp","content":"<h1 id=\"安装jupyter\"><a href=\"#安装jupyter\" class=\"headerlink\" title=\"安装jupyter\"></a>安装jupyter</h1><blockquote>\n<p>pip install ipython<br>pip install jupyter</p>\n</blockquote>\n<h1 id=\"生成jupyter配置文件\"><a href=\"#生成jupyter配置文件\" class=\"headerlink\" title=\"生成jupyter配置文件\"></a>生成jupyter配置文件</h1><blockquote>\n<p>jupyter notebook –generate-config</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ jupyter notebook --generate-config</span><br><span class=\"line\">Writing default config to: /home/xm/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>\n<h1 id=\"自动生成密码\"><a href=\"#自动生成密码\" class=\"headerlink\" title=\"自动生成密码\"></a>自动生成密码</h1><blockquote>\n<p>jupyter notebook password</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ jupyter notebook password</span><br><span class=\"line\">Enter password: # 这里输入密码不会显示字符的</span><br><span class=\"line\">Verify password: </span><br><span class=\"line\">[NotebookPasswordApp] Wrote hashed password to /home/xm/.jupyter/jupyter_notebook_config.json</span><br><span class=\"line\"># 密码已经被加密记录到这个文件中了</span><br></pre></td></tr></table></figure>\n<h1 id=\"获取密码\"><a href=\"#获取密码\" class=\"headerlink\" title=\"获取密码\"></a>获取密码</h1><blockquote>\n<p>cat /home/xm/.jupyter/jupyter_notebook_config.json</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ cat /home/xm/.jupyter/jupyter_notebook_config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;NotebookApp&quot;: &#123;</span><br><span class=\"line\">    &quot;password&quot;: &quot;这是你的密码，一整段都复制\b下来&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改配置文件\"><a href=\"#修改配置文件\" class=\"headerlink\" title=\"修改配置文件\"></a>修改配置文件</h1><blockquote>\n<p>vim /home/xm/.jupyter/jupyter_notebook_config.py</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#懒得找对应配置项的朋友，直接把这四项配置写到文件开头就可以了</span><br><span class=\"line\">c.NotebookApp.ip = &apos;*&apos;</span><br><span class=\"line\">c.NotebookApp.password = &apos;sha:ce...刚才复制的那个密文&apos;</span><br><span class=\"line\">c.NotebookApp.open_browser = False</span><br><span class=\"line\">c.NotebookApp.port = 8888 #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"安装jupyter\"><a href=\"#安装jupyter\" class=\"headerlink\" title=\"安装jupyter\"></a>安装jupyter</h1><blockquote>\n<p>pip install ipython<br>pip install jupyter</p>\n</blockquote>\n<h1 id=\"生成jupyter配置文件\"><a href=\"#生成jupyter配置文件\" class=\"headerlink\" title=\"生成jupyter配置文件\"></a>生成jupyter配置文件</h1><blockquote>\n<p>jupyter notebook –generate-config</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ jupyter notebook --generate-config</span><br><span class=\"line\">Writing default config to: /home/xm/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>\n<h1 id=\"自动生成密码\"><a href=\"#自动生成密码\" class=\"headerlink\" title=\"自动生成密码\"></a>自动生成密码</h1><blockquote>\n<p>jupyter notebook password</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ jupyter notebook password</span><br><span class=\"line\">Enter password: # 这里输入密码不会显示字符的</span><br><span class=\"line\">Verify password: </span><br><span class=\"line\">[NotebookPasswordApp] Wrote hashed password to /home/xm/.jupyter/jupyter_notebook_config.json</span><br><span class=\"line\"># 密码已经被加密记录到这个文件中了</span><br></pre></td></tr></table></figure>\n<h1 id=\"获取密码\"><a href=\"#获取密码\" class=\"headerlink\" title=\"获取密码\"></a>获取密码</h1><blockquote>\n<p>cat /home/xm/.jupyter/jupyter_notebook_config.json</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xm@r2d2:~$ cat /home/xm/.jupyter/jupyter_notebook_config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;NotebookApp&quot;: &#123;</span><br><span class=\"line\">    &quot;password&quot;: &quot;这是你的密码，一整段都复制\b下来&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改配置文件\"><a href=\"#修改配置文件\" class=\"headerlink\" title=\"修改配置文件\"></a>修改配置文件</h1><blockquote>\n<p>vim /home/xm/.jupyter/jupyter_notebook_config.py</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#懒得找对应配置项的朋友，直接把这四项配置写到文件开头就可以了</span><br><span class=\"line\">c.NotebookApp.ip = &apos;*&apos;</span><br><span class=\"line\">c.NotebookApp.password = &apos;sha:ce...刚才复制的那个密文&apos;</span><br><span class=\"line\">c.NotebookApp.open_browser = False</span><br><span class=\"line\">c.NotebookApp.port = 8888 #可自行指定一个端口, 访问时使用该端口</span><br></pre></td></tr></table></figure>"},{"title":"linux如何查看磁盘可用空间","date":"2018-09-19T03:00:49.000Z","_content":"\n# 命令\n\n> df -h\n\n# 示例\n\n```\nUSER_MANE@PC_NAME:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev             16G     0   16G   0% /dev\ntmpfs           3.2G   26M  3.2G   1% /run\n/dev/sda1       198G  151G   38G  81% /\ntmpfs            16G  4.0K   16G   1% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs            16G     0   16G   0% /sys/fs/cgroup\n/dev/sdb1       917G  290G  581G  34% /SATA\ntmpfs           3.2G  8.0K  3.2G   1% /run/user/1004\ntmpfs           3.2G     0  3.2G   0% /run/user/1010\ntmpfs           3.2G     0  3.2G   0% /run/user/1003\n```","source":"_posts/linux如何查看磁盘可用空间.md","raw":"---\ntitle: linux如何查看磁盘可用空间\ndate: 2018-09-19 11:00:49\ntags: [linux, 磁盘, 空间]\n---\n\n# 命令\n\n> df -h\n\n# 示例\n\n```\nUSER_MANE@PC_NAME:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev             16G     0   16G   0% /dev\ntmpfs           3.2G   26M  3.2G   1% /run\n/dev/sda1       198G  151G   38G  81% /\ntmpfs            16G  4.0K   16G   1% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs            16G     0   16G   0% /sys/fs/cgroup\n/dev/sdb1       917G  290G  581G  34% /SATA\ntmpfs           3.2G  8.0K  3.2G   1% /run/user/1004\ntmpfs           3.2G     0  3.2G   0% /run/user/1010\ntmpfs           3.2G     0  3.2G   0% /run/user/1003\n```","slug":"linux如何查看磁盘可用空间","published":1,"updated":"2018-09-19T03:05:04.344Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiqh000it7x0wh1o86yw","content":"<h1 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h1><blockquote>\n<p>df -h</p>\n</blockquote>\n<h1 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">USER_MANE@PC_NAME:~$ df -h</span><br><span class=\"line\">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class=\"line\">udev             16G     0   16G   0% /dev</span><br><span class=\"line\">tmpfs           3.2G   26M  3.2G   1% /run</span><br><span class=\"line\">/dev/sda1       198G  151G   38G  81% /</span><br><span class=\"line\">tmpfs            16G  4.0K   16G   1% /dev/shm</span><br><span class=\"line\">tmpfs           5.0M  4.0K  5.0M   1% /run/lock</span><br><span class=\"line\">tmpfs            16G     0   16G   0% /sys/fs/cgroup</span><br><span class=\"line\">/dev/sdb1       917G  290G  581G  34% /SATA</span><br><span class=\"line\">tmpfs           3.2G  8.0K  3.2G   1% /run/user/1004</span><br><span class=\"line\">tmpfs           3.2G     0  3.2G   0% /run/user/1010</span><br><span class=\"line\">tmpfs           3.2G     0  3.2G   0% /run/user/1003</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h1><blockquote>\n<p>df -h</p>\n</blockquote>\n<h1 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">USER_MANE@PC_NAME:~$ df -h</span><br><span class=\"line\">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class=\"line\">udev             16G     0   16G   0% /dev</span><br><span class=\"line\">tmpfs           3.2G   26M  3.2G   1% /run</span><br><span class=\"line\">/dev/sda1       198G  151G   38G  81% /</span><br><span class=\"line\">tmpfs            16G  4.0K   16G   1% /dev/shm</span><br><span class=\"line\">tmpfs           5.0M  4.0K  5.0M   1% /run/lock</span><br><span class=\"line\">tmpfs            16G     0   16G   0% /sys/fs/cgroup</span><br><span class=\"line\">/dev/sdb1       917G  290G  581G  34% /SATA</span><br><span class=\"line\">tmpfs           3.2G  8.0K  3.2G   1% /run/user/1004</span><br><span class=\"line\">tmpfs           3.2G     0  3.2G   0% /run/user/1010</span><br><span class=\"line\">tmpfs           3.2G     0  3.2G   0% /run/user/1003</span><br></pre></td></tr></table></figure>"},{"title":"mysql-python安装错误：EnvironmentError:mysql_config not found","date":"2017-04-23T18:45:19.000Z","comments":1,"_content":"\n## 问题描述：\n\n安装mysql-python时报错：\n\n```\nCollecting mysql-python\n  Using cached MySQL-python-1.2.5.zip\n    Complete output from command python setup.py egg_info:\n    sh: 1: mysql_config: not found\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/tmp/pip-build-_itbcX/mysql-python/setup.py\", line 17, in <module>\n        metadata, options = get_config()\n      File \"setup_posix.py\", line 43, in get_config\n        libs = mysql_config(\"libs_r\")\n      File \"setup_posix.py\", line 25, in mysql_config\n        raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n    EnvironmentError: mysql_config not found\n\n    ----------------------------------------\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-_itbcX/mysql-python/\n```\n\n## 问题原因：\n\n没有安装libmysqlclient-dev。\n\n## 解决方案：\n\n执行：\n\n`sudo apt-get install libmysqlclient-dev`\n\n安装成功后，再运行`pip install mysql-python`即可。","source":"_posts/mysql-python安装错误：EnvironmentError- mysql_config not found.md","raw":"---\ntitle: mysql-python安装错误：EnvironmentError:mysql_config not found\ndate: 2017-04-24 02:45:19\ntags: [mysql, python, mysql-python安装错误, trick]\ncomments: true\n---\n\n## 问题描述：\n\n安装mysql-python时报错：\n\n```\nCollecting mysql-python\n  Using cached MySQL-python-1.2.5.zip\n    Complete output from command python setup.py egg_info:\n    sh: 1: mysql_config: not found\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/tmp/pip-build-_itbcX/mysql-python/setup.py\", line 17, in <module>\n        metadata, options = get_config()\n      File \"setup_posix.py\", line 43, in get_config\n        libs = mysql_config(\"libs_r\")\n      File \"setup_posix.py\", line 25, in mysql_config\n        raise EnvironmentError(\"%s not found\" % (mysql_config.path,))\n    EnvironmentError: mysql_config not found\n\n    ----------------------------------------\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-_itbcX/mysql-python/\n```\n\n## 问题原因：\n\n没有安装libmysqlclient-dev。\n\n## 解决方案：\n\n执行：\n\n`sudo apt-get install libmysqlclient-dev`\n\n安装成功后，再运行`pip install mysql-python`即可。","slug":"mysql-python安装错误：EnvironmentError- mysql_config not found","published":1,"updated":"2018-07-31T05:56:06.519Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqi000kt7x0fkh3uxkk","content":"<h2 id=\"问题描述：\"><a href=\"#问题描述：\" class=\"headerlink\" title=\"问题描述：\"></a>问题描述：</h2><p>安装mysql-python时报错：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Collecting mysql-python</span><br><span class=\"line\">  Using cached MySQL-python-1.2.5.zip</span><br><span class=\"line\">    Complete output from command python setup.py egg_info:</span><br><span class=\"line\">    sh: 1: mysql_config: not found</span><br><span class=\"line\">    Traceback (most recent call last):</span><br><span class=\"line\">      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class=\"line\">      File &quot;/tmp/pip-build-_itbcX/mysql-python/setup.py&quot;, line 17, in &lt;module&gt;</span><br><span class=\"line\">        metadata, options = get_config()</span><br><span class=\"line\">      File &quot;setup_posix.py&quot;, line 43, in get_config</span><br><span class=\"line\">        libs = mysql_config(&quot;libs_r&quot;)</span><br><span class=\"line\">      File &quot;setup_posix.py&quot;, line 25, in mysql_config</span><br><span class=\"line\">        raise EnvironmentError(&quot;%s not found&quot; % (mysql_config.path,))</span><br><span class=\"line\">    EnvironmentError: mysql_config not found</span><br><span class=\"line\"></span><br><span class=\"line\">    ----------------------------------------</span><br><span class=\"line\">Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-_itbcX/mysql-python/</span><br></pre></td></tr></table></figure>\n<h2 id=\"问题原因：\"><a href=\"#问题原因：\" class=\"headerlink\" title=\"问题原因：\"></a>问题原因：</h2><p>没有安装libmysqlclient-dev。</p>\n<h2 id=\"解决方案：\"><a href=\"#解决方案：\" class=\"headerlink\" title=\"解决方案：\"></a>解决方案：</h2><p>执行：</p>\n<p><code>sudo apt-get install libmysqlclient-dev</code></p>\n<p>安装成功后，再运行<code>pip install mysql-python</code>即可。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题描述：\"><a href=\"#问题描述：\" class=\"headerlink\" title=\"问题描述：\"></a>问题描述：</h2><p>安装mysql-python时报错：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Collecting mysql-python</span><br><span class=\"line\">  Using cached MySQL-python-1.2.5.zip</span><br><span class=\"line\">    Complete output from command python setup.py egg_info:</span><br><span class=\"line\">    sh: 1: mysql_config: not found</span><br><span class=\"line\">    Traceback (most recent call last):</span><br><span class=\"line\">      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class=\"line\">      File &quot;/tmp/pip-build-_itbcX/mysql-python/setup.py&quot;, line 17, in &lt;module&gt;</span><br><span class=\"line\">        metadata, options = get_config()</span><br><span class=\"line\">      File &quot;setup_posix.py&quot;, line 43, in get_config</span><br><span class=\"line\">        libs = mysql_config(&quot;libs_r&quot;)</span><br><span class=\"line\">      File &quot;setup_posix.py&quot;, line 25, in mysql_config</span><br><span class=\"line\">        raise EnvironmentError(&quot;%s not found&quot; % (mysql_config.path,))</span><br><span class=\"line\">    EnvironmentError: mysql_config not found</span><br><span class=\"line\"></span><br><span class=\"line\">    ----------------------------------------</span><br><span class=\"line\">Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-_itbcX/mysql-python/</span><br></pre></td></tr></table></figure>\n<h2 id=\"问题原因：\"><a href=\"#问题原因：\" class=\"headerlink\" title=\"问题原因：\"></a>问题原因：</h2><p>没有安装libmysqlclient-dev。</p>\n<h2 id=\"解决方案：\"><a href=\"#解决方案：\" class=\"headerlink\" title=\"解决方案：\"></a>解决方案：</h2><p>执行：</p>\n<p><code>sudo apt-get install libmysqlclient-dev</code></p>\n<p>安装成功后，再运行<code>pip install mysql-python</code>即可。</p>\n"},{"title":"从0到100：zabbix及其支持环境的完整安装教程","date":"2016-08-07T18:45:19.000Z","comments":1,"_content":"\n版本信息：\nUbuntu15.10\nApache2.4.12\nphp5.6.11（zabbix3.0要求php版本至少5.4以上）\nMysql5.6.31\nzabbix3.0\n\n前言：本教程包括了ubuntu上LAMP(Linux+Apache+Mysql+Php)环境的搭建以及zabbix安装。因为我们最终是要通过外部计算机访问我们的服务器的，所以我希望你可以先运行一下“ifconfig -a”语句来查看以下自己的IP地址，以方便之后测试服务器。文中将以“IPAddr”来代替你的IP地址，阅读时请注意。\n这是博主虚拟机上的IP地址：\n<!-- more-->\n![虚拟机上的IP地址](/assets/images/zabbix_2.jpg)\n\n## 0.预安装\n后边会用到的软件，装一下即可：\n`sudo apt-get install vim -y`\n\n## 1.Apache安装\n在命令行运行下列语句下载apache：\n`sudo apt-get install apache2 -y`\n启动apache服务：\n`sudo /etc/init.d/apache2 start`\n看到下列语句说明启动成功：\n![[OK]Starting apache2 (via systemctl):apache2.service](/assets/images/zabbix_1.jpg)\n从其他PC上打开浏览器，输入*http://IPAddr*，打开页面，如果显示如下，则表示Apache安装成功。\n![Apache默认页面](/assets/images/zabbix_3.jpg)\n\n## 2.安装Mysql\n在命令行运行下列语句下载mysql：\n`sudo apt-get install mysql-server -y`\n安装的时候会弹出窗口让你设置root帐户的初始密码，根据个人喜好设置一个即可。\n同样的，安装完了我们也要启动一下mysql的服务：\n`sudo /etc/init.d/mysql start`\n看到下列语句说明启动成功：\n![[OK]Starting mysql (via systemctl):mysql.service](/assets/images/zabbix_5.jpg)\n\n## 3.安装php5\n在命令行输入下列语句下载php5：\n`sudo apt-get install php5 -y`\n接着安装phpmyadmin：\n`sudo apt-get install phpmyadmin -y`\n安装的过程中根据提示，选择apache2，dbconfig-common那里选择YES，再输入系统root的密码和数据库root的密码即可。版本不同，顺序可能不大一样，总之问什么答什么就对了。\n顺便改写以下/var/www目录的权限，方便以后编辑网站文件：\n`sudo chmod 777 /var/www`\n创建phpmyadmin的链接：\n`sudo ln -s /usr/share/phpmyadmin /var/www/html/`\n修改一下php5的配置，打开配置文件：\n`sudo vim /etc/php5/apache2/php.ini`\n加入红框中的语句：\n![extension=mysqli.d](/assets/images/zabbix_6.jpg)\n保存退出。\n现在在其他的PC上打开浏览器，输入*http://IPAddr/phpmyadmin*，显示以下页面表示配置成功：\n![phpmyadmin登录页面](/assets/images/zabbix_7.jpg)\n## 4.安装配置zabbix server\n###4.1 下载deb：\n```\ncd ~\nwget http://repo.zabbix.com/zabbix/3.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_3.0-1+trusty_all.deb\ndpkg -i zabbix-release_3.0-1+trusty_all.deb\napt-get update\n```\n###4.2 安装服务器端\n运行下列语句：\n`sudo apt-get install zabbix-server-mysql zabbix-frontend-php -y`\n安装完成之后试着启动一下zabbix服务,出现下列语句即为成功：\n![[OK]Starting zabbix_server (via systemctl):zabbix_server.service](/assets/images/zabbix_8.jpg)\n\n###4.3 配置zabbix_server.conf\n打开配置文件：\n`sudo vim /etc/zabbix/zabbix_server.conf`\n把对应项的值改为如下(没有的自己在对应位置加上即可)：\n\n - DBHost=localhost\n - DBName=zabbix\n - DBUser=zabbix\n - DBPassword=zabbix\n\n###4.4 配置mysql\n```\nmysql -u root -p\n(输入你的数据库root密码)\nmysql> create database zabbix character set utf8 collate utf8_bin;\nmysql> grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\nmysql> flush privileges;\nmysql> \\q\ncd /usr/share/doc/zabbix-server-mysql\nzcat create.sql.gz | mysql -u root -p zabbix\n（输入你的数据库root密码，点击回车后稍微等一会儿）\nsudo cp -r /usr/share/zabbix /var/www/html/zabbix\n/etc/init.d/zabbix-server restart\n```\n最后出现下列语句即为成功：\n![[OK]Starting zabbix_server (via systemctl):zabbix_server.service](/assets/images/zabbix_8.jpg)\n###4.5 配置php\n编辑php的配置文件：\n`sudo vim /etc/php5/apache2/php.ini`\n把对应项的值改为如下(没有的自己在对应位置加上即可)：\n\n - post_max_size = 16M\n - max_execution_time = 300\n - max_input_time = 300\n - date.timezone = \"Asia/Shanghai\"\n\n改完之后重启apache2：\n`/etc/init.d/apache2 restart`\n\n## 5.进入zabbix\n在另外一台PC上打开浏览器，在地址栏输入：\n*http://IPAddr/zabbix*\n显示以下页面：\n![zabbix欢迎页面](/assets/images/zabbix_9.jpg)\n点击右下角的Next step进入Check of pre-requisites页面：\n![Check of pre-requisites页面](/assets/images/zabbix_10.jpg)\n这个页面是检测服务器配置是否合格的页面，必须全部为OK才可以点击Next step进入Configure DB connection页面。\n![Configure DB connection页面](/assets/images/zabbix_11.jpg)\n其中password为zabbix（我们刚刚配置数据库时设置的）。\n接下来的Zabbix server details和Pre-installation summary两个页面无脑点Next step即可。\n显示如下页面我们就可以点击Finish了。\n![Congratulations](/assets/images/zabbix_12.jpg)\n点击Finish之后出现zabbix server的登录页面，这里Username为Admin，Password为zabbix，最后点击Sign in，大功告成~\n![登录页面](/assets/images/zabbix_13.jpg)\n![zabbix管理页面](/assets/images/zabbix_14.jpg)","source":"_posts/从0到100：zabbix及其支持环境的完整安装教程.md","raw":"---\ntitle: 从0到100：zabbix及其支持环境的完整安装教程\ndate: 2016-08-08 02:45:19\ntags: [zabbix安装, lamp配置]\ncomments: true\n---\n\n版本信息：\nUbuntu15.10\nApache2.4.12\nphp5.6.11（zabbix3.0要求php版本至少5.4以上）\nMysql5.6.31\nzabbix3.0\n\n前言：本教程包括了ubuntu上LAMP(Linux+Apache+Mysql+Php)环境的搭建以及zabbix安装。因为我们最终是要通过外部计算机访问我们的服务器的，所以我希望你可以先运行一下“ifconfig -a”语句来查看以下自己的IP地址，以方便之后测试服务器。文中将以“IPAddr”来代替你的IP地址，阅读时请注意。\n这是博主虚拟机上的IP地址：\n<!-- more-->\n![虚拟机上的IP地址](/assets/images/zabbix_2.jpg)\n\n## 0.预安装\n后边会用到的软件，装一下即可：\n`sudo apt-get install vim -y`\n\n## 1.Apache安装\n在命令行运行下列语句下载apache：\n`sudo apt-get install apache2 -y`\n启动apache服务：\n`sudo /etc/init.d/apache2 start`\n看到下列语句说明启动成功：\n![[OK]Starting apache2 (via systemctl):apache2.service](/assets/images/zabbix_1.jpg)\n从其他PC上打开浏览器，输入*http://IPAddr*，打开页面，如果显示如下，则表示Apache安装成功。\n![Apache默认页面](/assets/images/zabbix_3.jpg)\n\n## 2.安装Mysql\n在命令行运行下列语句下载mysql：\n`sudo apt-get install mysql-server -y`\n安装的时候会弹出窗口让你设置root帐户的初始密码，根据个人喜好设置一个即可。\n同样的，安装完了我们也要启动一下mysql的服务：\n`sudo /etc/init.d/mysql start`\n看到下列语句说明启动成功：\n![[OK]Starting mysql (via systemctl):mysql.service](/assets/images/zabbix_5.jpg)\n\n## 3.安装php5\n在命令行输入下列语句下载php5：\n`sudo apt-get install php5 -y`\n接着安装phpmyadmin：\n`sudo apt-get install phpmyadmin -y`\n安装的过程中根据提示，选择apache2，dbconfig-common那里选择YES，再输入系统root的密码和数据库root的密码即可。版本不同，顺序可能不大一样，总之问什么答什么就对了。\n顺便改写以下/var/www目录的权限，方便以后编辑网站文件：\n`sudo chmod 777 /var/www`\n创建phpmyadmin的链接：\n`sudo ln -s /usr/share/phpmyadmin /var/www/html/`\n修改一下php5的配置，打开配置文件：\n`sudo vim /etc/php5/apache2/php.ini`\n加入红框中的语句：\n![extension=mysqli.d](/assets/images/zabbix_6.jpg)\n保存退出。\n现在在其他的PC上打开浏览器，输入*http://IPAddr/phpmyadmin*，显示以下页面表示配置成功：\n![phpmyadmin登录页面](/assets/images/zabbix_7.jpg)\n## 4.安装配置zabbix server\n###4.1 下载deb：\n```\ncd ~\nwget http://repo.zabbix.com/zabbix/3.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_3.0-1+trusty_all.deb\ndpkg -i zabbix-release_3.0-1+trusty_all.deb\napt-get update\n```\n###4.2 安装服务器端\n运行下列语句：\n`sudo apt-get install zabbix-server-mysql zabbix-frontend-php -y`\n安装完成之后试着启动一下zabbix服务,出现下列语句即为成功：\n![[OK]Starting zabbix_server (via systemctl):zabbix_server.service](/assets/images/zabbix_8.jpg)\n\n###4.3 配置zabbix_server.conf\n打开配置文件：\n`sudo vim /etc/zabbix/zabbix_server.conf`\n把对应项的值改为如下(没有的自己在对应位置加上即可)：\n\n - DBHost=localhost\n - DBName=zabbix\n - DBUser=zabbix\n - DBPassword=zabbix\n\n###4.4 配置mysql\n```\nmysql -u root -p\n(输入你的数据库root密码)\nmysql> create database zabbix character set utf8 collate utf8_bin;\nmysql> grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';\nmysql> flush privileges;\nmysql> \\q\ncd /usr/share/doc/zabbix-server-mysql\nzcat create.sql.gz | mysql -u root -p zabbix\n（输入你的数据库root密码，点击回车后稍微等一会儿）\nsudo cp -r /usr/share/zabbix /var/www/html/zabbix\n/etc/init.d/zabbix-server restart\n```\n最后出现下列语句即为成功：\n![[OK]Starting zabbix_server (via systemctl):zabbix_server.service](/assets/images/zabbix_8.jpg)\n###4.5 配置php\n编辑php的配置文件：\n`sudo vim /etc/php5/apache2/php.ini`\n把对应项的值改为如下(没有的自己在对应位置加上即可)：\n\n - post_max_size = 16M\n - max_execution_time = 300\n - max_input_time = 300\n - date.timezone = \"Asia/Shanghai\"\n\n改完之后重启apache2：\n`/etc/init.d/apache2 restart`\n\n## 5.进入zabbix\n在另外一台PC上打开浏览器，在地址栏输入：\n*http://IPAddr/zabbix*\n显示以下页面：\n![zabbix欢迎页面](/assets/images/zabbix_9.jpg)\n点击右下角的Next step进入Check of pre-requisites页面：\n![Check of pre-requisites页面](/assets/images/zabbix_10.jpg)\n这个页面是检测服务器配置是否合格的页面，必须全部为OK才可以点击Next step进入Configure DB connection页面。\n![Configure DB connection页面](/assets/images/zabbix_11.jpg)\n其中password为zabbix（我们刚刚配置数据库时设置的）。\n接下来的Zabbix server details和Pre-installation summary两个页面无脑点Next step即可。\n显示如下页面我们就可以点击Finish了。\n![Congratulations](/assets/images/zabbix_12.jpg)\n点击Finish之后出现zabbix server的登录页面，这里Username为Admin，Password为zabbix，最后点击Sign in，大功告成~\n![登录页面](/assets/images/zabbix_13.jpg)\n![zabbix管理页面](/assets/images/zabbix_14.jpg)","slug":"从0到100：zabbix及其支持环境的完整安装教程","published":1,"updated":"2016-12-10T15:58:40.000Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqj000nt7x0ue31dmar","content":"<p>版本信息：<br>Ubuntu15.10<br>Apache2.4.12<br>php5.6.11（zabbix3.0要求php版本至少5.4以上）<br>Mysql5.6.31<br>zabbix3.0</p>\n<p>前言：本教程包括了ubuntu上LAMP(Linux+Apache+Mysql+Php)环境的搭建以及zabbix安装。因为我们最终是要通过外部计算机访问我们的服务器的，所以我希望你可以先运行一下“ifconfig -a”语句来查看以下自己的IP地址，以方便之后测试服务器。文中将以“IPAddr”来代替你的IP地址，阅读时请注意。<br>这是博主虚拟机上的IP地址：<br><a id=\"more\"></a><br><img src=\"/assets/images/zabbix_2.jpg\" alt=\"虚拟机上的IP地址\"></p>\n<h2 id=\"0-预安装\"><a href=\"#0-预安装\" class=\"headerlink\" title=\"0.预安装\"></a>0.预安装</h2><p>后边会用到的软件，装一下即可：<br><code>sudo apt-get install vim -y</code></p>\n<h2 id=\"1-Apache安装\"><a href=\"#1-Apache安装\" class=\"headerlink\" title=\"1.Apache安装\"></a>1.Apache安装</h2><p>在命令行运行下列语句下载apache：<br><code>sudo apt-get install apache2 -y</code><br>启动apache服务：<br><code>sudo /etc/init.d/apache2 start</code><br>看到下列语句说明启动成功：<br><img src=\"/assets/images/zabbix_1.jpg\" alt=\"[OK]Starting apache2 (via systemctl):apache2.service\"><br>从其他PC上打开浏览器，输入<em><a href=\"http://IPAddr\" target=\"_blank\" rel=\"noopener\">http://IPAddr</a></em>，打开页面，如果显示如下，则表示Apache安装成功。<br><img src=\"/assets/images/zabbix_3.jpg\" alt=\"Apache默认页面\"></p>\n<h2 id=\"2-安装Mysql\"><a href=\"#2-安装Mysql\" class=\"headerlink\" title=\"2.安装Mysql\"></a>2.安装Mysql</h2><p>在命令行运行下列语句下载mysql：<br><code>sudo apt-get install mysql-server -y</code><br>安装的时候会弹出窗口让你设置root帐户的初始密码，根据个人喜好设置一个即可。<br>同样的，安装完了我们也要启动一下mysql的服务：<br><code>sudo /etc/init.d/mysql start</code><br>看到下列语句说明启动成功：<br><img src=\"/assets/images/zabbix_5.jpg\" alt=\"[OK]Starting mysql (via systemctl):mysql.service\"></p>\n<h2 id=\"3-安装php5\"><a href=\"#3-安装php5\" class=\"headerlink\" title=\"3.安装php5\"></a>3.安装php5</h2><p>在命令行输入下列语句下载php5：<br><code>sudo apt-get install php5 -y</code><br>接着安装phpmyadmin：<br><code>sudo apt-get install phpmyadmin -y</code><br>安装的过程中根据提示，选择apache2，dbconfig-common那里选择YES，再输入系统root的密码和数据库root的密码即可。版本不同，顺序可能不大一样，总之问什么答什么就对了。<br>顺便改写以下/var/www目录的权限，方便以后编辑网站文件：<br><code>sudo chmod 777 /var/www</code><br>创建phpmyadmin的链接：<br><code>sudo ln -s /usr/share/phpmyadmin /var/www/html/</code><br>修改一下php5的配置，打开配置文件：<br><code>sudo vim /etc/php5/apache2/php.ini</code><br>加入红框中的语句：<br><img src=\"/assets/images/zabbix_6.jpg\" alt=\"extension=mysqli.d\"><br>保存退出。<br>现在在其他的PC上打开浏览器，输入<em><a href=\"http://IPAddr/phpmyadmin\" target=\"_blank\" rel=\"noopener\">http://IPAddr/phpmyadmin</a></em>，显示以下页面表示配置成功：<br><img src=\"/assets/images/zabbix_7.jpg\" alt=\"phpmyadmin登录页面\"></p>\n<h2 id=\"4-安装配置zabbix-server\"><a href=\"#4-安装配置zabbix-server\" class=\"headerlink\" title=\"4.安装配置zabbix server\"></a>4.安装配置zabbix server</h2><p>###4.1 下载deb：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">wget http://repo.zabbix.com/zabbix/3.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_3.0-1+trusty_all.deb</span><br><span class=\"line\">dpkg -i zabbix-release_3.0-1+trusty_all.deb</span><br><span class=\"line\">apt-get update</span><br></pre></td></tr></table></figure></p>\n<p>###4.2 安装服务器端<br>运行下列语句：<br><code>sudo apt-get install zabbix-server-mysql zabbix-frontend-php -y</code><br>安装完成之后试着启动一下zabbix服务,出现下列语句即为成功：<br><img src=\"/assets/images/zabbix_8.jpg\" alt=\"[OK]Starting zabbix_server (via systemctl):zabbix_server.service\"></p>\n<p>###4.3 配置zabbix_server.conf<br>打开配置文件：<br><code>sudo vim /etc/zabbix/zabbix_server.conf</code><br>把对应项的值改为如下(没有的自己在对应位置加上即可)：</p>\n<ul>\n<li>DBHost=localhost</li>\n<li>DBName=zabbix</li>\n<li>DBUser=zabbix</li>\n<li>DBPassword=zabbix</li>\n</ul>\n<p>###4.4 配置mysql<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -p</span><br><span class=\"line\">(输入你的数据库root密码)</span><br><span class=\"line\">mysql&gt; create database zabbix character set utf8 collate utf8_bin;</span><br><span class=\"line\">mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;</span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">mysql&gt; \\q</span><br><span class=\"line\">cd /usr/share/doc/zabbix-server-mysql</span><br><span class=\"line\">zcat create.sql.gz | mysql -u root -p zabbix</span><br><span class=\"line\">（输入你的数据库root密码，点击回车后稍微等一会儿）</span><br><span class=\"line\">sudo cp -r /usr/share/zabbix /var/www/html/zabbix</span><br><span class=\"line\">/etc/init.d/zabbix-server restart</span><br></pre></td></tr></table></figure></p>\n<p>最后出现下列语句即为成功：<br><img src=\"/assets/images/zabbix_8.jpg\" alt=\"[OK]Starting zabbix_server (via systemctl):zabbix_server.service\"></p>\n<p>###4.5 配置php<br>编辑php的配置文件：<br><code>sudo vim /etc/php5/apache2/php.ini</code><br>把对应项的值改为如下(没有的自己在对应位置加上即可)：</p>\n<ul>\n<li>post_max_size = 16M</li>\n<li>max_execution_time = 300</li>\n<li>max_input_time = 300</li>\n<li>date.timezone = “Asia/Shanghai”</li>\n</ul>\n<p>改完之后重启apache2：<br><code>/etc/init.d/apache2 restart</code></p>\n<h2 id=\"5-进入zabbix\"><a href=\"#5-进入zabbix\" class=\"headerlink\" title=\"5.进入zabbix\"></a>5.进入zabbix</h2><p>在另外一台PC上打开浏览器，在地址栏输入：<br><em><a href=\"http://IPAddr/zabbix\" target=\"_blank\" rel=\"noopener\">http://IPAddr/zabbix</a></em><br>显示以下页面：<br><img src=\"/assets/images/zabbix_9.jpg\" alt=\"zabbix欢迎页面\"><br>点击右下角的Next step进入Check of pre-requisites页面：<br><img src=\"/assets/images/zabbix_10.jpg\" alt=\"Check of pre-requisites页面\"><br>这个页面是检测服务器配置是否合格的页面，必须全部为OK才可以点击Next step进入Configure DB connection页面。<br><img src=\"/assets/images/zabbix_11.jpg\" alt=\"Configure DB connection页面\"><br>其中password为zabbix（我们刚刚配置数据库时设置的）。<br>接下来的Zabbix server details和Pre-installation summary两个页面无脑点Next step即可。<br>显示如下页面我们就可以点击Finish了。<br><img src=\"/assets/images/zabbix_12.jpg\" alt=\"Congratulations\"><br>点击Finish之后出现zabbix server的登录页面，这里Username为Admin，Password为zabbix，最后点击Sign in，大功告成~<br><img src=\"/assets/images/zabbix_13.jpg\" alt=\"登录页面\"><br><img src=\"/assets/images/zabbix_14.jpg\" alt=\"zabbix管理页面\"></p>\n","site":{"data":{}},"excerpt":"<p>版本信息：<br>Ubuntu15.10<br>Apache2.4.12<br>php5.6.11（zabbix3.0要求php版本至少5.4以上）<br>Mysql5.6.31<br>zabbix3.0</p>\n<p>前言：本教程包括了ubuntu上LAMP(Linux+Apache+Mysql+Php)环境的搭建以及zabbix安装。因为我们最终是要通过外部计算机访问我们的服务器的，所以我希望你可以先运行一下“ifconfig -a”语句来查看以下自己的IP地址，以方便之后测试服务器。文中将以“IPAddr”来代替你的IP地址，阅读时请注意。<br>这是博主虚拟机上的IP地址：<br>","more":"<br><img src=\"/assets/images/zabbix_2.jpg\" alt=\"虚拟机上的IP地址\"></p>\n<h2 id=\"0-预安装\"><a href=\"#0-预安装\" class=\"headerlink\" title=\"0.预安装\"></a>0.预安装</h2><p>后边会用到的软件，装一下即可：<br><code>sudo apt-get install vim -y</code></p>\n<h2 id=\"1-Apache安装\"><a href=\"#1-Apache安装\" class=\"headerlink\" title=\"1.Apache安装\"></a>1.Apache安装</h2><p>在命令行运行下列语句下载apache：<br><code>sudo apt-get install apache2 -y</code><br>启动apache服务：<br><code>sudo /etc/init.d/apache2 start</code><br>看到下列语句说明启动成功：<br><img src=\"/assets/images/zabbix_1.jpg\" alt=\"[OK]Starting apache2 (via systemctl):apache2.service\"><br>从其他PC上打开浏览器，输入<em><a href=\"http://IPAddr\" target=\"_blank\" rel=\"noopener\">http://IPAddr</a></em>，打开页面，如果显示如下，则表示Apache安装成功。<br><img src=\"/assets/images/zabbix_3.jpg\" alt=\"Apache默认页面\"></p>\n<h2 id=\"2-安装Mysql\"><a href=\"#2-安装Mysql\" class=\"headerlink\" title=\"2.安装Mysql\"></a>2.安装Mysql</h2><p>在命令行运行下列语句下载mysql：<br><code>sudo apt-get install mysql-server -y</code><br>安装的时候会弹出窗口让你设置root帐户的初始密码，根据个人喜好设置一个即可。<br>同样的，安装完了我们也要启动一下mysql的服务：<br><code>sudo /etc/init.d/mysql start</code><br>看到下列语句说明启动成功：<br><img src=\"/assets/images/zabbix_5.jpg\" alt=\"[OK]Starting mysql (via systemctl):mysql.service\"></p>\n<h2 id=\"3-安装php5\"><a href=\"#3-安装php5\" class=\"headerlink\" title=\"3.安装php5\"></a>3.安装php5</h2><p>在命令行输入下列语句下载php5：<br><code>sudo apt-get install php5 -y</code><br>接着安装phpmyadmin：<br><code>sudo apt-get install phpmyadmin -y</code><br>安装的过程中根据提示，选择apache2，dbconfig-common那里选择YES，再输入系统root的密码和数据库root的密码即可。版本不同，顺序可能不大一样，总之问什么答什么就对了。<br>顺便改写以下/var/www目录的权限，方便以后编辑网站文件：<br><code>sudo chmod 777 /var/www</code><br>创建phpmyadmin的链接：<br><code>sudo ln -s /usr/share/phpmyadmin /var/www/html/</code><br>修改一下php5的配置，打开配置文件：<br><code>sudo vim /etc/php5/apache2/php.ini</code><br>加入红框中的语句：<br><img src=\"/assets/images/zabbix_6.jpg\" alt=\"extension=mysqli.d\"><br>保存退出。<br>现在在其他的PC上打开浏览器，输入<em><a href=\"http://IPAddr/phpmyadmin\" target=\"_blank\" rel=\"noopener\">http://IPAddr/phpmyadmin</a></em>，显示以下页面表示配置成功：<br><img src=\"/assets/images/zabbix_7.jpg\" alt=\"phpmyadmin登录页面\"></p>\n<h2 id=\"4-安装配置zabbix-server\"><a href=\"#4-安装配置zabbix-server\" class=\"headerlink\" title=\"4.安装配置zabbix server\"></a>4.安装配置zabbix server</h2><p>###4.1 下载deb：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">wget http://repo.zabbix.com/zabbix/3.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_3.0-1+trusty_all.deb</span><br><span class=\"line\">dpkg -i zabbix-release_3.0-1+trusty_all.deb</span><br><span class=\"line\">apt-get update</span><br></pre></td></tr></table></figure></p>\n<p>###4.2 安装服务器端<br>运行下列语句：<br><code>sudo apt-get install zabbix-server-mysql zabbix-frontend-php -y</code><br>安装完成之后试着启动一下zabbix服务,出现下列语句即为成功：<br><img src=\"/assets/images/zabbix_8.jpg\" alt=\"[OK]Starting zabbix_server (via systemctl):zabbix_server.service\"></p>\n<p>###4.3 配置zabbix_server.conf<br>打开配置文件：<br><code>sudo vim /etc/zabbix/zabbix_server.conf</code><br>把对应项的值改为如下(没有的自己在对应位置加上即可)：</p>\n<ul>\n<li>DBHost=localhost</li>\n<li>DBName=zabbix</li>\n<li>DBUser=zabbix</li>\n<li>DBPassword=zabbix</li>\n</ul>\n<p>###4.4 配置mysql<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -p</span><br><span class=\"line\">(输入你的数据库root密码)</span><br><span class=\"line\">mysql&gt; create database zabbix character set utf8 collate utf8_bin;</span><br><span class=\"line\">mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;</span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">mysql&gt; \\q</span><br><span class=\"line\">cd /usr/share/doc/zabbix-server-mysql</span><br><span class=\"line\">zcat create.sql.gz | mysql -u root -p zabbix</span><br><span class=\"line\">（输入你的数据库root密码，点击回车后稍微等一会儿）</span><br><span class=\"line\">sudo cp -r /usr/share/zabbix /var/www/html/zabbix</span><br><span class=\"line\">/etc/init.d/zabbix-server restart</span><br></pre></td></tr></table></figure></p>\n<p>最后出现下列语句即为成功：<br><img src=\"/assets/images/zabbix_8.jpg\" alt=\"[OK]Starting zabbix_server (via systemctl):zabbix_server.service\"></p>\n<p>###4.5 配置php<br>编辑php的配置文件：<br><code>sudo vim /etc/php5/apache2/php.ini</code><br>把对应项的值改为如下(没有的自己在对应位置加上即可)：</p>\n<ul>\n<li>post_max_size = 16M</li>\n<li>max_execution_time = 300</li>\n<li>max_input_time = 300</li>\n<li>date.timezone = “Asia/Shanghai”</li>\n</ul>\n<p>改完之后重启apache2：<br><code>/etc/init.d/apache2 restart</code></p>\n<h2 id=\"5-进入zabbix\"><a href=\"#5-进入zabbix\" class=\"headerlink\" title=\"5.进入zabbix\"></a>5.进入zabbix</h2><p>在另外一台PC上打开浏览器，在地址栏输入：<br><em><a href=\"http://IPAddr/zabbix\" target=\"_blank\" rel=\"noopener\">http://IPAddr/zabbix</a></em><br>显示以下页面：<br><img src=\"/assets/images/zabbix_9.jpg\" alt=\"zabbix欢迎页面\"><br>点击右下角的Next step进入Check of pre-requisites页面：<br><img src=\"/assets/images/zabbix_10.jpg\" alt=\"Check of pre-requisites页面\"><br>这个页面是检测服务器配置是否合格的页面，必须全部为OK才可以点击Next step进入Configure DB connection页面。<br><img src=\"/assets/images/zabbix_11.jpg\" alt=\"Configure DB connection页面\"><br>其中password为zabbix（我们刚刚配置数据库时设置的）。<br>接下来的Zabbix server details和Pre-installation summary两个页面无脑点Next step即可。<br>显示如下页面我们就可以点击Finish了。<br><img src=\"/assets/images/zabbix_12.jpg\" alt=\"Congratulations\"><br>点击Finish之后出现zabbix server的登录页面，这里Username为Admin，Password为zabbix，最后点击Sign in，大功告成~<br><img src=\"/assets/images/zabbix_13.jpg\" alt=\"登录页面\"><br><img src=\"/assets/images/zabbix_14.jpg\" alt=\"zabbix管理页面\"></p>"},{"title":"[Python]获取某文件夹下所有文件名","date":"2018-08-06T07:35:36.000Z","_content":"\n# 导入模块\n\n> import os\n\n# 读取目录下文件\n\n> os.listdir() #读取当前工作目录下文件名，返回列表\n> os.listdir('/username/folder_name/') #读取路径下所有文件名，返回列表\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180806154423.png)","source":"_posts/python-获取某文件夹下所有文件名.md","raw":"---\ntitle: '[Python]获取某文件夹下所有文件名'\ndate: 2018-08-06 15:35:36\ntags: [python, os, python入门]\n---\n\n# 导入模块\n\n> import os\n\n# 读取目录下文件\n\n> os.listdir() #读取当前工作目录下文件名，返回列表\n> os.listdir('/username/folder_name/') #读取路径下所有文件名，返回列表\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180806154423.png)","slug":"python-获取某文件夹下所有文件名","published":1,"updated":"2018-08-08T01:32:10.968Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjmlcpiqk000pt7x0s8n7bd0k","content":"<h1 id=\"导入模块\"><a href=\"#导入模块\" class=\"headerlink\" title=\"导入模块\"></a>导入模块</h1><blockquote>\n<p>import os</p>\n</blockquote>\n<h1 id=\"读取目录下文件\"><a href=\"#读取目录下文件\" class=\"headerlink\" title=\"读取目录下文件\"></a>读取目录下文件</h1><blockquote>\n<p>os.listdir() #读取当前工作目录下文件名，返回列表<br>os.listdir(‘/username/folder_name/‘) #读取路径下所有文件名，返回列表</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180806154423.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"导入模块\"><a href=\"#导入模块\" class=\"headerlink\" title=\"导入模块\"></a>导入模块</h1><blockquote>\n<p>import os</p>\n</blockquote>\n<h1 id=\"读取目录下文件\"><a href=\"#读取目录下文件\" class=\"headerlink\" title=\"读取目录下文件\"></a>读取目录下文件</h1><blockquote>\n<p>os.listdir() #读取当前工作目录下文件名，返回列表<br>os.listdir(‘/username/folder_name/‘) #读取路径下所有文件名，返回列表</p>\n</blockquote>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180806154423.png\" alt=\"\"></p>\n"},{"title":"[Trick]在linux上创建root权限用户（不修改系统文件）","date":"2018-07-31T02:26:05.000Z","comments":1,"_content":"\n# 简略过程\n\n如我们现在要创建一个名为imonce的新用户：\n\n> adduser imonce\n\n赋予root权限\n\n> usermod -g sudo imonce\n\n万事大吉\n\n# 详细过程\n\n有空再补吧~(‾⌣‾~)","source":"_posts/在linux上创建root权限用户（不修改系统文件）.md","raw":"---\ntitle: '[Trick]在linux上创建root权限用户（不修改系统文件）'\ndate: 2018-07-31 10:26:05\ntags: [linux, trick]\ncomments: true\n---\n\n# 简略过程\n\n如我们现在要创建一个名为imonce的新用户：\n\n> adduser imonce\n\n赋予root权限\n\n> usermod -g sudo imonce\n\n万事大吉\n\n# 详细过程\n\n有空再补吧~(‾⌣‾~)","slug":"在linux上创建root权限用户（不修改系统文件）","published":1,"updated":"2018-08-01T06:59:39.882Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqm000rt7x0uw7vfwuk","content":"<h1 id=\"简略过程\"><a href=\"#简略过程\" class=\"headerlink\" title=\"简略过程\"></a>简略过程</h1><p>如我们现在要创建一个名为imonce的新用户：</p>\n<blockquote>\n<p>adduser imonce</p>\n</blockquote>\n<p>赋予root权限</p>\n<blockquote>\n<p>usermod -g sudo imonce</p>\n</blockquote>\n<p>万事大吉</p>\n<h1 id=\"详细过程\"><a href=\"#详细过程\" class=\"headerlink\" title=\"详细过程\"></a>详细过程</h1><p>有空再补吧~(‾⌣‾~)</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"简略过程\"><a href=\"#简略过程\" class=\"headerlink\" title=\"简略过程\"></a>简略过程</h1><p>如我们现在要创建一个名为imonce的新用户：</p>\n<blockquote>\n<p>adduser imonce</p>\n</blockquote>\n<p>赋予root权限</p>\n<blockquote>\n<p>usermod -g sudo imonce</p>\n</blockquote>\n<p>万事大吉</p>\n<h1 id=\"详细过程\"><a href=\"#详细过程\" class=\"headerlink\" title=\"详细过程\"></a>详细过程</h1><p>有空再补吧~(‾⌣‾~)</p>\n"},{"title":"远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server","date":"2017-04-23T18:45:19.000Z","comments":1,"_content":"\n## 问题原因\n\nMySQL自带配置数据库mysql中的表user中，User=root一栏，Host的值为localhost，导致root用户只能通过本地登录。\n\n## 解决思路\n\n将User=root对应行的Host一栏的值修改为`%`，允许任意ip登录root。\n\n## 具体解决方案\n\n在本机登入mysql后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改称'%'即可 \n\n```\nmysql -u root -p  \nmysql>use mysql;  \nmysql>update user set host = '%' where user ='root';  \nmysql>flush privileges;\n```","source":"_posts/远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server.md","raw":"---\ntitle: 远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server\ndate: 2017-04-24 02:45:19\ntags: [mysql, 远程连接报错, trick]\ncomments: true\n---\n\n## 问题原因\n\nMySQL自带配置数据库mysql中的表user中，User=root一栏，Host的值为localhost，导致root用户只能通过本地登录。\n\n## 解决思路\n\n将User=root对应行的Host一栏的值修改为`%`，允许任意ip登录root。\n\n## 具体解决方案\n\n在本机登入mysql后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改称'%'即可 \n\n```\nmysql -u root -p  \nmysql>use mysql;  \nmysql>update user set host = '%' where user ='root';  \nmysql>flush privileges;\n```","slug":"远程连接mysql报错：1130 - Host '192.168.2.204' is not allowed to connect to this MySQL server","published":1,"updated":"2018-07-31T05:56:39.974Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqn000st7x0849qy1j5","content":"<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>MySQL自带配置数据库mysql中的表user中，User=root一栏，Host的值为localhost，导致root用户只能通过本地登录。</p>\n<h2 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h2><p>将User=root对应行的Host一栏的值修改为<code>%</code>，允许任意ip登录root。</p>\n<h2 id=\"具体解决方案\"><a href=\"#具体解决方案\" class=\"headerlink\" title=\"具体解决方案\"></a>具体解决方案</h2><p>在本机登入mysql后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改称’%’即可 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -p  </span><br><span class=\"line\">mysql&gt;use mysql;  </span><br><span class=\"line\">mysql&gt;update user set host = &apos;%&apos; where user =&apos;root&apos;;  </span><br><span class=\"line\">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>MySQL自带配置数据库mysql中的表user中，User=root一栏，Host的值为localhost，导致root用户只能通过本地登录。</p>\n<h2 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h2><p>将User=root对应行的Host一栏的值修改为<code>%</code>，允许任意ip登录root。</p>\n<h2 id=\"具体解决方案\"><a href=\"#具体解决方案\" class=\"headerlink\" title=\"具体解决方案\"></a>具体解决方案</h2><p>在本机登入mysql后，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，从”localhost”改称’%’即可 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql -u root -p  </span><br><span class=\"line\">mysql&gt;use mysql;  </span><br><span class=\"line\">mysql&gt;update user set host = &apos;%&apos; where user =&apos;root&apos;;  </span><br><span class=\"line\">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure>"},{"title":"远程连接mysql报错：error 2003 （hy000）:can't connect to mysql server on 'localhost' (10061)","date":"2017-04-23T18:45:19.000Z","comments":1,"_content":"\n## 问题原因\n\nmysql配置文件中有一句：\n\n`bind-address = 127.0.0.1`\n\n导致mysql只能从本地进行连接。\n\n## 解决思路\n\n找到mysql的配置文件，将这一行注释掉。\n\n## 具体解决方案\n\n去两个配置文件中找这个配置项：\n\n1. /etc/mysql/my.cnf\n2. /etc/mysql/mysqld.cnf\n\n在这两个文件的任意一个中找到\n\n`bind-address = 127.0.0.1`\n\n后，将其修改成：\n\n`#bind-address = 127.0.0.1`\n\n然后执行 `service mysql restart`重新启动mysql服务使配置生效即可。\n\n\n","source":"_posts/远程连接mysql报错：error 2003 （hy000）-can't connect to mysql server on 'localhost' (10061).md","raw":"---\ntitle: 远程连接mysql报错：error 2003 （hy000）:can't connect to mysql server on 'localhost' (10061)\ndate: 2017-04-24 02:45:19\ntags: [mysql, 远程连接报错, trick]\ncomments: true\n---\n\n## 问题原因\n\nmysql配置文件中有一句：\n\n`bind-address = 127.0.0.1`\n\n导致mysql只能从本地进行连接。\n\n## 解决思路\n\n找到mysql的配置文件，将这一行注释掉。\n\n## 具体解决方案\n\n去两个配置文件中找这个配置项：\n\n1. /etc/mysql/my.cnf\n2. /etc/mysql/mysqld.cnf\n\n在这两个文件的任意一个中找到\n\n`bind-address = 127.0.0.1`\n\n后，将其修改成：\n\n`#bind-address = 127.0.0.1`\n\n然后执行 `service mysql restart`重新启动mysql服务使配置生效即可。\n\n\n","slug":"远程连接mysql报错：error 2003 （hy000）-can't connect to mysql server on 'localhost' (10061)","published":1,"updated":"2018-07-31T05:56:36.646Z","layout":"post","photos":[],"link":"","_id":"cjmlcpiqp000ut7x0ujn2tsky","content":"<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>mysql配置文件中有一句：</p>\n<p><code>bind-address = 127.0.0.1</code></p>\n<p>导致mysql只能从本地进行连接。</p>\n<h2 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h2><p>找到mysql的配置文件，将这一行注释掉。</p>\n<h2 id=\"具体解决方案\"><a href=\"#具体解决方案\" class=\"headerlink\" title=\"具体解决方案\"></a>具体解决方案</h2><p>去两个配置文件中找这个配置项：</p>\n<ol>\n<li>/etc/mysql/my.cnf</li>\n<li>/etc/mysql/mysqld.cnf</li>\n</ol>\n<p>在这两个文件的任意一个中找到</p>\n<p><code>bind-address = 127.0.0.1</code></p>\n<p>后，将其修改成：</p>\n<p><code>#bind-address = 127.0.0.1</code></p>\n<p>然后执行 <code>service mysql restart</code>重新启动mysql服务使配置生效即可。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>mysql配置文件中有一句：</p>\n<p><code>bind-address = 127.0.0.1</code></p>\n<p>导致mysql只能从本地进行连接。</p>\n<h2 id=\"解决思路\"><a href=\"#解决思路\" class=\"headerlink\" title=\"解决思路\"></a>解决思路</h2><p>找到mysql的配置文件，将这一行注释掉。</p>\n<h2 id=\"具体解决方案\"><a href=\"#具体解决方案\" class=\"headerlink\" title=\"具体解决方案\"></a>具体解决方案</h2><p>去两个配置文件中找这个配置项：</p>\n<ol>\n<li>/etc/mysql/my.cnf</li>\n<li>/etc/mysql/mysqld.cnf</li>\n</ol>\n<p>在这两个文件的任意一个中找到</p>\n<p><code>bind-address = 127.0.0.1</code></p>\n<p>后，将其修改成：</p>\n<p><code>#bind-address = 127.0.0.1</code></p>\n<p>然后执行 <code>service mysql restart</code>重新启动mysql服务使配置生效即可。</p>\n"},{"title":"Chainer入门教程(下)：MNIST手写体识别","date":"2016-12-15T14:57:00.000Z","comments":1,"_content":"\n## III. 训练一个手写体识别器\n\n在这一部分中，我们将使用MNIST手写数字数据集来尝试区分一个28x28像素的手写体图像。这是一个典型的有监督的深度学习。\n\n对于这个问题，我们将会改变我们之前的线性回归器，同时引入一些隐藏的线性神经网络层，当然，也会引入一些非线性的激活函数。这种类型的架构通常被称为Multilayer Perceptron(MLP)。接下来我们就来看一下它是如何处理眼下的这个任务的。\n\n下面的这一段代码会帮助你下载、引入并结构化MNIST数据集。然而为了完成这部分工作，你还需要下载[data.py](https://github.com/imonce/imonce.github.io/tree/master/assets/files/data.py)文件，并把它放在你的工作目录(你的脚本或notebook所在的目录)下以方便导入。\n\n<!-- more-->\n\n```python\n# functions for importing the MNIST dataset.\nimport data\n\n# We'll first import the data as a variable mnist.\n# (If this is the first time you've run this function\n# it could take a minute or two)\n\nmnist = data.load_mnist_data()\n```\n\n现在我们可以先看一下这些图片的样子：\n\n```python\nplt.figure(figsize=(12,5))\nfor i in range(10):\n    example = mnist['data'][i].reshape(28, 28)\n    target = mnist['target'][i]\n    plt.subplot(2, 5, i+1)\n    plt.imshow(example, cmap='gray')\n    plt.title(\"Target Number: {0}\".format(target))\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809232611.png)\n\n现在，我们要把数据集中的图像和对应的真实数字分开，并分成训练集和测试集两部分，以便我们在最后检验我们的学习成果。\n\n```python\n# Separate the two parts of the MNIST dataset\nfeatures = mnist['data'].astype(np.float32) / 255\ntargets = mnist['target'].astype(np.int32)\n\n# Make a train/test split.\nx_train, x_test = np.split(features, [60000])\ny_train, y_test = np.split(targets, [60000])\n```\n\n这样一来，我们就可以集中精力训练我们的MLP了。MLP包含一系列不同的layer，Chainer又有一个很不错的方法，这可以帮我们把神经网络中所有的layer都封装到一个对象中。\n\n### FunctionSet简介\n\n这个方便的对象以命名后的layer作为关键字参数，以便我们之后可以引用它们。FunctionSet工作的方式如下：\n\n```\nmodel = FunctionSet(layer1=<place link here>, layer2=<place link here>, ...etc.)\n```\n\n然后layer就会在类的实例中作为属性存在。这些layer都可以通过把FunctionSet实例交给optimizer的setup方法同时进行优化：\n\n```\noptimizer.setup(model)\n```\n\n理解了这个小tip之后，我们就可以继续构建我们的分类器了。我们需要把一个28x28像素的图像降维成一个10维的单形。输出的每一个维度代表一个具体的数字。\n\n### MLP架构\n\n为了方便教学以及理解，我们在这里建立一个只有三层的神经网络。\n\n我们需要一个link来引入我们的28x28=784的图像，然后一步一步把它降维到10维。\n\n**另外，因为线性函数的组织是线性的，而深度学习又具有引入非线性变换的优点，所以当我们引入一些非线性函数时就会有非常好的重复线性层的堆叠。**\n\n<!-- Additionally, since compositions of linear functions are linear and the benefit of deep learning models are their ability to approximate arbitrary nonlinear functions, it wouldn’t do us much good to stack repeated linear layers together without adding some nonlinear function to send them through-->\n\n因此，在前向传播时，我们希望线性变换层和非线性的激活函数层交替出现。通过这种方法，我们的神经网络可以学习到非线性的数据模型以得到更好的预测结果。最后我们通过一个名为softmax的交叉熵损失函数来比较输出的矢量与我们的原本提取出的答案，然后基于计算出的损失来进行反向传播。\n\n最终我们的前向传播的架构应该是这样的形式：\n\n```\nout = linear_layer1(data)\nout = relu(out)\nout = linear_layer2(out)\nout = relu(out)\nout = linear_layer3(out)\n```\n\n到了训练我们的模型的时候，我们希望能够每次处理一部分的样品并在更新权重前来统计它们的损失。\n\n### Define the Model\n\n首先，我们通过声明link的集以及在训练过程中要用到的optimizer来定义模型。\n\n```python\n# Declare the model layers together as a FunctionSet\nmnist_model = FunctionSet(\n    linear1=L.Linear(784, 300),\n    linear2=L.Linear(300, 100),\n    linear3=L.Linear(100, 10)\n    )\n\n# Instantiate an optimizer (you should probably use an\n# Adam optimizer here for best performance)\n# and then setup the optimizer on the FunctionSet.\nmnist_optimizer = optimizers.Adam()\nmnist_optimizer.setup(mnist_model)\n```\n\n### 构造训练函数\n\n现在我们构造一个合适的函数来进行前向传播、定义训练用的数据集以及生成训练之后对MNIST手写图像进行预测的结果。\n\n```python\n# Construct a forward pass through the network,\n# moving sequentially through a layer then activation function\n# as stated above.\ndef mnist_forward(data, model):\n\n    out1 = model.linear1(data)\n    out2 = F.relu(out1)\n    out3 = model.linear2(out2)\n    out4 = F.relu(out3)\n    final = model.linear3(out4)\n    return final\n\n# Make a training function which takes in training data and targets\n# as an input.\ndef mnist_train(x, y, model, batchsize=1000, n_epochs=20):\n\n    data_size = x.shape[0]\n    # loop over epochs\n    for epoch in range(n_epochs):\n        print('epoch %d' % (epoch + 1))\n\n        # randomly shuffle the indices of the training data\n        shuffler = np.random.permutation(data_size)\n\n        # loop over batches\n        for i in range(0, data_size, batchsize):\n            x_var = Variable(x[shuffler[i : i + batchsize]])\n            y_var = Variable(y[shuffler[i : i + batchsize]])\n\n            output = mnist_forward(x_var, model)\n            model.zerograds()\n            loss = F.softmax_cross_entropy(output, y_var)\n            loss.backward()\n            mnist_optimizer.update()\n\n# Make a prediction function, using a softmax and argmax in order to\n# match the target space so that we can validate.\ndef mnist_predict(x, model):\n    x = Variable(x)\n\n    output = mnist_forward(x, model)\n\n    return F.softmax(output).data.argmax(1)\n```\n\n### Train the Model\n\n我们现在可以开始训练神经网络了（这里我们使用一个比较小的训练次数和一个比较大的批大小，这样可以帮我们节省一些训练时间。你也可以修改一些参数，说不定就会出现更好的结果呢~）\n\n```python\nmnist_train(x_train, y_train, mnist_model, n_epochs=5)\n```\n\n### 进行预测\n\n最后一件事情就是通过测试集来验证我们的模型的精确度，看看是否出现了过拟合的情况。\n\n```python\n# Call your prediction function on the test set\npred = mnist_predict(x_test, mnist_model)\n\n# Compare the prediction to the ground truth target values.\naccuracy = (pred==y_test).mean()\n\n# Print out test accuracy\nprint(\"Test accuracy: %f\" % accuracy)\n```\n\nout: `Test accuracy: 0.965900`\n\n可以看到，我们才训练了5次就有了一个96.59%的准确率，amazing~\n\n### 模型复用\n\n如果你觉得某一次的训练结果不错，想要保存下来以后使用，你可以通过Chainer的serializers来将其保存成hdf5格式：\n\n```python\nserializers.save_hdf5('test.model', mnist_model)\nserializers.save_hdf5('test.state', mnist_optimizer)\n```\n\n要调出使用的时候也很简单：\n\n```python\nserializers.load_hdf5('my_model.model', model_name)\nserializers.load_hdf5('my_optimizer.state', optimizer_name)\n```\n\n## Conclusion\n\n通过这篇入门教程，相信大家对于机器学习以及Chainer都有了一定的概念。可以看出，Chainer是一个非常灵活且实用的框架，机器学习也并非难以理解。如果你想进一步Chaier这个框架，个人觉得去看看[Chainer的官方文档](http://docs.chainer.org/en/stable/)也是一个不错的选择~\n\n### Note:\n本文译自：[Introduction to Chainer: Neural Networks in Python](http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/)\n\n### 每日一句：\nOn n'est jamais content là où on est.（人们从来不会满意自己所在的地方。）\n\n\n\n\n","source":"_posts/Chainer入门教程(下)-MNIST手写体识别.md","raw":"---\ntitle: \"Chainer入门教程(下)：MNIST手写体识别\"\ndate: 2016-12-15 22:57:00\ntags: [chainer, 入门教程, 机器学习, 神经网络]\ncomments: true\n---\n\n## III. 训练一个手写体识别器\n\n在这一部分中，我们将使用MNIST手写数字数据集来尝试区分一个28x28像素的手写体图像。这是一个典型的有监督的深度学习。\n\n对于这个问题，我们将会改变我们之前的线性回归器，同时引入一些隐藏的线性神经网络层，当然，也会引入一些非线性的激活函数。这种类型的架构通常被称为Multilayer Perceptron(MLP)。接下来我们就来看一下它是如何处理眼下的这个任务的。\n\n下面的这一段代码会帮助你下载、引入并结构化MNIST数据集。然而为了完成这部分工作，你还需要下载[data.py](https://github.com/imonce/imonce.github.io/tree/master/assets/files/data.py)文件，并把它放在你的工作目录(你的脚本或notebook所在的目录)下以方便导入。\n\n<!-- more-->\n\n```python\n# functions for importing the MNIST dataset.\nimport data\n\n# We'll first import the data as a variable mnist.\n# (If this is the first time you've run this function\n# it could take a minute or two)\n\nmnist = data.load_mnist_data()\n```\n\n现在我们可以先看一下这些图片的样子：\n\n```python\nplt.figure(figsize=(12,5))\nfor i in range(10):\n    example = mnist['data'][i].reshape(28, 28)\n    target = mnist['target'][i]\n    plt.subplot(2, 5, i+1)\n    plt.imshow(example, cmap='gray')\n    plt.title(\"Target Number: {0}\".format(target))\n    plt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n![](https://raw.githubusercontent.com/imonce/imgs/master/20180809232611.png)\n\n现在，我们要把数据集中的图像和对应的真实数字分开，并分成训练集和测试集两部分，以便我们在最后检验我们的学习成果。\n\n```python\n# Separate the two parts of the MNIST dataset\nfeatures = mnist['data'].astype(np.float32) / 255\ntargets = mnist['target'].astype(np.int32)\n\n# Make a train/test split.\nx_train, x_test = np.split(features, [60000])\ny_train, y_test = np.split(targets, [60000])\n```\n\n这样一来，我们就可以集中精力训练我们的MLP了。MLP包含一系列不同的layer，Chainer又有一个很不错的方法，这可以帮我们把神经网络中所有的layer都封装到一个对象中。\n\n### FunctionSet简介\n\n这个方便的对象以命名后的layer作为关键字参数，以便我们之后可以引用它们。FunctionSet工作的方式如下：\n\n```\nmodel = FunctionSet(layer1=<place link here>, layer2=<place link here>, ...etc.)\n```\n\n然后layer就会在类的实例中作为属性存在。这些layer都可以通过把FunctionSet实例交给optimizer的setup方法同时进行优化：\n\n```\noptimizer.setup(model)\n```\n\n理解了这个小tip之后，我们就可以继续构建我们的分类器了。我们需要把一个28x28像素的图像降维成一个10维的单形。输出的每一个维度代表一个具体的数字。\n\n### MLP架构\n\n为了方便教学以及理解，我们在这里建立一个只有三层的神经网络。\n\n我们需要一个link来引入我们的28x28=784的图像，然后一步一步把它降维到10维。\n\n**另外，因为线性函数的组织是线性的，而深度学习又具有引入非线性变换的优点，所以当我们引入一些非线性函数时就会有非常好的重复线性层的堆叠。**\n\n<!-- Additionally, since compositions of linear functions are linear and the benefit of deep learning models are their ability to approximate arbitrary nonlinear functions, it wouldn’t do us much good to stack repeated linear layers together without adding some nonlinear function to send them through-->\n\n因此，在前向传播时，我们希望线性变换层和非线性的激活函数层交替出现。通过这种方法，我们的神经网络可以学习到非线性的数据模型以得到更好的预测结果。最后我们通过一个名为softmax的交叉熵损失函数来比较输出的矢量与我们的原本提取出的答案，然后基于计算出的损失来进行反向传播。\n\n最终我们的前向传播的架构应该是这样的形式：\n\n```\nout = linear_layer1(data)\nout = relu(out)\nout = linear_layer2(out)\nout = relu(out)\nout = linear_layer3(out)\n```\n\n到了训练我们的模型的时候，我们希望能够每次处理一部分的样品并在更新权重前来统计它们的损失。\n\n### Define the Model\n\n首先，我们通过声明link的集以及在训练过程中要用到的optimizer来定义模型。\n\n```python\n# Declare the model layers together as a FunctionSet\nmnist_model = FunctionSet(\n    linear1=L.Linear(784, 300),\n    linear2=L.Linear(300, 100),\n    linear3=L.Linear(100, 10)\n    )\n\n# Instantiate an optimizer (you should probably use an\n# Adam optimizer here for best performance)\n# and then setup the optimizer on the FunctionSet.\nmnist_optimizer = optimizers.Adam()\nmnist_optimizer.setup(mnist_model)\n```\n\n### 构造训练函数\n\n现在我们构造一个合适的函数来进行前向传播、定义训练用的数据集以及生成训练之后对MNIST手写图像进行预测的结果。\n\n```python\n# Construct a forward pass through the network,\n# moving sequentially through a layer then activation function\n# as stated above.\ndef mnist_forward(data, model):\n\n    out1 = model.linear1(data)\n    out2 = F.relu(out1)\n    out3 = model.linear2(out2)\n    out4 = F.relu(out3)\n    final = model.linear3(out4)\n    return final\n\n# Make a training function which takes in training data and targets\n# as an input.\ndef mnist_train(x, y, model, batchsize=1000, n_epochs=20):\n\n    data_size = x.shape[0]\n    # loop over epochs\n    for epoch in range(n_epochs):\n        print('epoch %d' % (epoch + 1))\n\n        # randomly shuffle the indices of the training data\n        shuffler = np.random.permutation(data_size)\n\n        # loop over batches\n        for i in range(0, data_size, batchsize):\n            x_var = Variable(x[shuffler[i : i + batchsize]])\n            y_var = Variable(y[shuffler[i : i + batchsize]])\n\n            output = mnist_forward(x_var, model)\n            model.zerograds()\n            loss = F.softmax_cross_entropy(output, y_var)\n            loss.backward()\n            mnist_optimizer.update()\n\n# Make a prediction function, using a softmax and argmax in order to\n# match the target space so that we can validate.\ndef mnist_predict(x, model):\n    x = Variable(x)\n\n    output = mnist_forward(x, model)\n\n    return F.softmax(output).data.argmax(1)\n```\n\n### Train the Model\n\n我们现在可以开始训练神经网络了（这里我们使用一个比较小的训练次数和一个比较大的批大小，这样可以帮我们节省一些训练时间。你也可以修改一些参数，说不定就会出现更好的结果呢~）\n\n```python\nmnist_train(x_train, y_train, mnist_model, n_epochs=5)\n```\n\n### 进行预测\n\n最后一件事情就是通过测试集来验证我们的模型的精确度，看看是否出现了过拟合的情况。\n\n```python\n# Call your prediction function on the test set\npred = mnist_predict(x_test, mnist_model)\n\n# Compare the prediction to the ground truth target values.\naccuracy = (pred==y_test).mean()\n\n# Print out test accuracy\nprint(\"Test accuracy: %f\" % accuracy)\n```\n\nout: `Test accuracy: 0.965900`\n\n可以看到，我们才训练了5次就有了一个96.59%的准确率，amazing~\n\n### 模型复用\n\n如果你觉得某一次的训练结果不错，想要保存下来以后使用，你可以通过Chainer的serializers来将其保存成hdf5格式：\n\n```python\nserializers.save_hdf5('test.model', mnist_model)\nserializers.save_hdf5('test.state', mnist_optimizer)\n```\n\n要调出使用的时候也很简单：\n\n```python\nserializers.load_hdf5('my_model.model', model_name)\nserializers.load_hdf5('my_optimizer.state', optimizer_name)\n```\n\n## Conclusion\n\n通过这篇入门教程，相信大家对于机器学习以及Chainer都有了一定的概念。可以看出，Chainer是一个非常灵活且实用的框架，机器学习也并非难以理解。如果你想进一步Chaier这个框架，个人觉得去看看[Chainer的官方文档](http://docs.chainer.org/en/stable/)也是一个不错的选择~\n\n### Note:\n本文译自：[Introduction to Chainer: Neural Networks in Python](http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/)\n\n### 每日一句：\nOn n'est jamais content là où on est.（人们从来不会满意自己所在的地方。）\n\n\n\n\n","slug":"Chainer入门教程(下)-MNIST手写体识别","published":1,"updated":"2018-08-09T15:32:08.977Z","layout":"post","photos":[],"link":"","_id":"cjmlcpirf003vt7x014d3grwa","content":"<h2 id=\"III-训练一个手写体识别器\"><a href=\"#III-训练一个手写体识别器\" class=\"headerlink\" title=\"III. 训练一个手写体识别器\"></a>III. 训练一个手写体识别器</h2><p>在这一部分中，我们将使用MNIST手写数字数据集来尝试区分一个28x28像素的手写体图像。这是一个典型的有监督的深度学习。</p>\n<p>对于这个问题，我们将会改变我们之前的线性回归器，同时引入一些隐藏的线性神经网络层，当然，也会引入一些非线性的激活函数。这种类型的架构通常被称为Multilayer Perceptron(MLP)。接下来我们就来看一下它是如何处理眼下的这个任务的。</p>\n<p>下面的这一段代码会帮助你下载、引入并结构化MNIST数据集。然而为了完成这部分工作，你还需要下载<a href=\"https://github.com/imonce/imonce.github.io/tree/master/assets/files/data.py\" target=\"_blank\" rel=\"noopener\">data.py</a>文件，并把它放在你的工作目录(你的脚本或notebook所在的目录)下以方便导入。</p>\n<a id=\"more\"></a>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># functions for importing the MNIST dataset.</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># We'll first import the data as a variable mnist.</span></span><br><span class=\"line\"><span class=\"comment\"># (If this is the first time you've run this function</span></span><br><span class=\"line\"><span class=\"comment\"># it could take a minute or two)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = data.load_mnist_data()</span><br></pre></td></tr></table></figure>\n<p>现在我们可以先看一下这些图片的样子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    example = mnist[<span class=\"string\">'data'</span>][i].reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">    target = mnist[<span class=\"string\">'target'</span>][i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">5</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.imshow(example, cmap=<span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Target Number: &#123;0&#125;\"</span>.format(target))</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809232611.png\" alt=\"\"></p>\n<p>现在，我们要把数据集中的图像和对应的真实数字分开，并分成训练集和测试集两部分，以便我们在最后检验我们的学习成果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Separate the two parts of the MNIST dataset</span></span><br><span class=\"line\">features = mnist[<span class=\"string\">'data'</span>].astype(np.float32) / <span class=\"number\">255</span></span><br><span class=\"line\">targets = mnist[<span class=\"string\">'target'</span>].astype(np.int32)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a train/test split.</span></span><br><span class=\"line\">x_train, x_test = np.split(features, [<span class=\"number\">60000</span>])</span><br><span class=\"line\">y_train, y_test = np.split(targets, [<span class=\"number\">60000</span>])</span><br></pre></td></tr></table></figure>\n<p>这样一来，我们就可以集中精力训练我们的MLP了。MLP包含一系列不同的layer，Chainer又有一个很不错的方法，这可以帮我们把神经网络中所有的layer都封装到一个对象中。</p>\n<h3 id=\"FunctionSet简介\"><a href=\"#FunctionSet简介\" class=\"headerlink\" title=\"FunctionSet简介\"></a>FunctionSet简介</h3><p>这个方便的对象以命名后的layer作为关键字参数，以便我们之后可以引用它们。FunctionSet工作的方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = FunctionSet(layer1=&lt;place link here&gt;, layer2=&lt;place link here&gt;, ...etc.)</span><br></pre></td></tr></table></figure>\n<p>然后layer就会在类的实例中作为属性存在。这些layer都可以通过把FunctionSet实例交给optimizer的setup方法同时进行优化：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer.setup(model)</span><br></pre></td></tr></table></figure>\n<p>理解了这个小tip之后，我们就可以继续构建我们的分类器了。我们需要把一个28x28像素的图像降维成一个10维的单形。输出的每一个维度代表一个具体的数字。</p>\n<h3 id=\"MLP架构\"><a href=\"#MLP架构\" class=\"headerlink\" title=\"MLP架构\"></a>MLP架构</h3><p>为了方便教学以及理解，我们在这里建立一个只有三层的神经网络。</p>\n<p>我们需要一个link来引入我们的28x28=784的图像，然后一步一步把它降维到10维。</p>\n<p><strong>另外，因为线性函数的组织是线性的，而深度学习又具有引入非线性变换的优点，所以当我们引入一些非线性函数时就会有非常好的重复线性层的堆叠。</strong></p>\n<!-- Additionally, since compositions of linear functions are linear and the benefit of deep learning models are their ability to approximate arbitrary nonlinear functions, it wouldn’t do us much good to stack repeated linear layers together without adding some nonlinear function to send them through-->\n<p>因此，在前向传播时，我们希望线性变换层和非线性的激活函数层交替出现。通过这种方法，我们的神经网络可以学习到非线性的数据模型以得到更好的预测结果。最后我们通过一个名为softmax的交叉熵损失函数来比较输出的矢量与我们的原本提取出的答案，然后基于计算出的损失来进行反向传播。</p>\n<p>最终我们的前向传播的架构应该是这样的形式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">out = linear_layer1(data)</span><br><span class=\"line\">out = relu(out)</span><br><span class=\"line\">out = linear_layer2(out)</span><br><span class=\"line\">out = relu(out)</span><br><span class=\"line\">out = linear_layer3(out)</span><br></pre></td></tr></table></figure>\n<p>到了训练我们的模型的时候，我们希望能够每次处理一部分的样品并在更新权重前来统计它们的损失。</p>\n<h3 id=\"Define-the-Model\"><a href=\"#Define-the-Model\" class=\"headerlink\" title=\"Define the Model\"></a>Define the Model</h3><p>首先，我们通过声明link的集以及在训练过程中要用到的optimizer来定义模型。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Declare the model layers together as a FunctionSet</span></span><br><span class=\"line\">mnist_model = FunctionSet(</span><br><span class=\"line\">    linear1=L.Linear(<span class=\"number\">784</span>, <span class=\"number\">300</span>),</span><br><span class=\"line\">    linear2=L.Linear(<span class=\"number\">300</span>, <span class=\"number\">100</span>),</span><br><span class=\"line\">    linear3=L.Linear(<span class=\"number\">100</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Instantiate an optimizer (you should probably use an</span></span><br><span class=\"line\"><span class=\"comment\"># Adam optimizer here for best performance)</span></span><br><span class=\"line\"><span class=\"comment\"># and then setup the optimizer on the FunctionSet.</span></span><br><span class=\"line\">mnist_optimizer = optimizers.Adam()</span><br><span class=\"line\">mnist_optimizer.setup(mnist_model)</span><br></pre></td></tr></table></figure>\n<h3 id=\"构造训练函数\"><a href=\"#构造训练函数\" class=\"headerlink\" title=\"构造训练函数\"></a>构造训练函数</h3><p>现在我们构造一个合适的函数来进行前向传播、定义训练用的数据集以及生成训练之后对MNIST手写图像进行预测的结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Construct a forward pass through the network,</span></span><br><span class=\"line\"><span class=\"comment\"># moving sequentially through a layer then activation function</span></span><br><span class=\"line\"><span class=\"comment\"># as stated above.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_forward</span><span class=\"params\">(data, model)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    out1 = model.linear1(data)</span><br><span class=\"line\">    out2 = F.relu(out1)</span><br><span class=\"line\">    out3 = model.linear2(out2)</span><br><span class=\"line\">    out4 = F.relu(out3)</span><br><span class=\"line\">    final = model.linear3(out4)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> final</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a training function which takes in training data and targets</span></span><br><span class=\"line\"><span class=\"comment\"># as an input.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_train</span><span class=\"params\">(x, y, model, batchsize=<span class=\"number\">1000</span>, n_epochs=<span class=\"number\">20</span>)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    data_size = x.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># loop over epochs</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(n_epochs):</span><br><span class=\"line\">        print(<span class=\"string\">'epoch %d'</span> % (epoch + <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># randomly shuffle the indices of the training data</span></span><br><span class=\"line\">        shuffler = np.random.permutation(data_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># loop over batches</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, data_size, batchsize):</span><br><span class=\"line\">            x_var = Variable(x[shuffler[i : i + batchsize]])</span><br><span class=\"line\">            y_var = Variable(y[shuffler[i : i + batchsize]])</span><br><span class=\"line\"></span><br><span class=\"line\">            output = mnist_forward(x_var, model)</span><br><span class=\"line\">            model.zerograds()</span><br><span class=\"line\">            loss = F.softmax_cross_entropy(output, y_var)</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            mnist_optimizer.update()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a prediction function, using a softmax and argmax in order to</span></span><br><span class=\"line\"><span class=\"comment\"># match the target space so that we can validate.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_predict</span><span class=\"params\">(x, model)</span>:</span></span><br><span class=\"line\">    x = Variable(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    output = mnist_forward(x, model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> F.softmax(output).data.argmax(<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Train-the-Model\"><a href=\"#Train-the-Model\" class=\"headerlink\" title=\"Train the Model\"></a>Train the Model</h3><p>我们现在可以开始训练神经网络了（这里我们使用一个比较小的训练次数和一个比较大的批大小，这样可以帮我们节省一些训练时间。你也可以修改一些参数，说不定就会出现更好的结果呢~）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mnist_train(x_train, y_train, mnist_model, n_epochs=<span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"进行预测\"><a href=\"#进行预测\" class=\"headerlink\" title=\"进行预测\"></a>进行预测</h3><p>最后一件事情就是通过测试集来验证我们的模型的精确度，看看是否出现了过拟合的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Call your prediction function on the test set</span></span><br><span class=\"line\">pred = mnist_predict(x_test, mnist_model)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Compare the prediction to the ground truth target values.</span></span><br><span class=\"line\">accuracy = (pred==y_test).mean()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Print out test accuracy</span></span><br><span class=\"line\">print(<span class=\"string\">\"Test accuracy: %f\"</span> % accuracy)</span><br></pre></td></tr></table></figure>\n<p>out: <code>Test accuracy: 0.965900</code></p>\n<p>可以看到，我们才训练了5次就有了一个96.59%的准确率，amazing~</p>\n<h3 id=\"模型复用\"><a href=\"#模型复用\" class=\"headerlink\" title=\"模型复用\"></a>模型复用</h3><p>如果你觉得某一次的训练结果不错，想要保存下来以后使用，你可以通过Chainer的serializers来将其保存成hdf5格式：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">serializers.save_hdf5(<span class=\"string\">'test.model'</span>, mnist_model)</span><br><span class=\"line\">serializers.save_hdf5(<span class=\"string\">'test.state'</span>, mnist_optimizer)</span><br></pre></td></tr></table></figure>\n<p>要调出使用的时候也很简单：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">serializers.load_hdf5(<span class=\"string\">'my_model.model'</span>, model_name)</span><br><span class=\"line\">serializers.load_hdf5(<span class=\"string\">'my_optimizer.state'</span>, optimizer_name)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>通过这篇入门教程，相信大家对于机器学习以及Chainer都有了一定的概念。可以看出，Chainer是一个非常灵活且实用的框架，机器学习也并非难以理解。如果你想进一步Chaier这个框架，个人觉得去看看<a href=\"http://docs.chainer.org/en/stable/\" target=\"_blank\" rel=\"noopener\">Chainer的官方文档</a>也是一个不错的选择~</p>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note:\"></a>Note:</h3><p>本文译自：<a href=\"http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/\" target=\"_blank\" rel=\"noopener\">Introduction to Chainer: Neural Networks in Python</a></p>\n<h3 id=\"每日一句：\"><a href=\"#每日一句：\" class=\"headerlink\" title=\"每日一句：\"></a>每日一句：</h3><p>On n’est jamais content là où on est.（人们从来不会满意自己所在的地方。）</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"III-训练一个手写体识别器\"><a href=\"#III-训练一个手写体识别器\" class=\"headerlink\" title=\"III. 训练一个手写体识别器\"></a>III. 训练一个手写体识别器</h2><p>在这一部分中，我们将使用MNIST手写数字数据集来尝试区分一个28x28像素的手写体图像。这是一个典型的有监督的深度学习。</p>\n<p>对于这个问题，我们将会改变我们之前的线性回归器，同时引入一些隐藏的线性神经网络层，当然，也会引入一些非线性的激活函数。这种类型的架构通常被称为Multilayer Perceptron(MLP)。接下来我们就来看一下它是如何处理眼下的这个任务的。</p>\n<p>下面的这一段代码会帮助你下载、引入并结构化MNIST数据集。然而为了完成这部分工作，你还需要下载<a href=\"https://github.com/imonce/imonce.github.io/tree/master/assets/files/data.py\" target=\"_blank\" rel=\"noopener\">data.py</a>文件，并把它放在你的工作目录(你的脚本或notebook所在的目录)下以方便导入。</p>","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># functions for importing the MNIST dataset.</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># We'll first import the data as a variable mnist.</span></span><br><span class=\"line\"><span class=\"comment\"># (If this is the first time you've run this function</span></span><br><span class=\"line\"><span class=\"comment\"># it could take a minute or two)</span></span><br><span class=\"line\"></span><br><span class=\"line\">mnist = data.load_mnist_data()</span><br></pre></td></tr></table></figure>\n<p>现在我们可以先看一下这些图片的样子：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">12</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    example = mnist[<span class=\"string\">'data'</span>][i].reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">    target = mnist[<span class=\"string\">'target'</span>][i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">5</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.imshow(example, cmap=<span class=\"string\">'gray'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Target Number: &#123;0&#125;\"</span>.format(target))</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/imonce/imgs/master/20180809232611.png\" alt=\"\"></p>\n<p>现在，我们要把数据集中的图像和对应的真实数字分开，并分成训练集和测试集两部分，以便我们在最后检验我们的学习成果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Separate the two parts of the MNIST dataset</span></span><br><span class=\"line\">features = mnist[<span class=\"string\">'data'</span>].astype(np.float32) / <span class=\"number\">255</span></span><br><span class=\"line\">targets = mnist[<span class=\"string\">'target'</span>].astype(np.int32)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a train/test split.</span></span><br><span class=\"line\">x_train, x_test = np.split(features, [<span class=\"number\">60000</span>])</span><br><span class=\"line\">y_train, y_test = np.split(targets, [<span class=\"number\">60000</span>])</span><br></pre></td></tr></table></figure>\n<p>这样一来，我们就可以集中精力训练我们的MLP了。MLP包含一系列不同的layer，Chainer又有一个很不错的方法，这可以帮我们把神经网络中所有的layer都封装到一个对象中。</p>\n<h3 id=\"FunctionSet简介\"><a href=\"#FunctionSet简介\" class=\"headerlink\" title=\"FunctionSet简介\"></a>FunctionSet简介</h3><p>这个方便的对象以命名后的layer作为关键字参数，以便我们之后可以引用它们。FunctionSet工作的方式如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = FunctionSet(layer1=&lt;place link here&gt;, layer2=&lt;place link here&gt;, ...etc.)</span><br></pre></td></tr></table></figure>\n<p>然后layer就会在类的实例中作为属性存在。这些layer都可以通过把FunctionSet实例交给optimizer的setup方法同时进行优化：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer.setup(model)</span><br></pre></td></tr></table></figure>\n<p>理解了这个小tip之后，我们就可以继续构建我们的分类器了。我们需要把一个28x28像素的图像降维成一个10维的单形。输出的每一个维度代表一个具体的数字。</p>\n<h3 id=\"MLP架构\"><a href=\"#MLP架构\" class=\"headerlink\" title=\"MLP架构\"></a>MLP架构</h3><p>为了方便教学以及理解，我们在这里建立一个只有三层的神经网络。</p>\n<p>我们需要一个link来引入我们的28x28=784的图像，然后一步一步把它降维到10维。</p>\n<p><strong>另外，因为线性函数的组织是线性的，而深度学习又具有引入非线性变换的优点，所以当我们引入一些非线性函数时就会有非常好的重复线性层的堆叠。</strong></p>\n<!-- Additionally, since compositions of linear functions are linear and the benefit of deep learning models are their ability to approximate arbitrary nonlinear functions, it wouldn’t do us much good to stack repeated linear layers together without adding some nonlinear function to send them through-->\n<p>因此，在前向传播时，我们希望线性变换层和非线性的激活函数层交替出现。通过这种方法，我们的神经网络可以学习到非线性的数据模型以得到更好的预测结果。最后我们通过一个名为softmax的交叉熵损失函数来比较输出的矢量与我们的原本提取出的答案，然后基于计算出的损失来进行反向传播。</p>\n<p>最终我们的前向传播的架构应该是这样的形式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">out = linear_layer1(data)</span><br><span class=\"line\">out = relu(out)</span><br><span class=\"line\">out = linear_layer2(out)</span><br><span class=\"line\">out = relu(out)</span><br><span class=\"line\">out = linear_layer3(out)</span><br></pre></td></tr></table></figure>\n<p>到了训练我们的模型的时候，我们希望能够每次处理一部分的样品并在更新权重前来统计它们的损失。</p>\n<h3 id=\"Define-the-Model\"><a href=\"#Define-the-Model\" class=\"headerlink\" title=\"Define the Model\"></a>Define the Model</h3><p>首先，我们通过声明link的集以及在训练过程中要用到的optimizer来定义模型。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Declare the model layers together as a FunctionSet</span></span><br><span class=\"line\">mnist_model = FunctionSet(</span><br><span class=\"line\">    linear1=L.Linear(<span class=\"number\">784</span>, <span class=\"number\">300</span>),</span><br><span class=\"line\">    linear2=L.Linear(<span class=\"number\">300</span>, <span class=\"number\">100</span>),</span><br><span class=\"line\">    linear3=L.Linear(<span class=\"number\">100</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Instantiate an optimizer (you should probably use an</span></span><br><span class=\"line\"><span class=\"comment\"># Adam optimizer here for best performance)</span></span><br><span class=\"line\"><span class=\"comment\"># and then setup the optimizer on the FunctionSet.</span></span><br><span class=\"line\">mnist_optimizer = optimizers.Adam()</span><br><span class=\"line\">mnist_optimizer.setup(mnist_model)</span><br></pre></td></tr></table></figure>\n<h3 id=\"构造训练函数\"><a href=\"#构造训练函数\" class=\"headerlink\" title=\"构造训练函数\"></a>构造训练函数</h3><p>现在我们构造一个合适的函数来进行前向传播、定义训练用的数据集以及生成训练之后对MNIST手写图像进行预测的结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Construct a forward pass through the network,</span></span><br><span class=\"line\"><span class=\"comment\"># moving sequentially through a layer then activation function</span></span><br><span class=\"line\"><span class=\"comment\"># as stated above.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_forward</span><span class=\"params\">(data, model)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    out1 = model.linear1(data)</span><br><span class=\"line\">    out2 = F.relu(out1)</span><br><span class=\"line\">    out3 = model.linear2(out2)</span><br><span class=\"line\">    out4 = F.relu(out3)</span><br><span class=\"line\">    final = model.linear3(out4)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> final</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a training function which takes in training data and targets</span></span><br><span class=\"line\"><span class=\"comment\"># as an input.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_train</span><span class=\"params\">(x, y, model, batchsize=<span class=\"number\">1000</span>, n_epochs=<span class=\"number\">20</span>)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    data_size = x.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># loop over epochs</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(n_epochs):</span><br><span class=\"line\">        print(<span class=\"string\">'epoch %d'</span> % (epoch + <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># randomly shuffle the indices of the training data</span></span><br><span class=\"line\">        shuffler = np.random.permutation(data_size)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># loop over batches</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, data_size, batchsize):</span><br><span class=\"line\">            x_var = Variable(x[shuffler[i : i + batchsize]])</span><br><span class=\"line\">            y_var = Variable(y[shuffler[i : i + batchsize]])</span><br><span class=\"line\"></span><br><span class=\"line\">            output = mnist_forward(x_var, model)</span><br><span class=\"line\">            model.zerograds()</span><br><span class=\"line\">            loss = F.softmax_cross_entropy(output, y_var)</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            mnist_optimizer.update()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Make a prediction function, using a softmax and argmax in order to</span></span><br><span class=\"line\"><span class=\"comment\"># match the target space so that we can validate.</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mnist_predict</span><span class=\"params\">(x, model)</span>:</span></span><br><span class=\"line\">    x = Variable(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    output = mnist_forward(x, model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> F.softmax(output).data.argmax(<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"Train-the-Model\"><a href=\"#Train-the-Model\" class=\"headerlink\" title=\"Train the Model\"></a>Train the Model</h3><p>我们现在可以开始训练神经网络了（这里我们使用一个比较小的训练次数和一个比较大的批大小，这样可以帮我们节省一些训练时间。你也可以修改一些参数，说不定就会出现更好的结果呢~）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mnist_train(x_train, y_train, mnist_model, n_epochs=<span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"进行预测\"><a href=\"#进行预测\" class=\"headerlink\" title=\"进行预测\"></a>进行预测</h3><p>最后一件事情就是通过测试集来验证我们的模型的精确度，看看是否出现了过拟合的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Call your prediction function on the test set</span></span><br><span class=\"line\">pred = mnist_predict(x_test, mnist_model)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Compare the prediction to the ground truth target values.</span></span><br><span class=\"line\">accuracy = (pred==y_test).mean()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Print out test accuracy</span></span><br><span class=\"line\">print(<span class=\"string\">\"Test accuracy: %f\"</span> % accuracy)</span><br></pre></td></tr></table></figure>\n<p>out: <code>Test accuracy: 0.965900</code></p>\n<p>可以看到，我们才训练了5次就有了一个96.59%的准确率，amazing~</p>\n<h3 id=\"模型复用\"><a href=\"#模型复用\" class=\"headerlink\" title=\"模型复用\"></a>模型复用</h3><p>如果你觉得某一次的训练结果不错，想要保存下来以后使用，你可以通过Chainer的serializers来将其保存成hdf5格式：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">serializers.save_hdf5(<span class=\"string\">'test.model'</span>, mnist_model)</span><br><span class=\"line\">serializers.save_hdf5(<span class=\"string\">'test.state'</span>, mnist_optimizer)</span><br></pre></td></tr></table></figure>\n<p>要调出使用的时候也很简单：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">serializers.load_hdf5(<span class=\"string\">'my_model.model'</span>, model_name)</span><br><span class=\"line\">serializers.load_hdf5(<span class=\"string\">'my_optimizer.state'</span>, optimizer_name)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>通过这篇入门教程，相信大家对于机器学习以及Chainer都有了一定的概念。可以看出，Chainer是一个非常灵活且实用的框架，机器学习也并非难以理解。如果你想进一步Chaier这个框架，个人觉得去看看<a href=\"http://docs.chainer.org/en/stable/\" target=\"_blank\" rel=\"noopener\">Chainer的官方文档</a>也是一个不错的选择~</p>\n<h3 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note:\"></a>Note:</h3><p>本文译自：<a href=\"http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/\" target=\"_blank\" rel=\"noopener\">Introduction to Chainer: Neural Networks in Python</a></p>\n<h3 id=\"每日一句：\"><a href=\"#每日一句：\" class=\"headerlink\" title=\"每日一句：\"></a>每日一句：</h3><p>On n’est jamais content là où on est.（人们从来不会满意自己所在的地方。）</p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjmlcpipv0000t7x0a36oz7ae","tag_id":"cjmlcpiq20002t7x0x5rs9a7u","_id":"cjmlcpiqh000ht7x0oxjwye7o"},{"post_id":"cjmlcpipv0000t7x0a36oz7ae","tag_id":"cjmlcpiq60006t7x0cbx6fhcj","_id":"cjmlcpiqi000jt7x00nszhei7"},{"post_id":"cjmlcpipv0000t7x0a36oz7ae","tag_id":"cjmlcpiq90009t7x079gttbxd","_id":"cjmlcpiqj000mt7x03wrggs4l"},{"post_id":"cjmlcpipv0000t7x0a36oz7ae","tag_id":"cjmlcpiqd000ct7x0kuyvr256","_id":"cjmlcpiqk000ot7x0nl5etwzj"},{"post_id":"cjmlcpiq00001t7x04bjcai3i","tag_id":"cjmlcpiqg000ft7x0hh766hdc","_id":"cjmlcpiqp000vt7x0tf1z7kf1"},{"post_id":"cjmlcpiq00001t7x04bjcai3i","tag_id":"cjmlcpiqj000lt7x09fqe3ejd","_id":"cjmlcpiqq000wt7x0mve1tahb"},{"post_id":"cjmlcpiq00001t7x04bjcai3i","tag_id":"cjmlcpiql000qt7x0vko5ug8y","_id":"cjmlcpiqq000yt7x0jq6s9nbm"},{"post_id":"cjmlcpiq30003t7x0n4hfbmcd","tag_id":"cjmlcpiqo000tt7x0jvk3hj0g","_id":"cjmlcpiqs0012t7x0d4q818uv"},{"post_id":"cjmlcpiq30003t7x0n4hfbmcd","tag_id":"cjmlcpiqq000xt7x0s8yfmwpc","_id":"cjmlcpiqs0013t7x0kk94od9y"},{"post_id":"cjmlcpiq30003t7x0n4hfbmcd","tag_id":"cjmlcpiqq000zt7x01uc21dxi","_id":"cjmlcpiqs0015t7x0l5tji9x6"},{"post_id":"cjmlcpiq30003t7x0n4hfbmcd","tag_id":"cjmlcpiqq0010t7x0rhgt9gn3","_id":"cjmlcpiqs0016t7x0p2cd81vl"},{"post_id":"cjmlcpiq40004t7x0jzvdphte","tag_id":"cjmlcpiqo000tt7x0jvk3hj0g","_id":"cjmlcpiqt001at7x04otcpr6n"},{"post_id":"cjmlcpiq40004t7x0jzvdphte","tag_id":"cjmlcpiqs0014t7x06xvztayj","_id":"cjmlcpiqt001bt7x0ynhc3wh6"},{"post_id":"cjmlcpiq40004t7x0jzvdphte","tag_id":"cjmlcpiqs0017t7x0k7q9jyta","_id":"cjmlcpiqu001dt7x00dcqymxe"},{"post_id":"cjmlcpiq40004t7x0jzvdphte","tag_id":"cjmlcpiqq0010t7x0rhgt9gn3","_id":"cjmlcpiqu001et7x0yanifdac"},{"post_id":"cjmlcpiq50005t7x08ej5izpb","tag_id":"cjmlcpiqt0019t7x05flzknor","_id":"cjmlcpiqu001ht7x0dp2oxdjf"},{"post_id":"cjmlcpiq50005t7x08ej5izpb","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpiqu001it7x0zd5owbvb"},{"post_id":"cjmlcpiq50005t7x08ej5izpb","tag_id":"cjmlcpiqu001ft7x07s87lhv5","_id":"cjmlcpiqv001kt7x0kfuvxdwa"},{"post_id":"cjmlcpiq70007t7x07084hr2d","tag_id":"cjmlcpiqu001gt7x0r44wdz78","_id":"cjmlcpiqv001mt7x0c9ubpbp6"},{"post_id":"cjmlcpiq70007t7x07084hr2d","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpiqv001nt7x0ryzse3o6"},{"post_id":"cjmlcpiq80008t7x0i3d4ncj7","tag_id":"cjmlcpiqo000tt7x0jvk3hj0g","_id":"cjmlcpiqw001rt7x03i9y78p5"},{"post_id":"cjmlcpiq80008t7x0i3d4ncj7","tag_id":"cjmlcpiqv001ot7x0hbcbr585","_id":"cjmlcpiqw001st7x0gtcyjzwp"},{"post_id":"cjmlcpiq80008t7x0i3d4ncj7","tag_id":"cjmlcpiqq0010t7x0rhgt9gn3","_id":"cjmlcpiqw001ut7x00u0ot9t2"},{"post_id":"cjmlcpiq9000at7x0wsnuv7an","tag_id":"cjmlcpiqw001qt7x0orijqc60","_id":"cjmlcpiqx001yt7x07dkbgll7"},{"post_id":"cjmlcpiq9000at7x0wsnuv7an","tag_id":"cjmlcpiqw001tt7x0f82kh1u3","_id":"cjmlcpiqx001zt7x0rfidb48t"},{"post_id":"cjmlcpiq9000at7x0wsnuv7an","tag_id":"cjmlcpiqx001vt7x0ffoupjvj","_id":"cjmlcpiqx0021t7x02yfsmk4h"},{"post_id":"cjmlcpiq9000at7x0wsnuv7an","tag_id":"cjmlcpiqx001wt7x0v679fptq","_id":"cjmlcpiqx0022t7x0vacoeoah"},{"post_id":"cjmlcpiqb000bt7x0rpp86pgc","tag_id":"cjmlcpiqx001xt7x0tsrmysac","_id":"cjmlcpiqy0026t7x079tu7rli"},{"post_id":"cjmlcpiqb000bt7x0rpp86pgc","tag_id":"cjmlcpiqx0020t7x0qt1cqvg8","_id":"cjmlcpiqy0027t7x06sgjf0tj"},{"post_id":"cjmlcpiqb000bt7x0rpp86pgc","tag_id":"cjmlcpiqy0023t7x0916oemlp","_id":"cjmlcpiqz0029t7x0wh23qv2j"},{"post_id":"cjmlcpiqb000bt7x0rpp86pgc","tag_id":"cjmlcpiqx001wt7x0v679fptq","_id":"cjmlcpiqz002at7x08cexqhgk"},{"post_id":"cjmlcpiqf000et7x046atqnb6","tag_id":"cjmlcpiqy0025t7x0oakz9or6","_id":"cjmlcpir0002ft7x0c3jgbr07"},{"post_id":"cjmlcpiqf000et7x046atqnb6","tag_id":"cjmlcpiqw001tt7x0f82kh1u3","_id":"cjmlcpir0002gt7x0mssnxyot"},{"post_id":"cjmlcpiqf000et7x046atqnb6","tag_id":"cjmlcpiqz002bt7x04fqwnwob","_id":"cjmlcpir1002it7x0699q3wm7"},{"post_id":"cjmlcpiqf000et7x046atqnb6","tag_id":"cjmlcpiqz002ct7x0qsqhi2iw","_id":"cjmlcpir1002jt7x0b20h944a"},{"post_id":"cjmlcpiqf000et7x046atqnb6","tag_id":"cjmlcpiqx001wt7x0v679fptq","_id":"cjmlcpir1002lt7x0rpzohtaa"},{"post_id":"cjmlcpiqg000gt7x0ci9yicgp","tag_id":"cjmlcpir0002et7x0dbi6kcry","_id":"cjmlcpir2002ot7x01f79y6z0"},{"post_id":"cjmlcpiqg000gt7x0ci9yicgp","tag_id":"cjmlcpir0002ht7x0fvusvx34","_id":"cjmlcpir2002pt7x0tu4jwtpb"},{"post_id":"cjmlcpiqg000gt7x0ci9yicgp","tag_id":"cjmlcpir1002kt7x0aq5sl8v1","_id":"cjmlcpir2002rt7x0wr74tq77"},{"post_id":"cjmlcpiqg000gt7x0ci9yicgp","tag_id":"cjmlcpir1002mt7x0h1y3xpdy","_id":"cjmlcpir2002st7x00kctx0hc"},{"post_id":"cjmlcpiqh000it7x0wh1o86yw","tag_id":"cjmlcpiqt0019t7x05flzknor","_id":"cjmlcpir3002vt7x0fawq9sag"},{"post_id":"cjmlcpiqh000it7x0wh1o86yw","tag_id":"cjmlcpir2002qt7x0jppisi6r","_id":"cjmlcpir3002wt7x0ltiyld89"},{"post_id":"cjmlcpiqh000it7x0wh1o86yw","tag_id":"cjmlcpir2002tt7x0rrozpw0l","_id":"cjmlcpir3002yt7x0xz27tice"},{"post_id":"cjmlcpiqi000kt7x0fkh3uxkk","tag_id":"cjmlcpir2002ut7x0osjz7s0a","_id":"cjmlcpir50032t7x0m0eeccej"},{"post_id":"cjmlcpiqi000kt7x0fkh3uxkk","tag_id":"cjmlcpiqo000tt7x0jvk3hj0g","_id":"cjmlcpir50033t7x0p367h0ix"},{"post_id":"cjmlcpiqi000kt7x0fkh3uxkk","tag_id":"cjmlcpir3002zt7x0qwigbuu3","_id":"cjmlcpir50035t7x082xm7y94"},{"post_id":"cjmlcpiqi000kt7x0fkh3uxkk","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpir50036t7x0nnij2unj"},{"post_id":"cjmlcpiqj000nt7x0ue31dmar","tag_id":"cjmlcpir40031t7x0yjwe637u","_id":"cjmlcpir50038t7x0tc34yhjh"},{"post_id":"cjmlcpiqj000nt7x0ue31dmar","tag_id":"cjmlcpir50034t7x01wj9vh8q","_id":"cjmlcpir50039t7x0uh8mzaaz"},{"post_id":"cjmlcpiqk000pt7x0s8n7bd0k","tag_id":"cjmlcpiqo000tt7x0jvk3hj0g","_id":"cjmlcpir7003dt7x051nl20ne"},{"post_id":"cjmlcpiqk000pt7x0s8n7bd0k","tag_id":"cjmlcpir6003at7x01nuixz3p","_id":"cjmlcpir8003et7x0sxpwm0tz"},{"post_id":"cjmlcpiqk000pt7x0s8n7bd0k","tag_id":"cjmlcpiqq0010t7x0rhgt9gn3","_id":"cjmlcpir8003gt7x04af0bjl6"},{"post_id":"cjmlcpiqm000rt7x0uw7vfwuk","tag_id":"cjmlcpiqt0019t7x05flzknor","_id":"cjmlcpir9003it7x000cg1xdh"},{"post_id":"cjmlcpiqm000rt7x0uw7vfwuk","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpir9003jt7x0izy3kg7y"},{"post_id":"cjmlcpiqn000st7x0849qy1j5","tag_id":"cjmlcpir2002ut7x0osjz7s0a","_id":"cjmlcpira003nt7x08ul48qzp"},{"post_id":"cjmlcpiqn000st7x0849qy1j5","tag_id":"cjmlcpir9003kt7x09gqir7n2","_id":"cjmlcpira003ot7x0n9r0m5c5"},{"post_id":"cjmlcpiqn000st7x0849qy1j5","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpirb003qt7x06lgg5b83"},{"post_id":"cjmlcpiqp000ut7x0ujn2tsky","tag_id":"cjmlcpir2002ut7x0osjz7s0a","_id":"cjmlcpirc003st7x0ewn6a2ye"},{"post_id":"cjmlcpiqp000ut7x0ujn2tsky","tag_id":"cjmlcpir9003kt7x09gqir7n2","_id":"cjmlcpirc003tt7x057xf4a1g"},{"post_id":"cjmlcpiqp000ut7x0ujn2tsky","tag_id":"cjmlcpiqu001ct7x0wj7yn5ux","_id":"cjmlcpirc003ut7x0io08ne50"},{"post_id":"cjmlcpirf003vt7x014d3grwa","tag_id":"cjmlcpiq20002t7x0x5rs9a7u","_id":"cjmlcpirh003wt7x074te50io"},{"post_id":"cjmlcpirf003vt7x014d3grwa","tag_id":"cjmlcpiq60006t7x0cbx6fhcj","_id":"cjmlcpirh003xt7x086tquxij"},{"post_id":"cjmlcpirf003vt7x014d3grwa","tag_id":"cjmlcpiq90009t7x079gttbxd","_id":"cjmlcpirh003yt7x0qni7lvu4"},{"post_id":"cjmlcpirf003vt7x014d3grwa","tag_id":"cjmlcpiqd000ct7x0kuyvr256","_id":"cjmlcpirh003zt7x0koyo8jcr"}],"Tag":[{"name":"chainer","_id":"cjmlcpiq20002t7x0x5rs9a7u"},{"name":"入门教程","_id":"cjmlcpiq60006t7x0cbx6fhcj"},{"name":"机器学习","_id":"cjmlcpiq90009t7x079gttbxd"},{"name":"神经网络","_id":"cjmlcpiqd000ct7x0kuyvr256"},{"name":"mac","_id":"cjmlcpiqg000ft7x0hh766hdc"},{"name":"卸载","_id":"cjmlcpiqj000lt7x09fqe3ejd"},{"name":"pandoc","_id":"cjmlcpiql000qt7x0vko5ug8y"},{"name":"python","_id":"cjmlcpiqo000tt7x0jvk3hj0g"},{"name":"dict","_id":"cjmlcpiqq000xt7x0s8yfmwpc"},{"name":"字典","_id":"cjmlcpiqq000zt7x01uc21dxi"},{"name":"python入门","_id":"cjmlcpiqq0010t7x0rhgt9gn3"},{"name":"threading","_id":"cjmlcpiqs0014t7x06xvztayj"},{"name":"多线程","_id":"cjmlcpiqs0017t7x0k7q9jyta"},{"name":"linux","_id":"cjmlcpiqt0019t7x05flzknor"},{"name":"trick","_id":"cjmlcpiqu001ct7x0wj7yn5ux"},{"name":"tensorboard","_id":"cjmlcpiqu001ft7x07s87lhv5"},{"name":"git","_id":"cjmlcpiqu001gt7x0r44wdz78"},{"name":"pickle","_id":"cjmlcpiqv001ot7x0hbcbr585"},{"name":"PCNH","_id":"cjmlcpiqw001qt7x0orijqc60"},{"name":"CNN","_id":"cjmlcpiqw001tt7x0f82kh1u3"},{"name":"Privacy Detect","_id":"cjmlcpiqx001vt7x0ffoupjvj"},{"name":"Reading Notes","_id":"cjmlcpiqx001wt7x0v679fptq"},{"name":"web","_id":"cjmlcpiqx001xt7x0tsrmysac"},{"name":"crawler","_id":"cjmlcpiqx0020t7x0qt1cqvg8"},{"name":"unicrawl","_id":"cjmlcpiqy0023t7x0916oemlp"},{"name":"object detection","_id":"cjmlcpiqy0025t7x0oakz9or6"},{"name":"multi-region","_id":"cjmlcpiqz002bt7x04fqwnwob"},{"name":"sematic segmentation-aware","_id":"cjmlcpiqz002ct7x0qsqhi2iw"},{"name":"jupyter","_id":"cjmlcpir0002et7x0dbi6kcry"},{"name":"远程访问","_id":"cjmlcpir0002ht7x0fvusvx34"},{"name":"ipython","_id":"cjmlcpir1002kt7x0aq5sl8v1"},{"name":"notebook","_id":"cjmlcpir1002mt7x0h1y3xpdy"},{"name":"磁盘","_id":"cjmlcpir2002qt7x0jppisi6r"},{"name":"空间","_id":"cjmlcpir2002tt7x0rrozpw0l"},{"name":"mysql","_id":"cjmlcpir2002ut7x0osjz7s0a"},{"name":"mysql-python安装错误","_id":"cjmlcpir3002zt7x0qwigbuu3"},{"name":"zabbix安装","_id":"cjmlcpir40031t7x0yjwe637u"},{"name":"lamp配置","_id":"cjmlcpir50034t7x01wj9vh8q"},{"name":"os","_id":"cjmlcpir6003at7x01nuixz3p"},{"name":"远程连接报错","_id":"cjmlcpir9003kt7x09gqir7n2"}]}}