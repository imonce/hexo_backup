<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">

<script data-ad-client="ca-pub-1831623022964098" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/C_Meng.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/C_Meng.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/C_Meng.png?v=7.1.0">


  <link rel="mask-icon" href="/images/C_Meng.png?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  


  <meta name="description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta name="keywords" content="tensorflow,word2vec,n-gram,skip-gram,Embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="通过TensorFlow实现word embedding">
<meta property="og:url" content="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/index.html">
<meta property="og:site_name" content="C_Meng PSNA">
<meta property="og:description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png">
<meta property="og:updated_time" content="2020-01-04T08:02:49.214Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="通过TensorFlow实现word embedding">
<meta name="twitter:description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta name="twitter:image" content="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png">





  
  
  <link rel="canonical" href="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>通过TensorFlow实现word embedding | C_Meng PSNA</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">C_Meng PSNA</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <h1 class="site-subtitle" itemprop="description">Never wait for the storm to pass, just dance in the rain.</h1>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags<span class="badge">179</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories<span class="badge">27</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives<span class="badge">111</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
    
      
    

    
      
    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  

  

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ltb25jZQ==" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></span>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="C_Meng">
      <meta itemprop="description" content="A stupid old man with a humble learning heart. And his dog.">
      <meta itemprop="image" content="/images/C_Meng.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="C_Meng PSNA">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">通过TensorFlow实现word embedding

              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-10-21 10:16:01" itemprop="dateCreated datePublished" datetime="2019-10-21T10:16:01+08:00">2019-10-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2020-01-04 16:02:49" itemprop="dateModified" datetime="2020-01-04T16:02:49+08:00">2020-01-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/10/21/通过TensorFlow实现word-embedding/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/10/21/通过TensorFlow实现word-embedding/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             Views:  
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。</p>
<p>两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。</p>
<p>这里，我们使用skip-gram的方法。</p>
<h1 id="python脚本"><a href="#python脚本" class="headerlink" title="python脚本"></a>python脚本</h1><p>IDE: jupyter notebook</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicPatternEmbedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.url = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line">        self.data_index = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        self.vocabulary_size = <span class="number">5000</span></span><br><span class="line">        </span><br><span class="line">        self.batch_size = <span class="number">128</span></span><br><span class="line">        self.embedding_size = <span class="number">128</span>  <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">        self.skip_window = <span class="number">1</span>       <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">        self.num_skips = <span class="number">2</span>         <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># We pick a random validation set to sample nearest neighbors. Here we limit the</span></span><br><span class="line">        <span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line">        <span class="comment"># construction are also the most frequent.</span></span><br><span class="line">        self.valid_size = <span class="number">16</span>     <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">        self.valid_window = <span class="number">100</span>  <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">        <span class="comment"># choose 16 numbers from 0 to 99 randomly</span></span><br><span class="line">        self.valid_examples = np.random.choice(self.valid_window, self.valid_size, replace=<span class="keyword">False</span>)</span><br><span class="line">        self.num_sampled = <span class="number">64</span>    <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line">        self.num_steps = <span class="number">10001</span></span><br><span class="line">        </span><br><span class="line">        self.final_embedding = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># download and verify the dataset file</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(self, filename, expected_bytes)</span>:</span></span><br><span class="line">        <span class="comment"># If the dataset file is not under the current path, download it directly</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">            filename, _ = urllib.request.urlretrieve(self.url + filename, filename)</span><br><span class="line">        <span class="comment"># get dataset file infomationn</span></span><br><span class="line">        statinfo = os.stat(filename)</span><br><span class="line">        <span class="comment"># verify file size</span></span><br><span class="line">        <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">            print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(statinfo.st_size)</span><br><span class="line">            <span class="keyword">raise</span> Exception(</span><br><span class="line">                <span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">        <span class="keyword">return</span> filename</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># read the data from zip into a list of strings</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># separate by default separators, that is, all null characters, including spaces, newlines (\n), tabs (\t), etc.</span></span><br><span class="line">            data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># process raw inputs into a dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="comment"># add unknown words into count list</span></span><br><span class="line">        count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]</span><br><span class="line">        <span class="comment"># count the words list and add the pairs (word_name, number) into count list</span></span><br><span class="line">        count.extend(collections.Counter(words).most_common(self.vocabulary_size - <span class="number">1</span>))</span><br><span class="line">        dictionary = dict()</span><br><span class="line">        <span class="comment"># create a dictionary of the words with serial number</span></span><br><span class="line">        <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">            dictionary[word] = len(dictionary)</span><br><span class="line">        data = list()</span><br><span class="line">        unk_count = <span class="number">0</span></span><br><span class="line">        <span class="comment"># convert the word list into a number list, 0 for unknown words</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">                index = dictionary[word]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                index = <span class="number">0</span></span><br><span class="line">                unk_count += <span class="number">1</span></span><br><span class="line">            data.append(index)</span><br><span class="line">        <span class="comment"># update the number of UNK</span></span><br><span class="line">        count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">        <span class="comment"># generate a new dictionary by exchanging key and value</span></span><br><span class="line">        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">        <span class="keyword">return</span> data, count, dictionary, reversed_dictionary</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># function to generate a training batch for the skip-gram model</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># make sure the data length is OK</span></span><br><span class="line">        <span class="keyword">assert</span> self.batch_size % self.num_skips == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self.num_skips &lt;= <span class="number">2</span> * self.skip_window</span><br><span class="line"></span><br><span class="line">        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)</span><br><span class="line">        labels = np.ndarray(shape=(self.batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">        span = <span class="number">2</span> * self.skip_window + <span class="number">1</span>  <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">        <span class="comment"># create a new double-ended queue to store the buffer</span></span><br><span class="line">        buffer = collections.deque(maxlen=span)</span><br><span class="line">        <span class="comment"># data_index indicates the end point of the current window</span></span><br><span class="line">        <span class="keyword">if</span> self.data_index + span &gt; len(data):</span><br><span class="line">            data_index = <span class="number">0</span></span><br><span class="line">        buffer.extend(data[self.data_index:self.data_index + span])</span><br><span class="line">        self.data_index += span</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.batch_size // self.num_skips):</span><br><span class="line">            target = self.skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">            targets_to_avoid = [self.skip_window]</span><br><span class="line">            <span class="comment"># sample num_skips batches and labels, optimizable</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.num_skips):</span><br><span class="line">                <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                    target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># avoid sampling to the same target</span></span><br><span class="line">                targets_to_avoid.append(target)</span><br><span class="line">                <span class="comment"># each batch item stands for input</span></span><br><span class="line">                batch[i * self.num_skips + j] = buffer[self.skip_window]</span><br><span class="line">                <span class="comment"># each label item stands for ground truth</span></span><br><span class="line">                labels[i * self.num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">            <span class="keyword">if</span> self.data_index == len(data):</span><br><span class="line">                buffer[:] = data[:span]</span><br><span class="line">                self.data_index = span</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                buffer.append(data[self.data_index])</span><br><span class="line">                self.data_index += <span class="number">1</span></span><br><span class="line">        <span class="comment"># Backtrack a little bit to avoid skipping words in the end of a batch</span></span><br><span class="line">        self.data_index = self.data_index - span</span><br><span class="line">        <span class="keyword">return</span> batch, labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data, reverse_dictionary)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])</span><br><span class="line">            train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, <span class="number">1</span>])</span><br><span class="line">            valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ops and variables pinned to the CPU</span></span><br><span class="line">            <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">                <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">                embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">                <span class="comment"># according to embeddings, the 128-dimensional vector corresponding to the input word(train inputs) was extracted</span></span><br><span class="line">                embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Construct the variables for the NCE loss</span></span><br><span class="line">                nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, self.embedding_size], stddev=<span class="number">1.0</span> / math.sqrt(self.embedding_size)))</span><br><span class="line">                nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))</span><br><span class="line">            <span class="comment"># Compute the average NCE loss for the batch.</span></span><br><span class="line">            <span class="comment"># tf.nce_loss automatically draws a new sample of the negative labels each</span></span><br><span class="line">            <span class="comment"># time we evaluate the loss.</span></span><br><span class="line">            loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                             biases=nce_biases,</span><br><span class="line">                             labels=train_labels,</span><br><span class="line">                             inputs=embed,</span><br><span class="line">                             num_sampled=self.num_sampled,</span><br><span class="line">                             num_classes=self.vocabulary_size))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Construct the SGD optimizer using a learning rate of 1.0.</span></span><br><span class="line">            optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the cosine similarity between minibatch examples and all embeddings.</span></span><br><span class="line">            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">            normalized_embeddings = embeddings / norm</span><br><span class="line">            valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)</span><br><span class="line">            similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Add variable initializer.</span></span><br><span class="line">            init = tf.global_variables_initializer()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.Session(graph = self.graph) <span class="keyword">as</span> session:</span><br><span class="line">            init.run()</span><br><span class="line">            average_loss = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> xrange(self.num_steps):</span><br><span class="line">                batch_inputs, batch_labels = self.generate_batch(data)</span><br><span class="line">                feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment"># we perform one update step by evaluating the optimizer op (including it</span></span><br><span class="line">                <span class="comment"># in the list of returned values for session.run()</span></span><br><span class="line">                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">                average_loss += loss_val</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                        average_loss /= <span class="number">2000</span></span><br><span class="line">                    <span class="comment"># the average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">                    print(<span class="string">'Average loss at step '</span>, step, <span class="string">': '</span>, average_loss)</span><br><span class="line">                    average_loss = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># output the most similar eight words to the screen</span></span><br><span class="line">                <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                    sim = similarity.eval()</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.valid_size):</span><br><span class="line">                        valid_word = reverse_dictionary[self.valid_examples[i]]</span><br><span class="line">                        top_k = <span class="number">8</span>  <span class="comment"># number of nearest neighbors</span></span><br><span class="line">                        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">                        log_str = <span class="string">'Nearest to %s:'</span> % valid_word</span><br><span class="line">                        <span class="keyword">for</span> k <span class="keyword">in</span> xrange(top_k):</span><br><span class="line">                            close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">                            log_str = <span class="string">'%s %s,'</span> % (log_str, close_word)</span><br><span class="line">                        print(log_str)</span><br><span class="line">                        </span><br><span class="line">            self.final_embeddings = normalized_embeddings.eval()</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># visualize the embeddings</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(self, low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels than embeddings'</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))  <span class="comment"># in inches</span></span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">            x, y = low_dim_embs[i, :]</span><br><span class="line">            plt.scatter(x, y)</span><br><span class="line">            plt.annotate(label,</span><br><span class="line">                            xy=(x, y),</span><br><span class="line">                            xytext=(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">                            textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                            ha=<span class="string">'right'</span>,</span><br><span class="line">                            va=<span class="string">'bottom'</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="comment">#plt.savefig(filename)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        bpe = BasicPatternEmbedding()</span><br><span class="line">        filename = bpe.maybe_download(<span class="string">'text8.zip'</span>,<span class="number">31344016</span>)</span><br><span class="line">        vocabulary = bpe.read_data(filename)</span><br><span class="line">        print(<span class="string">'Data size'</span>, len(vocabulary))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'vocabulary:'</span>, vocabulary[:<span class="number">10</span>])</span><br><span class="line">        </span><br><span class="line">        data, count, dictionary, reverse_dictionary = bpe.build_dataset(vocabulary)</span><br><span class="line">        <span class="keyword">del</span> vocabulary  <span class="comment"># Hint to reduce memory.</span></span><br><span class="line">        print(<span class="string">'Most common words (+UNK)'</span>, count[:<span class="number">5</span>])</span><br><span class="line">        print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>], [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> data[:<span class="number">10</span>]])</span><br><span class="line">        </span><br><span class="line">        batch, labels = bpe.generate_batch(data)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">            print(batch[i], reverse_dictionary[batch[i]], <span class="string">'-&gt;'</span>, labels[i, <span class="number">0</span>], reverse_dictionary[labels[i, <span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">print</span> (dictionary[<span class="string">'a'</span>], dictionary[<span class="string">'as'</span>], dictionary[<span class="string">'term'</span>])</span><br><span class="line">        </span><br><span class="line">        bpe.train(data, reverse_dictionary)</span><br><span class="line">        </span><br><span class="line">        tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>, method=<span class="string">'exact'</span>)</span><br><span class="line">        plot_only = <span class="number">300</span></span><br><span class="line">        low_dim_embs = tsne.fit_transform(bpe.final_embeddings[:plot_only, :])</span><br><span class="line"></span><br><span class="line">        labels = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> xrange(plot_only)]</span><br><span class="line">        bpe.plot_with_labels(low_dim_embs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        print(<span class="string">'Please install sklearn, matplotlib, and scipy to show embeddings.'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Found and verified text8.zip
Data size 17005207
vocabulary: [&apos;anarchism&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;, &apos;used&apos;, &apos;against&apos;]
Most common words (+UNK) [[&apos;UNK&apos;, 2735459], (&apos;the&apos;, 1061396), (&apos;of&apos;, 593677), (&apos;and&apos;, 416629), (&apos;one&apos;, 411764)]
Sample data [0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] [&apos;UNK&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;, &apos;used&apos;, &apos;against&apos;]
3081 originated -&gt; 12 as
3081 originated -&gt; 0 UNK
12 as -&gt; 6 a
12 as -&gt; 3081 originated
6 a -&gt; 195 term
6 a -&gt; 12 as
195 term -&gt; 2 of
195 term -&gt; 6 a
6 12 195
Average loss at step  0 :  185.77481079101562
Nearest to it: confidence, doesn, theatre, came, gulf, cultural, sites, corps,
Nearest to use: buried, grave, observation, dust, batman, security, hungarian, opens,
Nearest to at: warrior, total, rivers, yards, reaction, extinction, exclusively, eu,
Nearest to if: emergency, present, developing, dates, life, for, pennsylvania, genesis,
Nearest to between: grant, execution, generally, power, official, interpreted, hiv, binary,
Nearest to people: unlikely, mainly, prussian, dedicated, shot, spending, dangerous, pick,
Nearest to states: forward, racing, begins, printed, follow, vacuum, study, mythology,
Nearest to by: rulers, protestant, marvel, republic, zero, letters, researchers, amiga,
Nearest to american: hit, stores, managed, practiced, intermediate, retrieved, moreover, unique,
Nearest to world: leadership, decay, culture, false, vii, et, dialogue, gave,
Nearest to but: denominations, passing, according, germans, medical, emperors, working, grant,
Nearest to an: removed, marxist, experts, ac, eugene, bones, tree, ne,
Nearest to were: coat, facing, grammar, storage, teach, covering, solomon, circuit,
Nearest to to: plant, supporting, pay, pp, shell, problem, acids, post,
Nearest to be: judah, photo, films, both, senate, woman, villages, eating,
Nearest to used: legislative, hero, private, organ, spaces, vice, top, trivia,
Average loss at step  2000 :  22.257665908694268
Average loss at step  4000 :  5.249317247629166
Average loss at step  6000 :  4.652066127896309
Average loss at step  8000 :  4.529780765414238
Average loss at step  10000 :  4.432040006399155
Nearest to it: he, came, votes, matters, doesn, whole, confidence, continues,
Nearest to use: alien, buried, security, hungarian, grave, dust, batman, amount,
Nearest to at: in, killed, appearance, extinction, mathbf, rivers, eu, pronunciation,
Nearest to if: life, molecules, emergency, dates, present, pennsylvania, for, rates,
Nearest to between: eight, execution, vs, of, hiv, grant, official, documentary,
Nearest to people: UNK, mainly, dedicated, selection, unlikely, shot, fact, dangerous,
Nearest to states: forward, racing, cover, arithmetic, study, vacuum, vs, begins,
Nearest to by: and, as, in, infant, co, manufacturer, with, campaign,
Nearest to american: hit, importance, austin, entry, depending, retrieved, vs, intermediate,
Nearest to world: culture, leadership, UNK, false, mathbf, skills, et, titled,
Nearest to but: and, medical, working, was, connecticut, vs, europeans, denominations,
Nearest to an: the, ac, plant, challenge, experts, necessary, lake, marxist,
Nearest to were: jpg, are, facing, covering, manual, circuit, opposite, test,
Nearest to to: ends, and, plant, in, office, into, supporting, agave,
Nearest to be: iso, shorter, judah, self, painter, also, dependent, assistance,
Nearest to used: opposition, hero, private, illinois, legislative, regime, breaking, repeated,</code></pre><p><img src="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png" alt></p>
<h1 id="相关函数说明"><a href="#相关函数说明" class="headerlink" title="相关函数说明"></a>相关函数说明</h1><h2 id="Tensor-eval"><a href="#Tensor-eval" class="headerlink" title="Tensor.eval()"></a>Tensor.eval()</h2><p>.eval() 其实就是tf.Tensor的Session.run() 的另外一种写法，但两者有差别</p>
<ol>
<li>eval(): 将字符串string对象转化为有效的表达式参与求值运算返回计算结果</li>
<li>eval()也是启动计算的一种方式。基于Tensorflow的基本原理，首先需要定义图，然后计算图，其中计算图的函数常见的有run()函数，如sess.run()。同样eval()也是此类函数，</li>
<li>要注意的是，eval()只能用于tf.Tensor类对象，也就是有输出的Operation，写作Tensor.eval()。对于没有输出的Operation, 可以用.run()或者Session.run()；Session.run()没有这个限制。</li>
</ol>
<h2 id="np-argsort"><a href="#np-argsort" class="headerlink" title="np.argsort()"></a>np.argsort()</h2><p>argsort函数返回的是数组值从小到大的索引值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum()"></a>tf.reduce_sum()</h2><p>reduce_sum( ) 是求和函数，在 tensorflow 里面，计算的都是 tensor，可以通过调整 axis =0,1 的维度来控制求和维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x)</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">0</span>)</span><br><span class="line">[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">1</span>)</span><br><span class="line">[<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">[[<span class="number">3</span>],[<span class="number">3</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, [<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

<h2 id="tf-nn-nce-loss"><a href="#tf-nn-nce-loss" class="headerlink" title="tf.nn.nce_loss()"></a>tf.nn.nce_loss()</h2><p>假设nce_loss之前的输入数据是K维的，一共有N个类，那么</p>
<p>weight.shape = (N, K)</p>
<p>bias.shape = (N)</p>
<p>inputs.shape = (batch_size, K)</p>
<p>labels.shape = (batch_size, num_true)</p>
<p>num_true : 实际的正样本个数</p>
<p>num_sampled: 采样出多少个负样本</p>
<p>num_classes = N</p>
<p>sampled_values: 采样出的负样本，如果是None，就会用不同的sampler去采样。待会儿说sampler是什么。</p>
<p>remove_accidental_hits: 如果采样时不小心采样到的负样本刚好是正样本，要不要干掉</p>
<p>partition_strategy：对weights进行embedding_lookup时并行查表时的策略。TF的embeding_lookup是在CPU里实现的，这里需要考虑多线程查表时的锁的问题</p>
<p>nce_loss的实现逻辑如下：</p>
<p>_compute_sampled_logits: 通过这个函数计算出正样本和采样出的负样本对应的output和label</p>
<p>sigmoid_cross_entropy_with_logits: 通过 sigmoid cross entropy来计算output和label的loss，从而进行反向传播。这个函数把最后的问题转化为了num_sampled+num_real个两类分类问题，然后每个分类问题用了交叉熵的损伤函数，也就是logistic regression常用的损失函数。TF里还提供了一个softmax_cross_entropy_with_logits的函数，和这个有所区别。</p>
<p>在训练过程中，作为input的embed也会被自动更新</p>
<h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup()"></a>tf.nn.embedding_lookup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Signature:</span></span><br><span class="line">tf.nn.embedding_lookup(params, ids, partition_strategy=<span class="string">'mod'</span>, name=<span class="keyword">None</span>, validate_indices=<span class="keyword">True</span>, max_norm=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># Docstring:</span></span><br><span class="line"><span class="comment"># Looks up `ids` in a list of embedding tensors.</span></span><br></pre></td></tr></table></figure>

<p>是根据 ids 中的id，寻找 params 中的第id行。比如 ids=[1,3,5]，则找出params中第1，3，5行，组成一个tensor返回。</p>
<p>embedding_lookup不是简单的查表，params 对应的向量是可以训练的，训练参数个数应该是 feature_num * embedding_size，即前文表述的embedding层权重矩阵，就是说 lookup 的是一种全连接层。</p>
<p>partition_strategy 为张量编号方式，在张量存在多维时起作用，编号的方式有两种，”mod”（默认） 和 “div”。</p>
<p>假设：一共有三个tensor [a,b,c] 作为params 参数，所有tensor的第 0 维上一共有 10 个项目（id 0 ~ 9）。</p>
<p>“mod” : (id) mod len(params) 得到 多少就把 id 分到第几个tensor里面</p>
<ul>
<li>a 依次分到id： 0 3 6 9</li>
<li>b 依次分到id： 1 4 7</li>
<li>c 依次分到id： 2 5 8</li>
</ul>
<p>“div” : (id) div len(params) 可以理解为依次排序，但是这两种切分方式在无法均匀切分的情况下都是将前(max_id+1)%len(params)个 partition 多分配一个元素.</p>
<ul>
<li>a 依次分到id： 0 1 2 3</li>
<li>b 依次分到id： 4 5 6</li>
<li>c 依次分到id： 7 8 9</li>
</ul>
<h2 id="tf-SparseTensor"><a href="#tf-SparseTensor" class="headerlink" title="tf.SparseTensor()"></a>tf.SparseTensor()</h2><p>构造稀疏向量矩阵，每一行为一个样本</p>
<p>SparseTensor(indices, values, dense_shape)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparseTensor(indices=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>]], values=[<span class="number">1</span>, <span class="number">2</span>], dense_shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># represents the dense tensor</span></span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>reference:<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fvb3BxcHFwL2FydGljbGUvZGV0YWlscy83NjAzNzMzNA==" title="https://blog.csdn.net/qoopqpqp/article/details/76037334">https://blog.csdn.net/qoopqpqp/article/details/76037334<i class="fa fa-external-link"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zZWdtZW50ZmF1bHQuY29tL2EvMTE5MDAwMDAxNTI4NzA2Nj91dG1fc291cmNlPXRhZy1uZXdlc3Q=" title="https://segmentfault.com/a/1190000015287066?utm_source=tag-newest">https://segmentfault.com/a/1190000015287066?utm_source=tag-newest<i class="fa fa-external-link"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxOTM0MTYvYXJ0aWNsZS9kZXRhaWxzLzgzMzQ5MTM4" title="https://blog.csdn.net/u012193416/article/details/83349138">https://blog.csdn.net/u012193416/article/details/83349138<i class="fa fa-external-link"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MDkyMjUxL2FydGljbGUvZGV0YWlscy83OTY4NDcyMQ==" title="https://blog.csdn.net/qq_36092251/article/details/79684721">https://blog.csdn.net/qq_36092251/article/details/79684721<i class="fa fa-external-link"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9nc2h0aW1lLmdpdGh1Yi5pby8yMDE4LzA2LzAxL3RlbnNvcmZsb3ctZW1iZWRkaW5nLWxvb2t1cC1zcGFyc2Uv" title="https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/">https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/<i class="fa fa-external-link"></i></span></p>
</blockquote>

      
    </div>

    

    
    
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>您的支持将鼓励我继续创作，欢迎留言私信交朋友呀~</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.png" alt="C_Meng WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.png" alt="C_Meng Alipay">
        <p>Alipay</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/images/paypal.png" alt="C_Meng paypal">
        <p>paypal</p>
      </div>
    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
            <a href="/tags/word2vec/" rel="tag"># word2vec</a>
          
            <a href="/tags/n-gram/" rel="tag"># n-gram</a>
          
            <a href="/tags/skip-gram/" rel="tag"># skip-gram</a>
          
            <a href="/tags/Embedding/" rel="tag"># Embedding</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/10/一文读懂Curry-Howard同构/" rel="next" title="一文读懂Curry-Howard同构">
                <i class="fa fa-chevron-left"></i> 一文读懂Curry-Howard同构
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/21/3小时精通lxml-etree-Python中xml的读取、解析、生成和查找/" rel="prev" title="3小时精通lxml.etree:Python中xml的读取、解析、生成和查找">
                3小时精通lxml.etree:Python中xml的读取、解析、生成和查找 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  

  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1831623022964098" data-ad-slot="5648729926" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/C_Meng.png" alt="C_Meng">
            
              <p class="site-author-name" itemprop="name">C_Meng</p>
              <div class="site-description motion-element" itemprop="description">A stupid old man with a humble learning heart. And his dog.</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">111</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">27</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">179</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2ltb25jZQ==" title="GitHub &rarr; https://github.com/imonce"><i class="fa fa-fw fa-github"></i>GitHub</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="bWFpbHRvOmltb25jZUBvdXRsb29rLmNvbQ==" title="E-Mail &rarr; mailto:imonce@outlook.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="aHR0cHM6Ly93ZWliby5jb20vMzkxMjM5MDgyOS9wcm9maWxlP3RvcG5hdj0xJnd2cj02JmlzX2FsbD0x" title="Weibo &rarr; https://weibo.com/3912390829/profile?topnav=1&wvr=6&is_all=1"><i class="fa fa-fw fa-weibo"></i>Weibo</span>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9tb2VzY2Vl" title="Twitter &rarr; https://twitter.com/moescee"><i class="fa fa-fw fa-twitter"></i>Twitter</span>
                </span>
              
            </div>
          

          

          
          

          
            
          
          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#python脚本"><span class="nav-number">1.</span> <span class="nav-text">python脚本</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#相关函数说明"><span class="nav-number">2.</span> <span class="nav-text">相关函数说明</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-eval"><span class="nav-number">2.1.</span> <span class="nav-text">Tensor.eval()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#np-argsort"><span class="nav-number">2.2.</span> <span class="nav-text">np.argsort()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-reduce-sum"><span class="nav-number">2.3.</span> <span class="nav-text">tf.reduce_sum()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-nn-nce-loss"><span class="nav-number">2.4.</span> <span class="nav-text">tf.nn.nce_loss()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-nn-embedding-lookup"><span class="nav-number">2.5.</span> <span class="nav-text">tf.nn.embedding_lookup()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-SparseTensor"><span class="nav-number">2.6.</span> <span class="nav-text">tf.SparseTensor()</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

      <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-1831623022964098" data-ad-slot="5648729926" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">C_Meng</span>

  

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>




  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  
  <script src="/js/exturl.js?v=7.1.0"></script>


  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'hBUFmUyAMBRRHULc0Y4SPPzw-gzGzoHsz',
    appKey: 'WBQPPWjmwGWqvQcLqprPq0xs',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

  

  

  

  

  

  

</body>
</html>
